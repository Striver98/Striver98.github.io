<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/universe.png">
  <link rel="icon" href="/img/universe.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
    <meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Wang Zhixuan">
  <meta name="keywords" content="">
  
    <meta name="description" content="基于李宏毅机器学习2021/2022课程的笔记">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习笔记">
<meta property="og:url" content="https://striver98.github.io/2025/05/20/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="午夜飞行">
<meta property="og:description" content="基于李宏毅机器学习2021/2022课程的笔记">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250520142048813.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250520142416305.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250520143759484.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250520150115556.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250520151411975.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250520153835073.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250520153801918.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250520154223790.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250520155727316.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250520162332504.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250529152253718.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250528115217325.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250531160251469.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/loss%20on%20training%20data.jpg">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250529150802695.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250529150858566.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250529161104871.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250529161303465.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250529161403019.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250530134110822.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250530134503955.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250530134748892.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250530135024988.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250530135220927.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250530135312521.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250530150107574.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250530150129241.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250531140213148.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250531140243663.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250531140419141.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250531143321759.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250531143803606.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250807144723423.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250531144524458.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250531174912583.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250601142954885.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250601143541190.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250601143426031.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250601143908344.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250601143943607.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250601144635213.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250601144032614.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250601144215207.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250601144408840.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250601144808829.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250601144958923.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250602123656072.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250602141259096.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250602170448517.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250604223511224.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250604223735172.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250604223913217.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250604224019289.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250604224326832.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250604224542613.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250604224556942.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250604224610954.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250604225247315.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250604225312793.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250604225504166.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250604225527303.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250605151544966.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250605151737979.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250605154149324.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250605162715429.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250605214707665.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608114413816.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608115728748.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608115818813.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608115906244.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608115939826.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608120043049.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608120233383.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608120252454.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608120931491.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608125347892.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608125418850.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608130028061.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608131148029.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608131442819.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608132025796.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608132300975.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608132326352.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608132423910.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608132536964.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608132615627.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608132709721.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608132932332.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608133703142.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608134542090.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608134647240.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608135047232.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608150910603.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608150948350.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/93e50799eeb09416eb1e1727bcf09bd2.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608152524850.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250609164348887.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250609164706881.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250609164912819.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250609165356858.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250609165814803.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250609170005494.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250609170616047.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250609171418772.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250609171359287.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250609171802480.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250609171819388.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250609172347710.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250610190028832.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250610190522643.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250610190703305.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250610191218207.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250610193633852.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250610193751736.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250610194003693.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250610194058087.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250610194152345.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250610194222160.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250610195532702.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250610200802740.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250610201722352.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250610202249831.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250610202442479.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250610214342255.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250610214424250.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250610214921145.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250610215258306.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250612005735719.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250612005703078.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/Epoch_039.jpg">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250613181547163.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250613180236231.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250613181046365.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250613181953520.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250613182037798.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250613182117509.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250613182138731.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250613182227705.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250614000249908.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/x3.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250614010826805.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250614012124426.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250614013711437.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250615210451323.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250615213043548.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250615213744336.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616000937024.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250615234510645.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616000949805.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616003835183.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616004150556.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616004626601.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616005202355.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616005533670.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616005751658.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616010057896.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616010155364.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616010243043.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616010550353.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616010753826.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616011042979.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616011107134.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616011127575.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616011311868.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616011427459.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616011936043.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616220614941.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616220841973.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616221646340.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616221732877.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616222011145.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616222200467.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616223103939.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616223151920.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616223620605.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616225101928.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616225212681.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616225622543.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616225805703.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616230029573.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616230309585.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616231158232.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250617170926905.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250617171035588.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250617190346928.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250617192845564.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250617213103560.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250617230753387.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622152152202.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622153041762.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622153405707.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622153841583.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622154012626.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622160528098.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622161118850.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622163628467.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622163729186.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622164524166.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622165031545.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622165625175.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622170115475.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622170247135.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622171711720.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622172156784.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622172903448.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622172931921.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622173014790.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622173208811.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622194705557.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622194951599.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622195753771.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622200955334.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622234910690.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250623165028987.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250623170026200.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250623170243265.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250623170603943.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250623170810050.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250623171120600.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250623171649111.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250623181311356.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250623181455092.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250623181814478.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250623182242551.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250623182341673.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250717175702153.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250717181524098.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250717190902104.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250717192224512.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250717192915979.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250717192942916.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250717192931271.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250717193319389.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250717193454181.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250717193509971.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250717194211814.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250625171708840.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250625171918716.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250625172141329.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250625172155024.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250625172251955.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250625172401227.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250625172527110.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250625172608947.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250625172752316.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250625172821408.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250625173844578.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250625174139363.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250625174330891.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250625180216964.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250625180357804.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250625180700355.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250625180839647.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250625180924442.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250625181017272.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250626214227668.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250626214418668.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250626214549362.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250626214616088.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250626214738233.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250626214956056.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250626215211019.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250626215451625.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250626215529982.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250626215659779.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250626215757898.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250626215902106.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250626215958442.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250626220009072.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250626221813138.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250626221903170.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250626222353976.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250626222422819.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250626222458915.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250626222620225.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250626223134998.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250626223602651.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250627230016177.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250627230250550.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250627230328645.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250627230402337.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250627230500234.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250627230743450.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250627231542540.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250627231728846.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250627231840624.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250627232155711.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250627232440212.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250627232801690.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250627233403870.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250628195104936.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250628195247879.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250628195711071.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250628195747942.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250628200205510.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250628200254174.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250628200750040.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250628200906736.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250628201125737.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250628202043747.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250628202420935.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250628202819224.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250628202954873.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250718195915205.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250718200622943.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250718200723460.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250718205424157.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250718205543420.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250718205954373.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250718211204391.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250718214224598.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250718214531222.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250718214628815.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250718214759930.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250718215041674.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250718215130158.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250718222007637.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250718221030010.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250719192635778.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250719192744359.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250719193002062.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250719193009904.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250719193052867.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250719193129111.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250719193319101.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250719193340631.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250719194003703.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250719195519998.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250719204113405.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250719205131127.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250719213421792.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250723175005711.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250723175529548.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250723233701685.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250723175753361.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250723175950564.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250723182740927.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250723182823410.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/x1.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/x2.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250723231015943.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250723231737968.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250723232827616.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250724212042984.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250724212202764.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250725120807978.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250726095908765.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250727225004655.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250727225224325.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250727225759167.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250727230855214.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250727231759618.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250727232031577.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250727232013371.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250727232218580.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250727232757521.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250727233327907.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250727233731290.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250727234829525.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250727235030494.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250728000230973.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250728000333367.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250728000526513.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250728000609028.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250728000808856.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250728001055693.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250728001143056.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250728001226083.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250728001516209.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250728001813546.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250728115139605.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250728120139365.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250729191630989.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250729191949540.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250729192558133.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250729193440635.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250729193500662.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250729193818305.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250729212959703.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250729213234364.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250729213637026.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250729214145198.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250729214335964.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250729214447332.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250729214639798.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250729215043120.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250730215809443.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250730215934521.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250730225921346.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250801155206141.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250801155232858.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250801155619845.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250801155716042.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250801160918216.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250801161200199.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250801161516942.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250801161738725.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250801161806135.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250801161826579.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250801161943168.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250801162023513.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250801162052752.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250801162229005.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250801162258168.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250801164818923.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250801164842698.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250801165432065.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250801165449902.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250801165521444.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250801171313850.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250804141513530.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250804142038950.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250804142108945.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250804142329741.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250804143152438.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250804144745952.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250804144758404.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250804144941734.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250804150922376.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250804150934616.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250804150940444.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250804151201367.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250804151354095.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250804151519563.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250804151827395.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250804152130274.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250804155307575.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250804155346890.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250804155442027.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250804162623468.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250804163919579.png">
<meta property="og:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250804165751554.png">
<meta property="article:published_time" content="2025-05-20T03:16:08.000Z">
<meta property="article:modified_time" content="2025-08-07T07:07:27.836Z">
<meta property="article:author" content="Wang Zhixuan">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="pytorch">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250520142048813.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>机器学习笔记 - 午夜飞行</title>

  <link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css">



  <link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css">

  <link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css">

  <link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css">



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link rel="stylesheet" href="/css/main.css">


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css">
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css">
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"striver98.github.io","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":["home"]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":"lvc5jMNgiDCshZ3sR3KkiNjR-gzGzoHsz","app_key":"yBvnSteGkTHVkCJDH80pXa1d","server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script src="/js/utils.js"></script>
  <script src="/js/color-schema.js"></script>
  

  

  

  

  

  
    
  



  
<!-- hexo injector head_end start --><style>
body hanla:after {
    content: ' ';
    display: inline;
    font-family: inherit;
    font-size: 0.45em;
}

html code hanla,
html pre hanla,
html kbd hanla,
html samp hanla,
html ruby hanla,
html .tag-list-item hanla {
    display: none;
}

html ol > hanla,
html ul > hanla {
    display: none;
}
</style><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style><link rel="alternate" href="/atom.xml" title="午夜飞行" type="application/atom+xml">
</head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>午夜飞行</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/" target="_self">
                <i class="iconfont icon-link-fill"></i>
                <span>友链</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax="true" style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle">机器学习笔记</span>
          
        </div>

        
          
  <div class="mt-3">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-author" aria-hidden="true"></i>
        Wang Zhixuan
      </span>
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-05-20 11:16" pubdate="">
          2025<hanla></hanla>年<hanla></hanla>5<hanla></hanla>月<hanla></hanla>20<hanla></hanla>日 上午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          30k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          251 分钟
        
      </span>
    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> 次
        </span>
        

      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">机器学习笔记<hanla></hanla></h1>
            
            
              <div class="markdown-body">
                
                <h1 id="introduction-of-machinedeep-learning">Introduction of Machine/Deep Learning</h1>
<h2 id="different-types-of-functions">Different types of Functions</h2>
<p><u><strong>Regression(回归)</strong></u>: The function outputs a scalar.</p>
<p><u><strong>Classification(分类)</strong></u>: Given options(classes), the function outputs the correct one.</p>
<p><strong><u>Structure Learning</u></strong>: create something with structure(image,document).</p>
<h2 id="case-study-prediction-of-no.-of-views-of-a-youtube-channel">Case Study: Prediction of no. of views of a YouTube Channel</h2>
<ol type="1">
<li><p><strong>Function with Unknow Parameters</strong>: <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="11.764ex" height="2.034ex" role="img" focusable="false" viewBox="0 -694 5199.6 899"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(767.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(1823.6,0)"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="msub" transform="translate(2539.6,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(605,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="mo" transform="translate(3770.3,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(4770.6,0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g></g></g></svg></mjx-container> ——<strong>Model</strong></p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250520142048813.png" srcset="/img/loading.gif" lazyload></p></li>
<li><p><strong>Define Loss from Training Data</strong>: <span class="math inline"><em>L</em>(<em>b</em>, <em>w</em>)</span>——<strong>损失函数</strong></p>
<blockquote>
<p>Loss is a function of parameters, it represents how good a set of values is.</p>
</blockquote>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250520142416305.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<p>Loss: <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.781ex;" xmlns="http://www.w3.org/2000/svg" width="13.467ex" height="2.737ex" role="img" focusable="false" viewBox="0 -864.9 5952.3 1209.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mo" transform="translate(958.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mfrac" transform="translate(2014.6,0)"><g data-mml-node="mn" transform="translate(357.2,394) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mi" transform="translate(220,-345) scale(0.707)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g><rect width="827.9" height="60" x="120" y="220"></rect></g><g data-mml-node="munderover" transform="translate(3249.1,0)"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path></g><g data-mml-node="TeXAtom" transform="translate(1089,477.1) scale(0.707)" data-mjx-texclass="ORD"></g><g data-mml-node="TeXAtom" transform="translate(1089,-285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g><g data-mml-node="msub" transform="translate(4979.1,0)"><g data-mml-node="mi"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="TeXAtom" transform="translate(499,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></g></g></svg></mjx-container></span>——自行定义，可正可负</p>
<p>$e=| y- | <span class="math inline">——</span>L$ is mean absolute error(MAE, 平均绝对误差)</p>
<p><span class="math inline"><em>e</em> = (<em>y</em> − <em>ŷ</em>)<sup>2</sup></span>——<span class="math inline"><em>L</em></span> is mean square error(MSE, 均方误差)</p>
<p>if <span class="math inline"><em>y</em></span> and <span class="math inline"><em>ŷ</em></span> are both probability distributions——Cross-entropy(交叉熵)</p>
</blockquote>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250520143759484.png" srcset="/img/loading.gif" lazyload></p></li>
<li><p><strong>Optimization</strong>: <span class="math inline"><em>w</em><sup>′</sup>, <em>b</em><sup>′</sup> = <em>a</em><em>r</em><em>g</em><em>m</em><em>i</em><em>n</em><sub><em>w</em>, <em>b</em></sub><em>L</em></span></p>
<blockquote>
<p><strong>Gradient Descent(梯度下降)</strong>：</p>
<ul>
<li>(Randomly)Pick an initial value <span class="math inline"><em>w</em><sup>0</sup></span>,<span class="math inline"><em>b</em><sup>0</sup></span></li>
<li>Compute <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.817ex;" xmlns="http://www.w3.org/2000/svg" width="11.857ex" height="2.852ex" role="img" focusable="false" viewBox="0 -899.6 5241 1260.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mfrac"><g data-mml-node="mrow" transform="translate(232.4,394) scale(0.707)"><g data-mml-node="mi"><path data-c="1D715" d="M202 508Q179 508 169 520T158 547Q158 557 164 577T185 624T230 675T301 710L333 715H345Q378 715 384 714Q447 703 489 661T549 568T566 457Q566 362 519 240T402 53Q321 -22 223 -22Q123 -22 73 56Q42 102 42 148V159Q42 276 129 370T322 465Q383 465 414 434T455 367L458 378Q478 461 478 515Q478 603 437 639T344 676Q266 676 223 612Q264 606 264 572Q264 547 246 528T202 508ZM430 306Q430 372 401 400T333 428Q270 428 222 382Q197 354 183 323T150 221Q132 149 132 116Q132 21 232 21Q244 21 250 22Q327 35 374 112Q389 137 409 196T430 306Z"></path></g><g data-mml-node="mi" transform="translate(566,0)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g></g><g data-mml-node="mrow" transform="translate(220,-345.6) scale(0.707)"><g data-mml-node="mi"><path data-c="1D715" d="M202 508Q179 508 169 520T158 547Q158 557 164 577T185 624T230 675T301 710L333 715H345Q378 715 384 714Q447 703 489 661T549 568T566 457Q566 362 519 240T402 53Q321 -22 223 -22Q123 -22 73 56Q42 102 42 148V159Q42 276 129 370T322 465Q383 465 414 434T455 367L458 378Q478 461 478 515Q478 603 437 639T344 676Q266 676 223 612Q264 606 264 572Q264 547 246 528T202 508ZM430 306Q430 372 401 400T333 428Q270 428 222 382Q197 354 183 323T150 221Q132 149 132 116Q132 21 232 21Q244 21 250 22Q327 35 374 112Q389 137 409 196T430 306Z"></path></g><g data-mml-node="mi" transform="translate(566,0)"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g></g><rect width="1106.5" height="60" x="120" y="220"></rect></g><g data-mml-node="msub" transform="translate(1346.5,0)"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="TeXAtom" transform="translate(311,-183.8) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mo" transform="translate(716,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msup" transform="translate(1494,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mn" transform="translate(749,289) scale(0.707)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g><g data-mml-node="mo" transform="translate(2646.6,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(2924.6,0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="mo" transform="translate(3353.6,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msup" transform="translate(4131.6,0)"><g data-mml-node="mi"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="mn" transform="translate(462,289) scale(0.707)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g></g></g></g></g></svg></mjx-container></span> , <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.817ex;" xmlns="http://www.w3.org/2000/svg" width="11.801ex" height="2.852ex" role="img" focusable="false" viewBox="0 -899.6 5216.3 1260.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mfrac"><g data-mml-node="mrow" transform="translate(220,394) scale(0.707)"><g data-mml-node="mi"><path data-c="1D715" d="M202 508Q179 508 169 520T158 547Q158 557 164 577T185 624T230 675T301 710L333 715H345Q378 715 384 714Q447 703 489 661T549 568T566 457Q566 362 519 240T402 53Q321 -22 223 -22Q123 -22 73 56Q42 102 42 148V159Q42 276 129 370T322 465Q383 465 414 434T455 367L458 378Q478 461 478 515Q478 603 437 639T344 676Q266 676 223 612Q264 606 264 572Q264 547 246 528T202 508ZM430 306Q430 372 401 400T333 428Q270 428 222 382Q197 354 183 323T150 221Q132 149 132 116Q132 21 232 21Q244 21 250 22Q327 35 374 112Q389 137 409 196T430 306Z"></path></g><g data-mml-node="mi" transform="translate(566,0)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g></g><g data-mml-node="mrow" transform="translate(309.1,-345.6) scale(0.707)"><g data-mml-node="mi"><path data-c="1D715" d="M202 508Q179 508 169 520T158 547Q158 557 164 577T185 624T230 675T301 710L333 715H345Q378 715 384 714Q447 703 489 661T549 568T566 457Q566 362 519 240T402 53Q321 -22 223 -22Q123 -22 73 56Q42 102 42 148V159Q42 276 129 370T322 465Q383 465 414 434T455 367L458 378Q478 461 478 515Q478 603 437 639T344 676Q266 676 223 612Q264 606 264 572Q264 547 246 528T202 508ZM430 306Q430 372 401 400T333 428Q270 428 222 382Q197 354 183 323T150 221Q132 149 132 116Q132 21 232 21Q244 21 250 22Q327 35 374 112Q389 137 409 196T430 306Z"></path></g><g data-mml-node="mi" transform="translate(566,0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g></g><rect width="1081.8" height="60" x="120" y="220"></rect></g><g data-mml-node="msub" transform="translate(1321.8,0)"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="TeXAtom" transform="translate(311,-183.8) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mo" transform="translate(716,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msup" transform="translate(1494,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mn" transform="translate(749,289) scale(0.707)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g><g data-mml-node="mo" transform="translate(2646.6,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(2924.6,0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="mo" transform="translate(3353.6,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msup" transform="translate(4131.6,0)"><g data-mml-node="mi"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="mn" transform="translate(462,289) scale(0.707)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g></g></g></g></g></svg></mjx-container></span></li>
<li>Update <span class="math inline"><em>w</em></span> and <span class="math inline"><em>b</em></span> iteratively(迭代)</li>
</ul>
</blockquote>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250520150115556.png" srcset="/img/loading.gif" lazyload></p></li>
</ol>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250520151411975.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="change-model-piecewise-linear-function">Change Model: Piecewise Linear Function</h2>
<p><strong><u>Model Bias</u></strong>: 模型的局限性导致无法模拟真实的状况——need more sophisticated models</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250520153835073.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250520153801918.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>Different <span class="math inline"><em>w</em></span>——Change slopes</li>
<li>Different <span class="math inline"><em>b</em></span>——Shift</li>
<li>Different <span class="math inline"><em>c</em></span>——Change height</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250520154223790.png" srcset="/img/loading.gif" lazyload></p>
<p><strong>New Model</strong>: $y=b+<em>{i}c</em>{i}sigmoid(b_{i}+<em>{j}w</em>{ij}x_{j}) $</p>
<ul>
<li><span class="math inline"><em>r</em> = <em>b</em> + <em>W</em><em>x</em></span></li>
<li><span class="math inline"><em>a</em> = <em>σ</em>(<em>r</em>)</span></li>
<li><span class="math inline"><em>y</em> = <em>b</em> + <em>c</em><sup><em>T</em></sup><em>a</em></span></li>
<li>All: <span class="math inline"><em>y</em> = <em>b</em> + <em>c</em><sup><em>T</em></sup><em>σ</em>(<em>b</em> + <em>W</em><em>x</em>)</span></li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250520155727316.png" srcset="/img/loading.gif" lazyload></p>
<p>if more hidden layer?——Neural Network——Deep Learning</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250520162332504.png" srcset="/img/loading.gif" lazyload></p>
<p><u><strong>Attention</strong>: the definition of <strong>Loss</strong> and <strong>Optimization</strong> of new Model is almost same as previous.</u></p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250529152253718.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="homework-1-covid-19-cases-prediction">Homework 1: COVID-19 Cases Prediction</h2>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250528115217325.png" srcset="/img/loading.gif" lazyload></p>
<p>Code: <a target="_blank" rel="noopener" href="https://github.com/Striver98/Machine-Learning-Homework/blob/master/HW1.py">link</a></p>
<p>结果：</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250531160251469.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="what-to-do-if-my-network-fails-to-train">What to do if my network fails to train</h1>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/loss%20on%20training%20data.jpg" srcset="/img/loading.gif" lazyload alt="loss on training data"><figcaption aria-hidden="true">loss on training data</figcaption>
</figure>
<h2 id="local-minima-saddle-point">Local Minima &amp; Saddle Point</h2>
<h3 id="mathematical-principles">Mathematical Principles</h3>
<h4 id="tayler-series-approximation">Tayler Series Approximation</h4>
<p><span class="math inline"><em>L</em>(<em>θ</em>)</span> around <span class="math inline"><em>θ</em> = <em>θ</em><sup>′</sup></span>can be approximated below</p>
<p><span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.781ex;" xmlns="http://www.w3.org/2000/svg" width="47.239ex" height="2.737ex" role="img" focusable="false" viewBox="0 -864.9 20879.8 1209.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mo" transform="translate(681,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1070,0)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mo" transform="translate(1539,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(2205.8,0)"><path data-c="2248" d="M55 319Q55 360 72 393T114 444T163 472T205 482Q207 482 213 482T223 483Q262 483 296 468T393 413L443 381Q502 346 553 346Q609 346 649 375T694 454Q694 465 698 474T708 483Q722 483 722 452Q722 386 675 338T555 289Q514 289 468 310T388 357T308 404T224 426Q164 426 125 393T83 318Q81 289 69 289Q55 289 55 319ZM55 85Q55 126 72 159T114 210T163 238T205 248Q207 248 213 248T223 249Q262 249 296 234T393 179L443 147Q502 112 553 112Q609 112 649 141T694 220Q694 249 708 249T722 217Q722 153 675 104T555 55Q514 55 468 76T388 123T308 170T224 192Q164 192 125 159T83 84Q80 55 69 55Q55 55 55 85Z"></path></g><g data-mml-node="mi" transform="translate(3261.6,0)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mo" transform="translate(3942.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msup" transform="translate(4331.6,0)"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="TeXAtom" transform="translate(502,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path></g></g></g><g data-mml-node="mo" transform="translate(5078,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(5689.2,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mo" transform="translate(6689.5,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(7078.5,0)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mo" transform="translate(7769.7,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="msup" transform="translate(8769.9,0)"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="TeXAtom" transform="translate(502,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path></g></g></g><g data-mml-node="msup" transform="translate(9516.4,0)"><g data-mml-node="mo"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mi" transform="translate(422,363) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g><g data-mml-node="mi" transform="translate(10486.2,0)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mo" transform="translate(11185.4,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mfrac" transform="translate(12185.6,0)"><g data-mml-node="mn" transform="translate(220,394) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mn" transform="translate(220,-345) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><rect width="553.6" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(12979.2,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(13368.2,0)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mo" transform="translate(14059.4,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="msup" transform="translate(15059.6,0)"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="TeXAtom" transform="translate(502,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path></g></g></g><g data-mml-node="msup" transform="translate(15806.1,0)"><g data-mml-node="mo"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mi" transform="translate(422,363) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g><g data-mml-node="mi" transform="translate(16775.9,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mo" transform="translate(17663.9,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(18052.9,0)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mo" transform="translate(18744.1,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="msup" transform="translate(19744.3,0)"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="TeXAtom" transform="translate(502,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path></g></g></g><g data-mml-node="mo" transform="translate(20490.8,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></span></p>
<blockquote>
<p>Gradient <span class="math inline"><em>g</em></span> is a vector: <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.034ex;" xmlns="http://www.w3.org/2000/svg" width="10.265ex" height="3.418ex" role="img" focusable="false" viewBox="0 -1053.5 4537.2 1510.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="TeXAtom" transform="translate(510,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g><g data-mml-node="mo" transform="translate(1081.7,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mfrac" transform="translate(2137.5,0)"><g data-mml-node="mrow" transform="translate(220,516.8) scale(0.707)"><g data-mml-node="mi"><path data-c="1D715" d="M202 508Q179 508 169 520T158 547Q158 557 164 577T185 624T230 675T301 710L333 715H345Q378 715 384 714Q447 703 489 661T549 568T566 457Q566 362 519 240T402 53Q321 -22 223 -22Q123 -22 73 56Q42 102 42 148V159Q42 276 129 370T322 465Q383 465 414 434T455 367L458 378Q478 461 478 515Q478 603 437 639T344 676Q266 676 223 612Q264 606 264 572Q264 547 246 528T202 508ZM430 306Q430 372 401 400T333 428Q270 428 222 382Q197 354 183 323T150 221Q132 149 132 116Q132 21 232 21Q244 21 250 22Q327 35 374 112Q389 137 409 196T430 306Z"></path></g><g data-mml-node="mi" transform="translate(566,0)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mo" transform="translate(1247,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msup" transform="translate(1636,0)"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="TeXAtom" transform="translate(502,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path></g></g></g><g data-mml-node="mo" transform="translate(2382.5,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><g data-mml-node="mrow" transform="translate(718.3,-345.6) scale(0.707)"><g data-mml-node="mi"><path data-c="1D715" d="M202 508Q179 508 169 520T158 547Q158 557 164 577T185 624T230 675T301 710L333 715H345Q378 715 384 714Q447 703 489 661T549 568T566 457Q566 362 519 240T402 53Q321 -22 223 -22Q123 -22 73 56Q42 102 42 148V159Q42 276 129 370T322 465Q383 465 414 434T455 367L458 378Q478 461 478 515Q478 603 437 639T344 676Q266 676 223 612Q264 606 264 572Q264 547 246 528T202 508ZM430 306Q430 372 401 400T333 428Q270 428 222 382Q197 354 183 323T150 221Q132 149 132 116Q132 21 232 21Q244 21 250 22Q327 35 374 112Q389 137 409 196T430 306Z"></path></g><g data-mml-node="msub" transform="translate(566,0)"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="TeXAtom" transform="translate(502,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g><rect width="2159.7" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container></span></p>
<p>Hessian <span class="math inline"><em>H</em></span> is a matrix: <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.253ex;" xmlns="http://www.w3.org/2000/svg" width="11.725ex" height="3.478ex" role="img" focusable="false" viewBox="0 -983.7 5182.4 1537.3"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="TeXAtom" transform="translate(864,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(345,0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g></g><g data-mml-node="mo" transform="translate(1727.1,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mfrac" transform="translate(2782.8,0)"><g data-mml-node="msup" transform="translate(827,394) scale(0.707)"><g data-mml-node="mi"><path data-c="1D715" d="M202 508Q179 508 169 520T158 547Q158 557 164 577T185 624T230 675T301 710L333 715H345Q378 715 384 714Q447 703 489 661T549 568T566 457Q566 362 519 240T402 53Q321 -22 223 -22Q123 -22 73 56Q42 102 42 148V159Q42 276 129 370T322 465Q383 465 414 434T455 367L458 378Q478 461 478 515Q478 603 437 639T344 676Q266 676 223 612Q264 606 264 572Q264 547 246 528T202 508ZM430 306Q430 372 401 400T333 428Q270 428 222 382Q197 354 183 323T150 221Q132 149 132 116Q132 21 232 21Q244 21 250 22Q327 35 374 112Q389 137 409 196T430 306Z"></path></g><g data-mml-node="mn" transform="translate(650.8,363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mrow" transform="translate(220,-345.6) scale(0.707)"><g data-mml-node="mi"><path data-c="1D715" d="M202 508Q179 508 169 520T158 547Q158 557 164 577T185 624T230 675T301 710L333 715H345Q378 715 384 714Q447 703 489 661T549 568T566 457Q566 362 519 240T402 53Q321 -22 223 -22Q123 -22 73 56Q42 102 42 148V159Q42 276 129 370T322 465Q383 465 414 434T455 367L458 378Q478 461 478 515Q478 603 437 639T344 676Q266 676 223 612Q264 606 264 572Q264 547 246 528T202 508ZM430 306Q430 372 401 400T333 428Q270 428 222 382Q197 354 183 323T150 221Q132 149 132 116Q132 21 232 21Q244 21 250 22Q327 35 374 112Q389 137 409 196T430 306Z"></path></g><g data-mml-node="msub" transform="translate(566,0)"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="TeXAtom" transform="translate(502,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g><g data-mml-node="mi" transform="translate(1362,0)"><path data-c="1D715" d="M202 508Q179 508 169 520T158 547Q158 557 164 577T185 624T230 675T301 710L333 715H345Q378 715 384 714Q447 703 489 661T549 568T566 457Q566 362 519 240T402 53Q321 -22 223 -22Q123 -22 73 56Q42 102 42 148V159Q42 276 129 370T322 465Q383 465 414 434T455 367L458 378Q478 461 478 515Q478 603 437 639T344 676Q266 676 223 612Q264 606 264 572Q264 547 246 528T202 508ZM430 306Q430 372 401 400T333 428Q270 428 222 382Q197 354 183 323T150 221Q132 149 132 116Q132 21 232 21Q244 21 250 22Q327 35 374 112Q389 137 409 196T430 306Z"></path></g><g data-mml-node="msub" transform="translate(1928,0)"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="TeXAtom" transform="translate(502,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g></g></g><rect width="2159.6" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container></span></p>
</blockquote>
<h4 id="hessian">Hessian</h4>
<p>When at critical point, <span class="math inline">(<em>θ</em> − <em>θ</em><sup>′</sup>)<sup><em>T</em></sup><em>g</em> = 0</span>, <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.781ex;" xmlns="http://www.w3.org/2000/svg" width="19.67ex" height="2.737ex" role="img" focusable="false" viewBox="0 -864.9 8694.2 1209.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mfrac"><g data-mml-node="mn" transform="translate(220,394) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mn" transform="translate(220,-345) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><rect width="553.6" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(793.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1182.6,0)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mo" transform="translate(1873.8,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="msup" transform="translate(2874,0)"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="TeXAtom" transform="translate(502,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path></g></g></g><g data-mml-node="msup" transform="translate(3620.5,0)"><g data-mml-node="mo"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mi" transform="translate(422,363) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g><g data-mml-node="mi" transform="translate(4590.3,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mo" transform="translate(5478.3,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(5867.3,0)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mo" transform="translate(6558.5,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="msup" transform="translate(7558.7,0)"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="TeXAtom" transform="translate(502,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path></g></g></g><g data-mml-node="mo" transform="translate(8305.2,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></span> telling the properties of critical points.</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250529150802695.png" srcset="/img/loading.gif" lazyload></p>
<p>Set <span class="math inline"><em>θ</em> − <em>θ</em><sup>′</sup> = <em>v</em></span>:</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250529150858566.png" srcset="/img/loading.gif" lazyload></p>
<p><strong><span class="math inline"><em>H</em></span> may tell us parameter update direction!</strong></p>
<p>While <span class="math inline"><em>u</em></span> is an eigen vector of <span class="math inline"><em>H</em></span>, <span class="math inline"><em>λ</em></span> is the eigen value of <span class="math inline"><em>u</em></span>, <span class="math inline"><em>u</em><sup><em>T</em></sup><em>H</em><em>u</em> = <em>u</em><sup><em>T</em></sup>(<em>λ</em><em>u</em>) = <em>λ</em>∥<em>u</em>∥<sup>2</sup></span></p>
<p>Example: if <span class="math inline"><em>u</em> &lt; 0</span>, then <span class="math inline"><em>H</em> &lt; 0</span>, then <span class="math inline"><em>L</em>(<em>θ</em>) &lt; <em>L</em>(<em>θ</em><sup>′</sup>)</span> , <span class="math inline"><em>θ</em> = <em>θ</em><sup>′</sup> + <em>u</em></span>, <span class="math inline"><em>L</em></span> can be decreased.</p>
<blockquote>
<p>We can escape the saddle point and decrease the loss. However, <strong>this method is seldom used in practice</strong>.</p>
</blockquote>
<h2 id="batch-momentum">Batch &amp; Momentum</h2>
<h3 id="small-batch-v.s.-large-batch">Small Batch v.s. Large Batch</h3>
<ul>
<li>Larger batch size does <strong>not</strong> require longer time to compute gradient (unless batch size is too large)</li>
<li>Smaller batch requires <strong>longer</strong> time for one epoch (longer time for seeing all data once)</li>
<li>Smaller batch size has better performance in optimization</li>
<li>“Noisy” update is better for training</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250529161104871.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<p>Batch size is a hyperparameter you have to decide.</p>
</blockquote>
<h3 id="gradient-descent-momentum">Gradient Descent + Momentum</h3>
<ul>
<li>Movement: <strong>movement of last step</strong> minus <strong>gradient</strong> at present</li>
<li>Movement not just based on gradient, but previous movement.</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250529161303465.png" srcset="/img/loading.gif" lazyload></p>
<p><strong>Example:</strong></p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250529161403019.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="summary">Summary</h3>
<ul>
<li>Critical points have zero gradients.</li>
<li>Critical points can be either saddle points or local minima.
<ul>
<li>Can be determined by the Hessian matrix.</li>
<li>Local minima may be rare.</li>
<li>It is possible to escape saddle points along the direction of eigen vectors of the Hessian matrix.</li>
</ul></li>
<li><strong>Smaller batch size and momentum help escape critical points.</strong></li>
</ul>
<h2 id="adaptive-learning-rate">Adaptive Learning Rate</h2>
<ul>
<li>People believe training stuck because the parameters are around a critical point, but sometimes learning rate may be the reason.</li>
<li>While learning rate cannot be one-size-fits-all, so <strong>different parameters need different learning rate</strong>.</li>
</ul>
<p>Consider update one parameter(<span class="math inline"><em>t</em></span> means iteration time, <span class="math inline"><em>i</em></span> means the <strong>i</strong>th parameter):</p>
<p><span class="math inline"><em>θ</em><sub><em>i</em></sub><sup><em>t</em> + 1</sup> ← <em>θ</em><sub><em>i</em></sub><sup><em>t</em></sup> − <em>η</em><em>g</em><sub><em>i</em></sub><sup><em>t</em></sup></span></p>
<p><span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.034ex;" xmlns="http://www.w3.org/2000/svg" width="12.139ex" height="3.07ex" role="img" focusable="false" viewBox="0 -899.6 5365.5 1356.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mi"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="TeXAtom" transform="translate(510,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(510,-292.2) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g><g data-mml-node="mo" transform="translate(1093,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mfrac" transform="translate(2148.8,0)"><g data-mml-node="mrow" transform="translate(260.6,394) scale(0.707)"><g data-mml-node="mi"><path data-c="1D715" d="M202 508Q179 508 169 520T158 547Q158 557 164 577T185 624T230 675T301 710L333 715H345Q378 715 384 714Q447 703 489 661T549 568T566 457Q566 362 519 240T402 53Q321 -22 223 -22Q123 -22 73 56Q42 102 42 148V159Q42 276 129 370T322 465Q383 465 414 434T455 367L458 378Q478 461 478 515Q478 603 437 639T344 676Q266 676 223 612Q264 606 264 572Q264 547 246 528T202 508ZM430 306Q430 372 401 400T333 428Q270 428 222 382Q197 354 183 323T150 221Q132 149 132 116Q132 21 232 21Q244 21 250 22Q327 35 374 112Q389 137 409 196T430 306Z"></path></g><g data-mml-node="mi" transform="translate(566,0)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g></g><g data-mml-node="mrow" transform="translate(220,-345.6) scale(0.707)"><g data-mml-node="mi"><path data-c="1D715" d="M202 508Q179 508 169 520T158 547Q158 557 164 577T185 624T230 675T301 710L333 715H345Q378 715 384 714Q447 703 489 661T549 568T566 457Q566 362 519 240T402 53Q321 -22 223 -22Q123 -22 73 56Q42 102 42 148V159Q42 276 129 370T322 465Q383 465 414 434T455 367L458 378Q478 461 478 515Q478 603 437 639T344 676Q266 676 223 612Q264 606 264 572Q264 547 246 528T202 508ZM430 306Q430 372 401 400T333 428Q270 428 222 382Q197 354 183 323T150 221Q132 149 132 116Q132 21 232 21Q244 21 250 22Q327 35 374 112Q389 137 409 196T430 306Z"></path></g><g data-mml-node="msub" transform="translate(566,0)"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="TeXAtom" transform="translate(502,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g><rect width="1163" height="60" x="120" y="220"></rect></g><g data-mml-node="msub" transform="translate(3551.9,0)"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="TeXAtom" transform="translate(311,-163.8) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mo" transform="translate(469,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msup" transform="translate(1247,0)"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="TeXAtom" transform="translate(502,289) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g></g></g></g></g></g></svg></mjx-container></span></p>
<p>While larger gradient needs smaller learning rate and smaller gradient needs larger learning rate, so learning rate has to be <strong>parameter dependent</strong>:</p>
<p><span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.407ex;" xmlns="http://www.w3.org/2000/svg" width="16.277ex" height="3.403ex" role="img" focusable="false" viewBox="0 -882.5 7194.4 1504.3"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="TeXAtom" transform="translate(502,411.6) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(361,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mn" transform="translate(1139,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mi" transform="translate(502,-293.8) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(1988.7,0)"><path data-c="2190" d="M944 261T944 250T929 230H165Q167 228 182 216T211 189T244 152T277 96T303 25Q308 7 308 0Q308 -11 288 -11Q281 -11 278 -11T272 -7T267 2T263 21Q245 94 195 151T73 236Q58 242 55 247Q55 254 59 257T73 264Q121 283 158 314T215 375T247 434T264 480L267 497Q269 503 270 505T275 509T288 511Q308 511 308 500Q308 493 303 475Q293 438 278 406T246 352T215 315T185 287T165 270H929Q944 261 944 250Z"></path></g><g data-mml-node="msubsup" transform="translate(3266.5,0)"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mi" transform="translate(502,363) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(502,-292.2) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(4296,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mfrac" transform="translate(5296.2,0)"><g data-mml-node="mi" transform="translate(365.8,492.7) scale(0.707)"><path data-c="1D702" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q156 442 175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336V326Q503 302 439 53Q381 -182 377 -189Q364 -216 332 -216Q319 -216 310 -208T299 -186Q299 -177 358 57L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="msubsup" transform="translate(220,-408.5) scale(0.707)"><g data-mml-node="mi"><path data-c="1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path></g><g data-mml-node="mi" transform="translate(604,361.4) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(604,-293.8) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><rect width="842.9" height="60" x="120" y="220"></rect></g><g data-mml-node="msubsup" transform="translate(6379.2,0)"><g data-mml-node="mi"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mi" transform="translate(510,363) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(510,-292.2) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container></span></p>
<h3 id="root-mean-square">Root Mean Square</h3>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250530134110822.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250530134503955.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="rmsprop">RMSProp</h3>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250530134748892.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<p>The recent gradient has larger influence, and the past gradients have less influence.</p>
</blockquote>
<h3 id="learning-rate-scheduling">Learning Rate Scheduling</h3>
<h4 id="learning-rate-decay">Learning Rate Decay</h4>
<p>After the training goes, we are close to the destination, so we reduce the learning rate.</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250530135024988.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="warm-up">Warm Up</h4>
<ul>
<li>Increase and then decrease.</li>
<li>At the beginning, the estimate of <span class="math inline"><em>σ</em><sub><em>i</em></sub><sup><em>t</em></sup></span> has large variance.</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250530135220927.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="summary-of-optimization">Summary of Optimization</h3>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250530135312521.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="classification">Classification</h2>
<h3 id="classification-as-regression">Classification as Regression</h3>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250530150107574.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250530150129241.png" srcset="/img/loading.gif" lazyload></p>
<p><strong>Softmax</strong>: <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.307ex;" xmlns="http://www.w3.org/2000/svg" width="13.357ex" height="3.676ex" role="img" focusable="false" viewBox="0 -1047.1 5903.7 1624.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="TeXAtom" transform="translate(523,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(523,-254) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g><g data-mml-node="mo" transform="translate(1094.7,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mfrac" transform="translate(2150.5,0)"><g data-mml-node="mrow" transform="translate(767.9,516.8) scale(0.707)"><g data-mml-node="mi"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(466,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(1038,0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mo" transform="translate(1541,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(1930,0)"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(523,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(2747,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><g data-mml-node="mrow" transform="translate(220,-370.3) scale(0.707)"><g data-mml-node="munder"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path></g><g data-mml-node="TeXAtom" transform="translate(1089,-285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g><g data-mml-node="mi" transform="translate(1549.6,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(2015.6,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(2587.6,0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mo" transform="translate(3090.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(3479.6,0)"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(523,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(4296.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><rect width="3513.2" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container></span></p>
<ul>
<li><span class="math inline">1 &gt; <em>y</em><sub><em>i</em></sub><sup>′</sup> &gt; 0</span></li>
<li><span class="math inline">∑<sub><em>i</em></sub><em>y</em><sub><em>i</em></sub><sup>′</sup> = 1</span></li>
</ul>
<blockquote>
<p>The core function of Softmax is to <strong>normalize</strong> a model’s raw prediction scores (Logits) into a valid, interpretable <strong>probability distribution</strong> (all probabilities &gt;=0 and sum=1), explicitly representing the model’s prediction confidence across multiple mutually exclusive classes.</p>
</blockquote>
<h3 id="loss-of-classification">Loss of Classification</h3>
<p><em><u>Mean Square Error(MSE)</u></em>: <span class="math inline"><em>e</em> = ∑<sub><em>i</em></sub>(<em>ŷ</em><sub><em>i</em></sub> − <em>y</em><sub><em>i</em></sub><sup>′</sup>)<sup>2</sup></span></p>
<p><u><em>Cross-entropy</em></u>: <span class="math inline"><em>e</em> =  − ∑<sub><em>i</em></sub><em>ŷ</em><sub><em>i</em></sub><em>l</em><em>n</em><em>y</em><sub><em>i</em></sub><sup>′</sup></span> <strong>(More Competitive)</strong></p>
<blockquote>
<p><strong>Minimizing cross-entropy</strong> is equivalent to <strong>maximizing likelihood.</strong></p>
<p>Changing the loss function can change the difficulty of optimization.</p>
</blockquote>
<h2 id="case-study-pokémon-v.s.-digimon">Case Study: Pokémon v.s. Digimon</h2>
<ul>
<li>We want to find a function to classify Pokémon/Digimon</li>
<li>Determine a function with unknown parameters(based on domain knowledge)</li>
</ul>
<h3 id="observation">Observation</h3>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250531140213148.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250531140243663.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="function-with-unknown-parameters">Function with Unknown Parameters</h3>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250531140419141.png" srcset="/img/loading.gif" lazyload></p>
<p><span class="math inline">ℋ = {1, 2, ..., 10000}</span>, <span class="math inline">|ℋ|</span>: model “complexity”</p>
<h3 id="loss-of-a-function-given-data">Loss of a function (given data)</h3>
<ul>
<li>Given a dataset <span class="math inline">𝒟</span>
<ul>
<li><span class="math inline">𝒟 = {(<em>x</em><sup>1</sup>, <em>ŷ</em><sup>1</sup>), (<em>x</em><sup>2</sup>, <em>ŷ</em><sup>2</sup>), ..., (<em>x</em><sup><em>N</em></sup>, <em>ŷ</em><sup><em>N</em></sup>)}</span></li>
</ul></li>
<li>Loss of a threshold <span class="math inline"><em>h</em></span> given data set <span class="math inline">𝒟</span>
<ul>
<li><strong>Error rate</strong>: <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.781ex;" xmlns="http://www.w3.org/2000/svg" width="29.571ex" height="2.953ex" role="img" focusable="false" viewBox="0 -960 13070.3 1305"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mo" transform="translate(681,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1070,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mo" transform="translate(1646,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(2090.7,0)"><g data-mml-node="mi"><path data-c="44" d="M37 475Q19 475 19 487Q19 536 103 604T327 682H356Q386 683 408 683H419Q475 683 506 681T582 668T667 633Q766 571 766 450Q766 365 723 287T611 152T455 57T279 6Q248 1 160 0Q148 0 131 0T108 -1Q72 -1 72 11Q72 24 90 40T133 64L144 68L152 88Q247 328 272 587Q275 613 272 613Q272 613 269 613Q225 610 195 602T149 579T129 556T119 532Q118 530 116 525T113 518Q102 502 80 490T37 475ZM665 407Q665 596 412 613Q403 614 383 614Q370 614 370 612Q370 598 363 542T323 357T242 103L228 69H265Q391 73 481 119Q536 148 575 188T633 268T658 338T665 392V407Z"></path></g></g><g data-mml-node="mo" transform="translate(2861.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(3528.4,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mfrac" transform="translate(4584.2,0)"><g data-mml-node="mn" transform="translate(357.2,394) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mi" transform="translate(220,-345) scale(0.707)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g><rect width="827.9" height="60" x="120" y="220"></rect></g><g data-mml-node="munderover" transform="translate(5818.8,0)"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path></g><g data-mml-node="TeXAtom" transform="translate(1089,477.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(1089,-285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(600,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1378,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="mi" transform="translate(8452.4,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mo" transform="translate(8750.4,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(9139.4,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mo" transform="translate(9715.4,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msup" transform="translate(10160.1,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(605,363) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(11239.3,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msup" transform="translate(11684,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(300.6,16) translate(-250 0)"><path data-c="5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z"></path></g></g></g><g data-mml-node="mi" transform="translate(523,363) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(12681.3,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></span></li>
<li><span class="math inline"><em>l</em>(<em>h</em>, <em>x</em><sup><em>n</em></sup>, <em>ŷ</em><sup><em>n</em></sup>)</span> means <span class="math inline"><em>I</em>(<em>f</em><sub><em>h</em></sub>(<em>x</em><sup><em>n</em></sup>) ≠ <em>ŷ</em><sup><em>n</em></sup>)</span>: if <span class="math inline"><em>f</em><sub><em>h</em></sub>(<em>x</em><sup><em>n</em></sup>) ≠ <em>ŷ</em><sup><em>n</em></sup></span>, output 1, otherwise output 0.</li>
<li>Of course can choose cross entropy instead.</li>
</ul></li>
</ul>
<h3 id="training-examples">Training Examples</h3>
<ul>
<li>If we can collect all Pokémons and Digimons in the universe <span class="math inline">𝒟<sub><em>a</em><em>l</em><em>l</em></sub></span>, we can find the best threshold <span class="math inline"><em>h</em><sup><em>a</em><em>l</em><em>l</em></sup></span>
<ul>
<li><span class="math inline"><em>h</em><sup><em>a</em><em>l</em><em>l</em></sup> = 𝒶𝓇ℊ<em>m</em><em>i</em><em>n</em><sub><em>h</em></sub><em>L</em>(<em>h</em>, 𝒟<sub><em>a</em><em>l</em><em>l</em></sub>)</span></li>
</ul></li>
<li>We can only collect some examples <span class="math inline">𝒟<sub><em>t</em><em>r</em><em>a</em><em>i</em><em>n</em></sub></span>
<ul>
<li><span class="math inline">𝒟<sub>𝓉𝓇𝒶𝒾𝓃</sub> = {(<em>x</em><sup>1</sup>, <em>ŷ</em><sup>1</sup>), (<em>x</em><sup>2</sup>, <em>ŷ</em><sup>2</sup>), ..., (<em>x</em><sup><em>N</em></sup>, <em>ŷ</em><sup><em>N</em></sup>)}</span>, <span class="math inline">(<em>x</em><sup><em>N</em></sup>, <em>ŷ</em><sup><em>N</em></sup>) ∼ 𝒟<sub><em>a</em><em>l</em><em>l</em></sub></span></li>
<li><span class="math inline"><em>h</em><sup><em>t</em><em>r</em><em>a</em><em>i</em><em>n</em></sup> = 𝒶𝓇ℊ<em>m</em><em>i</em><em>n</em><sub><em>h</em></sub><em>L</em>(<em>h</em>, 𝒟<sub><em>t</em><em>r</em><em>a</em><em>i</em><em>n</em></sub>)</span></li>
<li><span class="math inline">𝒟<sub>𝓉𝓇𝒶𝒾𝓃</sub></span> is independently and identically distributed ( i.i.d.)</li>
</ul></li>
<li>We hope <span class="math inline"><em>L</em>(<em>h</em><sup><em>t</em><em>r</em><em>a</em><em>i</em><em>n</em></sup>, 𝒟<sub><em>a</em><em>l</em><em>l</em></sub>)</span> and <span class="math inline"><em>L</em>(<em>h</em><sup><em>a</em><em>l</em><em>l</em></sup>, 𝒟<sub><em>a</em><em>l</em><em>l</em></sub>)</span> are close.
<ul>
<li>We want <span class="math inline"><em>L</em>(<em>h</em><sup><em>t</em><em>r</em><em>a</em><em>i</em><em>n</em></sup>, 𝒟<sub><em>a</em><em>l</em><em>l</em></sub>) − <em>L</em>(<em>h</em><sup><em>a</em><em>l</em><em>l</em></sup>, 𝒟<sub><em>a</em><em>l</em><em>l</em></sub>) ≤ <em>δ</em></span></li>
<li>So <span class="math inline">𝒟<sub><em>t</em><em>r</em><em>a</em><em>i</em><em>n</em></sub></span> has to fulfill: <span class="math inline">∀<em>h</em> ∈ ℋ</span>, <span class="math inline">|<em>L</em>(<em>h</em>, 𝒟<sub><em>t</em><em>r</em><em>a</em><em>i</em><em>n</em></sub>) − <em>L</em>(<em>h</em>, 𝒟<sub><em>a</em><em>l</em><em>l</em></sub>)| ≤ <em>ϵ</em></span>, <span class="math inline"><em>ϵ</em> = <em>δ</em>/2</span></li>
</ul></li>
</ul>
<h3 id="probability-of-failure">Probability of Failure</h3>
<blockquote>
<p>The following discussion is <strong>model-agnostic.</strong></p>
<p>In the following discussion, we don’t have assumption about <strong>data distribution.</strong></p>
<p>In the following discussion, we can use any <strong>loss function.</strong></p>
</blockquote>
<p>Each point is a training set.</p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250531143321759.png" srcset="/img/loading.gif" lazyload alt="image-20250531143321759"><figcaption aria-hidden="true">image-20250531143321759</figcaption>
</figure>
<h4 id="hoeffdings-inequality">Hoeffding’s Inequality:</h4>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250531143803606.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="model-complexity">Model Complexity</h4>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250807144723423.png" srcset="/img/loading.gif" lazyload></p>
<p>What if the parameters are continuous?</p>
<ul>
<li><em><u>Answer 1</u></em> : Everything that happens in a computer is discrete.</li>
<li><em><u>Answer 2</u></em> : VC dimension</li>
</ul>
<p>Why don’t we simply use a very small <span class="math inline">|ℋ|</span> ?</p>
<ul>
<li>smaller <span class="math inline">|ℋ|</span> means fewer candidates in <span class="math inline"><em>h</em> ∈ ℋ</span></li>
</ul>
<h4 id="tradeoff-of-model-complexity">Tradeoff of Model Complexity</h4>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250531144524458.png" srcset="/img/loading.gif" lazyload></p>
<p>How to find best balance? <strong>DEEP LEARNING</strong>.</p>
<h2 id="homework-2-framewise-phoneme-prediction-from-speech">Homework 2: Framewise phoneme prediction from speech</h2>
<ol type="1">
<li>Data Preprocessing: Extract MFCC features from raw waveform</li>
<li>Classification: Perform framewise phoneme classification using pre-extracted MFCC features</li>
</ol>
<h3 id="report-questions">Report Questions</h3>
<ol type="1">
<li>Implement 2 models with approximately the same number of parameters, (A) one narrower and deeper (e.g.&nbsp;hidden_layers=6, hidden_dim=1024) and (B) the other wider and shallower (e.g.&nbsp;hidden_layers=2, hidden_dim=1700). Report training/validation accuracies for both models.</li>
</ol>
<ul>
<li>hidden_layers=6, hidden_dim=1024: <strong>acc 0.47458</strong></li>
<li>hidden_layers=2, hidden_dim=1700: <strong>acc 0.47491</strong></li>
</ul>
<ol start="2" type="1">
<li>Add dropout layers, and report training/validation accuracies with dropout rates equal to (A) 0.25/(B) 0.5/(C) 0.75 respectively.</li>
</ol>
<ul>
<li><ol type="A">
<li>0.25: <strong>acc 0.45756</strong></li>
</ol></li>
<li><ol start="2" type="A">
<li>0.5: <strong>acc 0.44790</strong></li>
</ol></li>
<li><ol start="3" type="A">
<li>0.75: <strong>acc 0.42627</strong></li>
</ol></li>
</ul>
<p>合适的<hanla></hanla>Dropout<hanla></hanla>能够防止过拟合，Dropout<hanla></hanla>过大反而会影响模型的准确率。</p>
<p>Code: <a target="_blank" rel="noopener" href="https://github.com/Striver98/Machine-Learning-Homework/blob/master/HW2.py">link</a></p>
<p>结果：</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250531174912583.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="images-input">Images Input</h1>
<h2 id="convolutional-neural-networkcnn">Convolutional Neural Network(CNN)</h2>
<blockquote>
<p>CNN is a Network Architecture designed for Image.</p>
</blockquote>
<h3 id="simplification-1-receptive-field">Simplification 1: Receptive Field</h3>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250601142954885.png" srcset="/img/loading.gif" lazyload alt="image-20250601142954885"><figcaption aria-hidden="true">image-20250601142954885</figcaption>
</figure>
<ul>
<li>Each receptive field has a set of neurons (e.g., 64 neurons).</li>
<li>The receptive fields cover the whole image.</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250601143541190.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250601143426031.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="simplification-2-parameter-sharing">Simplification 2: Parameter Sharing</h3>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250601143908344.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<p>Two neurons with the same receptive field would not share parameters.</p>
</blockquote>
<ul>
<li>Each receptive field has a set of neurons (e.g., 64 neurons).</li>
<li>Each receptive field has the neurons with the same set of parameters.</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250601143943607.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="convolutional-layer">Convolutional Layer</h3>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250601144635213.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250601144032614.png" srcset="/img/loading.gif" lazyload></p>
<p>Consider channel = 1(black and white image):</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250601144215207.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<p>The values in the filters are unknown parameters.</p>
</blockquote>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250601144408840.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<p>Do the same process for every filter, then we get the <strong>Feature Map.</strong></p>
</blockquote>
<h3 id="pooling">Pooling</h3>
<ul>
<li>Subsampling the pixels will not change the object.</li>
<li>Max Pooling, Mean Pooling…</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250601144808829.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="summary-1">Summary</h3>
<p>The Whole CNN</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250601144958923.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="homework-3-image-classification">Homework 3: Image Classification</h2>
<ol type="1">
<li>Solve image classification with <strong>convolutional neural networks</strong>.</li>
<li>Improve the performance with <strong>data augmentations</strong>.</li>
<li>Understand popular image model techniques such as <strong>residual</strong>.</li>
</ol>
<h3 id="baseline">Baseline</h3>
<ul>
<li>Simple : 0.50099</li>
<li>Medium : 0.73207 Training Augmentation + Train Longer</li>
<li>Strong : 0.81872 Training Augmentation + Model Design + Train Looonger (+Cross Validation + Ensemble)</li>
<li>Boss : 0.88446 Training Augmentation + Model Design +Test Time Augmentation + Train Looonger (+ Cross Validation + Ensemble)</li>
</ul>
<h3 id="record">Record</h3>
<h4 id="simple-baseline">Simple Baseline</h4>
<p>Just sample code</p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250602123656072.png" srcset="/img/loading.gif" lazyload alt="image-20250602123656072"><figcaption aria-hidden="true">image-20250602123656072</figcaption>
</figure>
<h4 id="medium-baseline">Medium Baseline</h4>
<ol type="1">
<li>Training Augmentation</li>
</ol>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Normally, We don't need augmentations in testing and validation.</span><br><span class="hljs-comment"># All we need here is to resize the PIL image and transform it into Tensor.</span><br>test_tfm = transforms.Compose([<br>    transforms.Resize((<span class="hljs-number">128</span>, <span class="hljs-number">128</span>)),<br>    transforms.ToTensor(),<br>])<br><br><span class="hljs-comment"># However, it is also possible to use augmentation in the testing phase.</span><br><span class="hljs-comment"># You may use train_tfm to produce a variety of images and then test using ensemble methods</span><br>train_tfm = transforms.Compose([<br>    <span class="hljs-comment"># Resize the image into a fixed shape (height = width = 128)</span><br>    transforms.Resize((<span class="hljs-number">128</span>, <span class="hljs-number">128</span>)),<br>    <span class="hljs-comment"># You may add some transforms here.</span><br>    <span class="hljs-comment"># 95%<hanla></hanla>概率做随机增强（TrivialAugmentWide），10%<hanla></hanla>概率保持原图.</span><br>    transforms.RandomChoice(transforms=[<br>        <span class="hljs-comment"># Apply TrivialAugmentWide data augmentation method</span><br>        transforms.TrivialAugmentWide(),<br><br>        <span class="hljs-comment"># Return original image</span><br>        transforms.Lambda(<span class="hljs-keyword">lambda</span> x: x),<br>    ],<br>                            p=[<span class="hljs-number">0.95</span>, <span class="hljs-number">0.05</span>]),<br>    <span class="hljs-comment"># ToTensor() should be the last one of the transforms.</span><br>    transforms.ToTensor(),<br>])<br></code></pre></td></tr></tbody></table></figure>
<ol start="2" type="1">
<li>Configuration</li>
</ol>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># The number of training epochs and patience.</span><br>n_epochs = <span class="hljs-number">20</span><br>patience = <span class="hljs-number">10</span> <span class="hljs-comment"># If no improvement in 'patience' epochs, early stop</span><br><br>optimizer = torch.optim.Adam(model.parameters(), lr=<span class="hljs-number">3e-4</span>, weight_decay=<span class="hljs-number">1e-5</span>) <br></code></pre></td></tr></tbody></table></figure>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250602141259096.png" srcset="/img/loading.gif" lazyload></p>
<p>增加<hanla></hanla>epoch<hanla></hanla>就可以达到<hanla></hanla>baseline，但是<hanla></hanla>colab<hanla></hanla>的时限到了。</p>
<h4 id="strong-baseline">Strong Baseline</h4>
<p>采用<hanla></hanla><code>ReduceLROnPlateau</code>, 验证集指标无提升时自动降低<hanla></hanla>lr</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Initialize optimizer, you may fine-tune some hyperparameters such as learning rate on your own.</span><br>optimizer = torch.optim.Adam(model.parameters(), lr=<span class="hljs-number">3e-4</span>, weight_decay=<span class="hljs-number">1e-5</span>)<br>scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=<span class="hljs-string">'max'</span>, factor=<span class="hljs-number">0.8</span>, patience=patience/<span class="hljs-number">2</span>,threshold=<span class="hljs-number">0.05</span>)<br><span class="hljs-comment"># 在每个<hanla></hanla>epoch<hanla></hanla>验证后加上<hanla></hanla></span><br>scheduler.step(best_acc)<br></code></pre></td></tr></tbody></table></figure>
<p>完整<hanla></hanla>Code：<a target="_blank" rel="noopener" href="https://github.com/Striver98/Machine-Learning-Homework/blob/master/HW3.py">link</a></p>
<p>结果：</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250602170448517.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="self-attention">Self-attention</h1>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250604223511224.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250604223735172.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="basic-mechanism">Basic Mechanism</h2>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250604223913217.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250604224019289.png" srcset="/img/loading.gif" lazyload></p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250604224326832.png" srcset="/img/loading.gif" lazyload alt="image-20250604224326832"><figcaption aria-hidden="true">image-20250604224326832</figcaption>
</figure>
<ul>
<li><span class="math inline"><em>α</em><sub>1, <em>i</em></sub><sup>′</sup> = <em>e</em><em>x</em><em>p</em>(<em>α</em><sub>1, <em>i</em></sub>)/∑<sub><em>j</em></sub><em>e</em><em>x</em><em>p</em>(<em>α</em><sub>1, <em>j</em></sub>)</span></li>
<li><span class="math inline"><em>b</em><sup>1</sup> = ∑<sub><em>i</em></sub><em>α</em><sub>1, <em>i</em></sub><sup>′</sup><em>v</em><sup><em>i</em></sup></span></li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250604224542613.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250604224556942.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250604224610954.png" srcset="/img/loading.gif" lazyload></p>
<p>Summary:</p>
<ul>
<li><span class="math inline"><em>Q</em> = <em>W</em><sup><em>q</em></sup><em>I</em></span>, <span class="math inline"><em>K</em> = <em>W</em><sup><em>k</em></sup><em>I</em></span>, <span class="math inline"><em>V</em> = <em>W</em><sup><em>v</em></sup><em>I</em></span></li>
<li><span class="math inline"><em>A</em> = <em>K</em><sup><em>T</em></sup><em>Q</em></span>, <span class="math inline"><em>A</em><sup>′</sup> = <em>S</em><em>o</em><em>f</em><em>t</em><em>M</em><em>a</em><em>x</em>(<em>A</em>)</span></li>
<li><span class="math inline"><em>O</em> = <em>V</em><em>A</em><sup>′</sup></span></li>
<li><span class="math inline"><em>W</em><sup><em>q</em></sup></span> , <span class="math inline"><em>W</em><sup><em>k</em></sup></span> and <span class="math inline"><em>W</em><sup><em>v</em></sup></span>​ are parameters to be learned.</li>
</ul>
<h2 id="multi-head-self-attention">Multi-head Self-attention</h2>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250604225247315.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250604225312793.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="positional-encoding">Positional Encoding</h2>
<ul>
<li>No position information in self-attention.</li>
<li>Each position has a unique positional vector <span class="math inline"><em>e</em><sup><em>i</em></sup></span></li>
<li><strong>hand-crafted</strong></li>
<li><strong>learned from data</strong></li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250604225504166.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250604225527303.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="homework-4-speaker-identification">Homework 4: Speaker Identification</h2>
<h3 id="task-multiclass-classification">Task: Multiclass Classification</h3>
<p>Predict speaker class from given speech.</p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250605151544966.png" srcset="/img/loading.gif" lazyload alt="image-20250605151544966"><figcaption aria-hidden="true">image-20250605151544966</figcaption>
</figure>
<h3 id="simple-baseline-1">Simple Baseline</h3>
<ul>
<li>Build a self-attention network to classify speakers with sample code.</li>
<li>Simple public baseline: 0.66025</li>
<li>Estimate training time: 30~40 mins on Colab.</li>
</ul>
<p>Output:</p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250605151737979.png" srcset="/img/loading.gif" lazyload alt="image-20250605151737979"><figcaption aria-hidden="true">image-20250605151737979</figcaption>
</figure>
<h3 id="medium-baseline-1">Medium Baseline</h3>
<ul>
<li>Modify the parameters of the transformer modules in the sample code.</li>
<li>Medium public baseline: 0.81750</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Classifier</span>(nn.Module):<br>	<span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_model=<span class="hljs-number">512</span>, n_spks=<span class="hljs-number">600</span>, dropout=<span class="hljs-number">0.2</span></span>):<br>		<span class="hljs-built_in">super</span>().__init__()<br>		<span class="hljs-comment"># Project the dimension of features from that of input into d_model.</span><br>		<span class="hljs-variable language_">self</span>.prenet = nn.Linear(<span class="hljs-number">40</span>, d_model)<br>		<span class="hljs-comment"># <span class="hljs-doctag">TODO:</span></span><br>		<span class="hljs-comment">#   Change Transformer to Conformer.</span><br>		<span class="hljs-comment">#   https://arxiv.org/abs/2005.08100</span><br>		<span class="hljs-variable language_">self</span>.encoder_layer = nn.TransformerEncoderLayer(<br>			d_model=d_model, dim_feedforward=<span class="hljs-number">256</span>, nhead=<span class="hljs-number">32</span><br>		)<br>		<span class="hljs-variable language_">self</span>.encoder = nn.TransformerEncoder(<span class="hljs-variable language_">self</span>.encoder_layer, num_layers=<span class="hljs-number">2</span>)<br><br>		<span class="hljs-comment"># Project the the dimension of features from d_model into speaker nums.</span><br>		<span class="hljs-variable language_">self</span>.pred_layer = nn.Sequential(<br>			nn.Linear(d_model, <span class="hljs-number">2</span> * d_model),<br>			nn.ReLU(),<br>			nn.Dropout(dropout),<br>			nn.Linear(<span class="hljs-number">2</span> * d_model, n_spks),<br>		)<br></code></pre></td></tr></tbody></table></figure>
<p>Output:</p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250605154149324.png" srcset="/img/loading.gif" lazyload alt="image-20250605154149324"><figcaption aria-hidden="true">image-20250605154149324</figcaption>
</figure>
<h3 id="strong-baseline-1">Strong Baseline</h3>
<ul>
<li>Construct <code>Conformer</code>, which is a variety of Transformer.</li>
<li>Strong public baseline: 0.88500</li>
</ul>
<p>Output:</p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250605162715429.png" srcset="/img/loading.gif" lazyload alt="image-20250605162715429"><figcaption aria-hidden="true">image-20250605162715429</figcaption>
</figure>
<p>Code: <a target="_blank" rel="noopener" href="https://github.com/Striver98/Machine-Learning-Homework/blob/master/HW4.py">link</a></p>
<h3 id="boss-baseline">Boss Baseline</h3>
<ul>
<li>Implement Self-Attention Pooling &amp; Additive Margin Softmax to further boost the performance.</li>
<li>Self-Attention Pooling.</li>
<li>Boss baseline : 0.93175</li>
</ul>
<p>实际上我把<hanla></hanla>epoch<hanla></hanla>放大到<hanla></hanla>200,000<hanla></hanla>就达到了<hanla></hanla>Boss Baseline.</p>
<p>Output:</p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250605214707665.png" srcset="/img/loading.gif" lazyload alt="image-20250605214707665"><figcaption aria-hidden="true">image-20250605214707665</figcaption>
</figure>
<h1 id="sequence-to-sequence-seq2seq">Sequence to sequence (Seq2seq)</h1>
<blockquote>
<ul>
<li>Input a sequence, output a sequence.</li>
<li>The output length is determined by model.</li>
</ul>
</blockquote>
<h2 id="batch-normalization">Batch Normalization</h2>
<p><span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.798ex;" xmlns="http://www.w3.org/2000/svg" width="9.281ex" height="3.241ex" role="img" focusable="false" viewBox="0 -1079.9 4102 1432.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="mo" transform="translate(288.1,332) translate(-250 0)"><path data-c="7E" d="M179 251Q164 251 151 245T131 234T111 215L97 227L83 238Q83 239 95 253T121 283T142 304Q165 318 187 318T253 300T320 282Q335 282 348 288T368 299T388 318L402 306L416 295Q375 236 344 222Q330 215 313 215Q292 215 248 233T179 251Z"></path></g></g></g><g data-mml-node="mi" transform="translate(498,363) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(1069.7,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mfrac" transform="translate(2125.5,0)"><g data-mml-node="mrow" transform="translate(220,492.7) scale(0.707)"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="mi" transform="translate(498,363) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(792,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(1570,0)"><path data-c="1D707" d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"></path></g></g><g data-mml-node="mi" transform="translate(786.4,-345) scale(0.707)"><path data-c="1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path></g><rect width="1736.5" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container></span></p>
<p><span class="math inline"><em>ẑ</em><sup><em>i</em></sup> = <em>γ</em>⨀<em>z̃</em><sup><em>i</em></sup> + <em>β</em></span></p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608114413816.png" srcset="/img/loading.gif" lazyload alt="image-20250608114413816"><figcaption aria-hidden="true">image-20250608114413816</figcaption>
</figure>
<p>How Does Batch Normalization Help Optimization?</p>
<p>Experimental results (and theoretically analysis) support batch normalization change the landscape of <strong>error surface</strong>.</p>
<h2 id="transformer">Transformer</h2>
<h3 id="the-applications-of-seq2seq">the Applications of Seq2seq</h3>
<ol type="1">
<li>Chatbot</li>
</ol>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608115728748.png" srcset="/img/loading.gif" lazyload alt="image-20250608115728748"><figcaption aria-hidden="true">image-20250608115728748</figcaption>
</figure>
<ol start="2" type="1">
<li>Question Answering</li>
</ol>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608115818813.png" srcset="/img/loading.gif" lazyload alt="image-20250608115818813"><figcaption aria-hidden="true">image-20250608115818813</figcaption>
</figure>
<ol start="3" type="1">
<li>Syntactic Parsing</li>
</ol>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608115906244.png" srcset="/img/loading.gif" lazyload alt="image-20250608115906244"><figcaption aria-hidden="true">image-20250608115906244</figcaption>
</figure>
<ol start="4" type="1">
<li>Multi-label Classification</li>
</ol>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608115939826.png" srcset="/img/loading.gif" lazyload alt="image-20250608115939826"><figcaption aria-hidden="true">image-20250608115939826</figcaption>
</figure>
<h3 id="the-architecture-of-transformer">The Architecture of Transformer</h3>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608120043049.png" srcset="/img/loading.gif" lazyload alt="image-20250608120043049"><figcaption aria-hidden="true">image-20250608120043049</figcaption>
</figure>
<h4 id="encoder">Encoder</h4>
<blockquote>
<p>Multi-Head Attention + Residue&amp;Layer Norm + Fully-connected Network + Residue&amp;Layer Norm</p>
</blockquote>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608120233383.png" srcset="/img/loading.gif" lazyload alt="image-20250608120233383"><figcaption aria-hidden="true">image-20250608120233383</figcaption>
</figure>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608120252454.png" srcset="/img/loading.gif" lazyload alt="image-20250608120252454"><figcaption aria-hidden="true">image-20250608120252454</figcaption>
</figure>
<h4 id="decoder">Decoder</h4>
<ol type="1">
<li><strong>Autoregressive (AT): 自回归</strong></li>
</ol>
<blockquote>
<p>Speech Recognition as example</p>
</blockquote>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608120931491.png" srcset="/img/loading.gif" lazyload alt="image-20250608120931491"><figcaption aria-hidden="true">image-20250608120931491</figcaption>
</figure>
<p><strong>Masked Self attention</strong></p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608125347892.png" srcset="/img/loading.gif" lazyload alt="image-20250608125347892"><figcaption aria-hidden="true">image-20250608125347892</figcaption>
</figure>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608125418850.png" srcset="/img/loading.gif" lazyload alt="image-20250608125418850"><figcaption aria-hidden="true">image-20250608125418850</figcaption>
</figure>
<p><strong>Adding “Stop Token”</strong></p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608130028061.png" srcset="/img/loading.gif" lazyload alt="image-20250608130028061"><figcaption aria-hidden="true">image-20250608130028061</figcaption>
</figure>
<ol start="2" type="1">
<li><strong>Non-autoregressive (NAT)</strong></li>
</ol>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608131148029.png" srcset="/img/loading.gif" lazyload alt="image-20250608131148029"><figcaption aria-hidden="true">image-20250608131148029</figcaption>
</figure>
<ul>
<li>How to decide the output length for NAT decoder?
<ul>
<li>Another predictor for output length</li>
<li>Output a very long sequence, ignore tokens after END</li>
</ul></li>
<li>Advantage: parallel, more stable generation (e.g., TTS)</li>
<li>NAT is usually worse than AT (why? Multi-modality)</li>
</ul>
<h4 id="training">Training</h4>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608131442819.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<p>The Decoder’s Input is different between Training process and Testing process.</p>
</blockquote>
<h2 id="self-attention-variances">Self-attention Variances</h2>
<h3 id="fixed-pattern">Fixed Pattern</h3>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608132025796.png" srcset="/img/loading.gif" lazyload alt="image-20250608132025796"><figcaption aria-hidden="true">image-20250608132025796</figcaption>
</figure>
<p>Window Attention, Stride Attention, Global Attention…</p>
<h3 id="learnable-patterns">Learnable Patterns</h3>
<blockquote>
<p>make Machine learn how to choose patterns.</p>
</blockquote>
<h4 id="clustering">Clustering</h4>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608132300975.png" srcset="/img/loading.gif" lazyload alt="image-20250608132300975"><figcaption aria-hidden="true">image-20250608132300975</figcaption>
</figure>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608132326352.png" srcset="/img/loading.gif" lazyload alt="image-20250608132326352"><figcaption aria-hidden="true">image-20250608132326352</figcaption>
</figure>
<h4 id="sinkhorn-sorting-network">Sinkhorn Sorting Network</h4>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608132423910.png" srcset="/img/loading.gif" lazyload alt="image-20250608132423910"><figcaption aria-hidden="true">image-20250608132423910</figcaption>
</figure>
<blockquote>
<p>simplified version</p>
</blockquote>
<h3 id="reduce-number-of-keys">Reduce Number of Keys</h3>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608132536964.png" srcset="/img/loading.gif" lazyload alt="image-20250608132536964"><figcaption aria-hidden="true">image-20250608132536964</figcaption>
</figure>
<p><strong>Compressed Attention</strong></p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608132615627.png" srcset="/img/loading.gif" lazyload alt="image-20250608132615627"><figcaption aria-hidden="true">image-20250608132615627</figcaption>
</figure>
<p><strong>Linformer</strong></p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608132709721.png" srcset="/img/loading.gif" lazyload alt="image-20250608132709721"><figcaption aria-hidden="true">image-20250608132709721</figcaption>
</figure>
<h3 id="change-the-order-of-matrix-multiplication">Change the order of matrix multiplication</h3>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608132932332.png" srcset="/img/loading.gif" lazyload alt="image-20250608132932332"><figcaption aria-hidden="true">image-20250608132932332</figcaption>
</figure>
<h2 id="homework-5-machine-translation">Homework 5: Machine Translation</h2>
<ul>
<li>In this homework, we’ll translate English to Traditional Chinese.</li>
<li>Since sentences are with different length in different languages, the seq2seq framework is applied to this task.</li>
</ul>
<h3 id="training-datasets">Training datasets</h3>
<ul>
<li>Paired data
<ul>
<li>TED2020: TED talks with transcripts translated by a global community of volunteers to more than 100 language</li>
<li>We will use (en, zh-tw) aligned pairs</li>
</ul></li>
<li>Monolingual data(单语言数据)
<ul>
<li>More TED talks in traditional Chinese</li>
</ul></li>
</ul>
<h3 id="evaluation">Evaluation</h3>
<p><span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.777ex;" xmlns="http://www.w3.org/2000/svg" width="35.469ex" height="2.982ex" role="img" focusable="false" viewBox="0 -974.6 15677.3 1317.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path></g><g data-mml-node="mi" transform="translate(759,0)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mi" transform="translate(1440,0)"><path data-c="1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path></g><g data-mml-node="mi" transform="translate(2204,0)"><path data-c="1D448" d="M107 637Q73 637 71 641Q70 643 70 649Q70 673 81 682Q83 683 98 683Q139 681 234 681Q268 681 297 681T342 682T362 682Q378 682 378 672Q378 670 376 658Q371 641 366 638H364Q362 638 359 638T352 638T343 637T334 637Q295 636 284 634T266 623Q265 621 238 518T184 302T154 169Q152 155 152 140Q152 86 183 55T269 24Q336 24 403 69T501 205L552 406Q599 598 599 606Q599 633 535 637Q511 637 511 648Q511 650 513 660Q517 676 519 679T529 683Q532 683 561 682T645 680Q696 680 723 681T752 682Q767 682 767 672Q767 650 759 642Q756 637 737 637Q666 633 648 597Q646 592 598 404Q557 235 548 205Q515 105 433 42T263 -22Q171 -22 116 34T60 167V183Q60 201 115 421Q164 622 164 628Q164 635 107 637Z"></path></g><g data-mml-node="mo" transform="translate(3248.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(4304.6,0)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path></g><g data-mml-node="mi" transform="translate(5063.6,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(6036.8,0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path></g><g data-mml-node="mi" transform="translate(6537,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(7003,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(7575,0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mo" transform="translate(8078,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="munderover" transform="translate(8467,0)"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path></g><g data-mml-node="TeXAtom" transform="translate(1089,477.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(1089,-285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(600,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1378,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="msub" transform="translate(11100.6,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mi" transform="translate(749,-150) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mi" transform="translate(12323.9,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(12621.9,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(13106.9,0)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="msub" transform="translate(13583.9,0)"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mi" transform="translate(536,-150) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="msup" transform="translate(14594.1,0)"><g data-mml-node="mo"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="TeXAtom" transform="translate(422,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mfrac"><g data-mml-node="mn" transform="translate(255.4,394) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mi" transform="translate(220,-345) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><rect width="624.3" height="60" x="120" y="220"></rect></g></g></g></g></g></svg></mjx-container></span></p>
<ul>
<li><p>Modified n-gram precision (<span class="math inline"><em>n</em></span> = 1~4), <span class="math inline"><em>N</em></span> is the <strong>maximum</strong> value of <span class="math inline"><em>n</em></span></p></li>
<li><p><strong>Brevity penalty</strong>: penalizes short hypotheses</p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608133703142.png" srcset="/img/loading.gif" lazyload alt="image-20250608133703142"><figcaption aria-hidden="true">image-20250608133703142</figcaption>
</figure>
<ul>
<li>c is the hypothesis length, r is the reference length.</li>
</ul></li>
<li><p><span class="math inline"><em>p</em><sub><em>n</em></sub></span> is the <strong>precision</strong> of n-gram</p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608134542090.png" srcset="/img/loading.gif" lazyload alt="image-20250608134542090"><figcaption aria-hidden="true">image-20250608134542090</figcaption>
</figure></li>
</ul>
<h3 id="workflow">Workflow</h3>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608134647240.png" srcset="/img/loading.gif" lazyload alt="image-20250608134647240"><figcaption aria-hidden="true">image-20250608134647240</figcaption>
</figure>
<ol type="1">
<li>Preprocessing
<ul>
<li>download raw data</li>
<li>clean and normalize</li>
<li>remove bad data (too long/short)</li>
<li>tokenization</li>
</ul></li>
<li>Training
<ul>
<li>initialize a model</li>
<li>train it with training data</li>
</ul></li>
<li>Testing
<ul>
<li>generate translation of test data</li>
<li>evaluate the performance</li>
</ul></li>
</ol>
<h3 id="baselines">Baselines</h3>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608135047232.png" srcset="/img/loading.gif" lazyload alt="image-20250608135047232"><figcaption aria-hidden="true">image-20250608135047232</figcaption>
</figure>
<p>HW05<hanla></hanla>是在 Judgeboi 上提交的，因此无法得到具体的分数，仅给出修改的代码。</p>
<p>PS：由于无法在 Judgeboi 上提交，所以原本打算得出在验证集上的分数，但是遇到了一个暂时无法解决的<hanla></hanla>bug。</p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608150910603.png" srcset="/img/loading.gif" lazyload alt="image-20250608150910603"><figcaption aria-hidden="true">image-20250608150910603</figcaption>
</figure>
<h3 id="simple-baseline-2">Simple Baseline</h3>
<blockquote>
<p>Running the sample code</p>
</blockquote>
<h3 id="medium-baseline-2">Medium Baseline</h3>
<p>Add learning rate scheduler and train longer</p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608150948350.png" srcset="/img/loading.gif" lazyload alt="image-20250608150948350"><figcaption aria-hidden="true">image-20250608150948350</figcaption>
</figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> math<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_rate</span>(<span class="hljs-params">d_model, step_num, warmup_step</span>):<br>    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> Change lr from constant to the equation shown above</span><br>    lr = <span class="hljs-number">1.0</span> / math.sqrt(d_model) * <span class="hljs-built_in">min</span>(<span class="hljs-number">1.0</span> / math.sqrt(step_num), step_num / (warmup_step * math.sqrt(warmup_step)))<br>    <span class="hljs-keyword">return</span> lr<br></code></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs python">config = Namespace(<br>    datadir = <span class="hljs-string">"./DATA/data-bin/ted2020"</span>,<br>    savedir = <span class="hljs-string">"./checkpoints/rnn"</span>,<br>    source_lang = src_lang,<br>    target_lang = tgt_lang,<br><br>    <span class="hljs-comment"># cpu threads when fetching &amp; processing data.</span><br>    num_workers=<span class="hljs-number">2</span>,<br>    <span class="hljs-comment"># batch size in terms of tokens. gradient accumulation increases the effective batchsize.</span><br>    max_tokens=<span class="hljs-number">8192</span>,<br>    accum_steps=<span class="hljs-number">2</span>,<br><br>    <span class="hljs-comment"># the lr s calculated from Noam lr scheduler. you can tune the maximum lr by this factor.</span><br>    lr_factor=<span class="hljs-number">2.</span>,<br>    lr_warmup=<span class="hljs-number">4000</span>,<br><br>    <span class="hljs-comment"># clipping gradient norm helps alleviate gradient exploding</span><br>    clip_norm=<span class="hljs-number">1.0</span>,<br><br>    <span class="hljs-comment"># maximum epochs for training (Medium)</span><br>    max_epoch=<span class="hljs-number">30</span>,<br>    start_epoch=<span class="hljs-number">1</span>,<br><br>    <span class="hljs-comment"># beam size for beam search</span><br>    beam=<span class="hljs-number">5</span>,<br>    <span class="hljs-comment"># generate sequences of maximum length ax + b, where x is the source length</span><br>    max_len_a=<span class="hljs-number">1.2</span>,<br>    max_len_b=<span class="hljs-number">10</span>,<br>    <span class="hljs-comment"># when decoding, post process sentence by removing sentencepiece symbols and jieba tokenization.</span><br>    post_process = <span class="hljs-string">"sentencepiece"</span>,<br><br>    <span class="hljs-comment"># checkpoints</span><br>    keep_last_epochs=<span class="hljs-number">5</span>,<br>    resume=<span class="hljs-literal">None</span>, <span class="hljs-comment"># if resume from checkpoint name (under config.savedir)</span><br><br>    <span class="hljs-comment"># logging</span><br>    use_wandb=<span class="hljs-literal">False</span>,<br>)<br></code></pre></td></tr></tbody></table></figure>
<h3 id="strong-baseline-2">Strong Baseline</h3>
<p>Switch to Transformer and tuning hyperparameter</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># # HINT: transformer architecture</span><br><span class="hljs-keyword">from</span> fairseq.models.transformer <span class="hljs-keyword">import</span> (<br>    TransformerEncoder,<br>    TransformerDecoder,<br>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">build_model</span>(<span class="hljs-params">args, task</span>):<br>    <span class="hljs-string">""" build a model instance based on hyperparameters """</span><br>    src_dict, tgt_dict = task.source_dictionary, task.target_dictionary<br><br>    <span class="hljs-comment"># token embeddings</span><br>    encoder_embed_tokens = nn.Embedding(<span class="hljs-built_in">len</span>(src_dict), args.encoder_embed_dim, src_dict.pad())<br>    decoder_embed_tokens = nn.Embedding(<span class="hljs-built_in">len</span>(tgt_dict), args.decoder_embed_dim, tgt_dict.pad())<br><br>    <span class="hljs-comment"># encoder decoder</span><br>    <span class="hljs-comment"># HINT: <span class="hljs-doctag">TODO:</span> switch to TransformerEncoder &amp; TransformerDecoder</span><br>    <span class="hljs-comment"># encoder = RNNEncoder(args, src_dict, encoder_embed_tokens)</span><br>    <span class="hljs-comment"># decoder = RNNDecoder(args, tgt_dict, decoder_embed_tokens)</span><br>    encoder = TransformerEncoder(args, src_dict, encoder_embed_tokens)<br>    decoder = TransformerDecoder(args, tgt_dict, decoder_embed_tokens)<br><br>    <span class="hljs-comment"># sequence to sequence model</span><br>    model = Seq2Seq(args, encoder, decoder)<br><br>    <span class="hljs-comment"># initialization for seq2seq model is important, requires extra handling</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">init_params</span>(<span class="hljs-params">module</span>):<br>        <span class="hljs-keyword">from</span> fairseq.modules <span class="hljs-keyword">import</span> MultiheadAttention<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(module, nn.Linear):<br>            module.weight.data.normal_(mean=<span class="hljs-number">0.0</span>, std=<span class="hljs-number">0.02</span>)<br>            <span class="hljs-keyword">if</span> module.bias <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                module.bias.data.zero_()<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(module, nn.Embedding):<br>            module.weight.data.normal_(mean=<span class="hljs-number">0.0</span>, std=<span class="hljs-number">0.02</span>)<br>            <span class="hljs-keyword">if</span> module.padding_idx <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                module.weight.data[module.padding_idx].zero_()<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(module, MultiheadAttention):<br>            module.q_proj.weight.data.normal_(mean=<span class="hljs-number">0.0</span>, std=<span class="hljs-number">0.02</span>)<br>            module.k_proj.weight.data.normal_(mean=<span class="hljs-number">0.0</span>, std=<span class="hljs-number">0.02</span>)<br>            module.v_proj.weight.data.normal_(mean=<span class="hljs-number">0.0</span>, std=<span class="hljs-number">0.02</span>)<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(module, nn.RNNBase):<br>            <span class="hljs-keyword">for</span> name, param <span class="hljs-keyword">in</span> module.named_parameters():<br>                <span class="hljs-keyword">if</span> <span class="hljs-string">"weight"</span> <span class="hljs-keyword">in</span> name <span class="hljs-keyword">or</span> <span class="hljs-string">"bias"</span> <span class="hljs-keyword">in</span> name:<br>                    param.data.uniform_(-<span class="hljs-number">0.1</span>, <span class="hljs-number">0.1</span>)<br><br>    <span class="hljs-comment"># weight initialization</span><br>    model.apply(init_params)<br>    <span class="hljs-keyword">return</span> model<br></code></pre></td></tr></tbody></table></figure>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/93e50799eeb09416eb1e1727bcf09bd2.png" srcset="/img/loading.gif" lazyload alt="image-20231115135033382"><figcaption aria-hidden="true">image-20231115135033382</figcaption>
</figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Follow transformer-base </span><br>arch_args = Namespace(<br>    encoder_embed_dim=<span class="hljs-number">256</span>,<br>    encoder_ffn_embed_dim=<span class="hljs-number">512</span>,<br>    encoder_layers=<span class="hljs-number">1</span>,<br>    decoder_embed_dim=<span class="hljs-number">256</span>,<br>    decoder_ffn_embed_dim=<span class="hljs-number">1024</span>,<br>    decoder_layers=<span class="hljs-number">1</span>,<br>    share_decoder_input_output_embed=<span class="hljs-literal">True</span>,<br>    dropout=<span class="hljs-number">0.3</span>,<br>)<br><br><span class="hljs-comment"># HINT: these patches on parameters for Transformer</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">add_transformer_args</span>(<span class="hljs-params">args</span>):<br>    args.encoder_attention_heads=<span class="hljs-number">4</span><br>    args.encoder_normalize_before=<span class="hljs-literal">True</span><br><br>    args.decoder_attention_heads=<span class="hljs-number">4</span><br>    args.decoder_normalize_before=<span class="hljs-literal">True</span><br><br>    args.activation_fn=<span class="hljs-string">"relu"</span><br>    args.max_source_positions=<span class="hljs-number">1024</span><br>    args.max_target_positions=<span class="hljs-number">1024</span><br><br>    <span class="hljs-comment"># patches on default parameters for Transformer (those not set above)</span><br>    <span class="hljs-keyword">from</span> fairseq.models.transformer <span class="hljs-keyword">import</span> base_architecture<br>    base_architecture(arch_args)<br><br>add_transformer_args(arch_args)<br></code></pre></td></tr></tbody></table></figure>
<h3 id="boss-baseline-1">Boss Baseline</h3>
<p>Apply back-translation</p>
<ol type="1">
<li><p>Train a backward model by switching languages</p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250608152524850.png" srcset="/img/loading.gif" lazyload alt="image-20250608152524850"><figcaption aria-hidden="true">image-20250608152524850</figcaption>
</figure></li>
<li><p>Translate monolingual data with backward model to obtain synthetic data</p>
<ul>
<li>Complete TODOs in the sample code</li>
<li>All the TODOs can be completed by using commands from earlier cells</li>
</ul></li>
<li><p>Train a stronger forward model with the new data</p>
<ul>
<li>If done correctly, 30 epochs on new data should pass the baseline</li>
</ul></li>
</ol>
<h4 id="configuration-for-experiments">Configuration for experiments</h4>
<ol type="1">
<li>Set <strong>BACK_TRANSLATION</strong> to <strong>True</strong> in the <strong>configuration for experiments</strong> and run. Train a back-translation model and process the corresponding corpus.</li>
<li>Set <strong>BACK_TRANSLATION</strong> to <strong>False</strong> in the <strong>configuration for experiments</strong> and run. Train with the corpus of ted2020 and mono (back-translation).</li>
</ol>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">BACK_TRANSLATION = <span class="hljs-literal">False</span> <br><span class="hljs-keyword">if</span> BACK_TRANSLATION:<br>    config.datadir = <span class="hljs-string">"./DATA/data-bin/ted2020"</span><br>    config.savedir = <span class="hljs-string">"./checkpoints/transformer-back"</span><br>    config.source_lang, config.target_lang= tgt_lang, src_lang<br></code></pre></td></tr></tbody></table></figure>
<h4 id="model-initialization">Model Initialization</h4>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># # HINT: transformer architecture</span><br><span class="hljs-keyword">from</span> fairseq.models.transformer <span class="hljs-keyword">import</span> (<br>    TransformerEncoder,<br>    TransformerDecoder,<br>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">build_model</span>(<span class="hljs-params">args, task</span>):<br>    <span class="hljs-string">""" build a model instance based on hyperparameters """</span><br>    src_dict, tgt_dict = task.source_dictionary, task.target_dictionary<br><br>    <span class="hljs-comment"># token embeddings</span><br>    encoder_embed_tokens = nn.Embedding(<span class="hljs-built_in">len</span>(src_dict), args.encoder_embed_dim, src_dict.pad())<br>    decoder_embed_tokens = nn.Embedding(<span class="hljs-built_in">len</span>(tgt_dict), args.decoder_embed_dim, tgt_dict.pad())<br><br>    <span class="hljs-comment"># encoder decoder</span><br>    <span class="hljs-comment"># HINT: <span class="hljs-doctag">TODO:</span> switch to TransformerEncoder &amp; TransformerDecoder</span><br>    <span class="hljs-comment"># encoder = RNNEncoder(args, src_dict, encoder_embed_tokens)</span><br>    <span class="hljs-comment"># decoder = RNNDecoder(args, tgt_dict, decoder_embed_tokens)</span><br>    encoder = TransformerEncoder(args, src_dict, encoder_embed_tokens)<br>    decoder = TransformerDecoder(args, tgt_dict, decoder_embed_tokens)<br><br>    <span class="hljs-comment"># sequence to sequence model</span><br>    model = Seq2Seq(args, encoder, decoder)<br><br>    <span class="hljs-comment"># initialization for seq2seq model is important, requires extra handling</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">init_params</span>(<span class="hljs-params">module</span>):<br>        <span class="hljs-keyword">from</span> fairseq.modules <span class="hljs-keyword">import</span> MultiheadAttention<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(module, nn.Linear):<br>            module.weight.data.normal_(mean=<span class="hljs-number">0.0</span>, std=<span class="hljs-number">0.02</span>)<br>            <span class="hljs-keyword">if</span> module.bias <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                module.bias.data.zero_()<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(module, nn.Embedding):<br>            module.weight.data.normal_(mean=<span class="hljs-number">0.0</span>, std=<span class="hljs-number">0.02</span>)<br>            <span class="hljs-keyword">if</span> module.padding_idx <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                module.weight.data[module.padding_idx].zero_()<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(module, MultiheadAttention):<br>            module.q_proj.weight.data.normal_(mean=<span class="hljs-number">0.0</span>, std=<span class="hljs-number">0.02</span>)<br>            module.k_proj.weight.data.normal_(mean=<span class="hljs-number">0.0</span>, std=<span class="hljs-number">0.02</span>)<br>            module.v_proj.weight.data.normal_(mean=<span class="hljs-number">0.0</span>, std=<span class="hljs-number">0.02</span>)<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(module, nn.RNNBase):<br>            <span class="hljs-keyword">for</span> name, param <span class="hljs-keyword">in</span> module.named_parameters():<br>                <span class="hljs-keyword">if</span> <span class="hljs-string">"weight"</span> <span class="hljs-keyword">in</span> name <span class="hljs-keyword">or</span> <span class="hljs-string">"bias"</span> <span class="hljs-keyword">in</span> name:<br>                    param.data.uniform_(-<span class="hljs-number">0.1</span>, <span class="hljs-number">0.1</span>)<br><br>    <span class="hljs-comment"># weight initialization</span><br>    model.apply(init_params)<br>    <span class="hljs-keyword">return</span> model<br></code></pre></td></tr></tbody></table></figure>
<h4 id="architecture-related-configuration">Architecture Related Configuration</h4>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python">arch_args = Namespace(<br>    encoder_embed_dim=<span class="hljs-number">512</span>,<br>    encoder_ffn_embed_dim=<span class="hljs-number">2048</span>,<br>    encoder_layers=<span class="hljs-number">6</span>,<br>    decoder_embed_dim=<span class="hljs-number">512</span>,<br>    decoder_ffn_embed_dim=<span class="hljs-number">2048</span>,<br>    decoder_layers=<span class="hljs-number">6</span>,<br>    share_decoder_input_output_embed=<span class="hljs-literal">True</span>,<br>    dropout=<span class="hljs-number">0.3</span>,<br>)<br><br><span class="hljs-comment"># HINT: these patches on parameters for Transformer</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">add_transformer_args</span>(<span class="hljs-params">args</span>):<br>    args.encoder_attention_heads=<span class="hljs-number">8</span><br>    args.encoder_normalize_before=<span class="hljs-literal">True</span><br><br>    args.decoder_attention_heads=<span class="hljs-number">8</span><br>    args.decoder_normalize_before=<span class="hljs-literal">True</span><br><br>    args.activation_fn=<span class="hljs-string">"relu"</span><br>    args.max_source_positions=<span class="hljs-number">1024</span><br>    args.max_target_positions=<span class="hljs-number">1024</span><br><br>    <span class="hljs-comment"># patches on default parameters for Transformer (those not set above)</span><br>    <span class="hljs-keyword">from</span> fairseq.models.transformer <span class="hljs-keyword">import</span> base_architecture<br>    base_architecture(arch_args)<br><br>add_transformer_args(arch_args)<br></code></pre></td></tr></tbody></table></figure>
<h4 id="back-translation">Back-translation</h4>
<h5 id="todo-clean-corpus">TODO: clean corpus</h5>
<ol type="1">
<li>remove sentences that are too long or too short</li>
<li>unify punctuation</li>
</ol>
<blockquote>
<p>hint: you can use clean_s() defined above to do this</p>
</blockquote>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">clean_mono_corpus</span>(<span class="hljs-params">prefix, l, ratio=<span class="hljs-number">9</span>, max_len=<span class="hljs-number">1000</span>, min_len=<span class="hljs-number">1</span></span>):<br>    <span class="hljs-keyword">if</span> Path(<span class="hljs-string">f'<span class="hljs-subst">{prefix}</span>.clean.zh'</span>).exists():<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f'<span class="hljs-subst">{prefix}</span>.clean.zh exists. skipping clean.'</span>)<br>        <span class="hljs-keyword">return</span><br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">f'<span class="hljs-subst">{prefix}</span>'</span>, <span class="hljs-string">'r'</span>) <span class="hljs-keyword">as</span> l_in_f:<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">f'<span class="hljs-subst">{prefix}</span>.clean.zh'</span>, <span class="hljs-string">'w'</span>) <span class="hljs-keyword">as</span> l_out_f:<br>            <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> l_in_f:<br>                s = s.strip()<br>                s = clean_s(s, l)<br>                s_len = len_s(s, l)<br>                <span class="hljs-keyword">if</span> min_len &gt; <span class="hljs-number">0</span>: <span class="hljs-comment"># remove short sentence</span><br>                    <span class="hljs-keyword">if</span> s_len &lt; min_len:<br>                        <span class="hljs-keyword">continue</span><br>                <span class="hljs-keyword">if</span> max_len &gt; <span class="hljs-number">0</span>: <span class="hljs-comment"># remove long sentence</span><br>                    <span class="hljs-keyword">if</span> s_len &gt; max_len:<br>                        <span class="hljs-keyword">continue</span><br>                <span class="hljs-built_in">print</span>(s, file=l_out_f)<br>                <br><br>mono_data_prefix = <span class="hljs-string">f'<span class="hljs-subst">{mono_prefix}</span>/ted_zh_corpus.deduped'</span><br>clean_mono_corpus(mono_data_prefix, <span class="hljs-string">'zh'</span>)<br></code></pre></td></tr></tbody></table></figure>
<h5 id="todo-subword-units">TODO: Subword Units</h5>
<p>Use the spm model of the backward model to tokenize the data into subword units</p>
<blockquote>
<p>hint: spm model is located at DATA/raw-data/<a href="#dataset-2">dataset</a>/spm[vocab_num].model</p>
</blockquote>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">spm_encode</span>(<span class="hljs-params">prefix, vocab_size, mono_prefix</span>):<br>    spm_model = spm.SentencePieceProcessor(model_file=<span class="hljs-built_in">str</span>(prefix/<span class="hljs-string">f'spm<span class="hljs-subst">{vocab_size}</span>.model'</span>))<br><br>    in_path = mono_prefix / <span class="hljs-string">'ted_zh_corpus.deduped.clean'</span><br><br>    <span class="hljs-keyword">for</span> lang <span class="hljs-keyword">in</span> [src_lang, tgt_lang]:<br>        out_path = mono_prefix / <span class="hljs-string">f'mono.tok.<span class="hljs-subst">{lang}</span>'</span><br>        <span class="hljs-keyword">if</span> out_path.exists():<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f"<span class="hljs-subst">{out_path}</span> exists. skipping spm_encode."</span>)<br>        <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(out_path, <span class="hljs-string">'w'</span>) <span class="hljs-keyword">as</span> out_f:<br>            <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">f'<span class="hljs-subst">{in_path}</span>.<span class="hljs-subst">{lang}</span>'</span>, <span class="hljs-string">'r'</span>) <span class="hljs-keyword">as</span> in_f:<br>                <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> in_f:<br>                    line = line.strip()<br>                    tok = spm_model.encode(line, out_type=<span class="hljs-built_in">str</span>)<br>                    <span class="hljs-built_in">print</span>(<span class="hljs-string">' '</span>.join(tok), file=out_f)<br></code></pre></td></tr></tbody></table></figure>
<h5 id="todo-generate-synthetic-data-with-backward-model">TODO: Generate synthetic data with backward model</h5>
<p>Add binarized monolingual data to the original data directory, and name it with “split_name”</p>
<p>ex. .<a target="_blank" rel="noopener" href="https://colab.research.google.com/drive/11PW6m9Cryis4nGUDeTKfv-2c0NZd21wE#">/DATA/data-bin/ted2020</a>/[split_name].zh-en.[“en”, “zh”].[“bin”, “idx”]</p>
<p>then you can use ‘generate_prediction(model, task, split=“split_name”)’ to generate translation prediction</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># hint: do prediction on split='mono' to create prediction_file</span><br>task.load_dataset(split=<span class="hljs-string">"mono"</span>, epoch=<span class="hljs-number">1</span>)<br>generate_prediction(model, task, split=<span class="hljs-string">'mono'</span>, outfile=<span class="hljs-string">'./prediction.txt'</span> )<br></code></pre></td></tr></tbody></table></figure>
<h5 id="todo-create-new-dataset">TODO: Create new dataset</h5>
<ol type="1">
<li>Combine the prediction data with monolingual data</li>
<li>Use the original spm model to tokenize data into Subword Units</li>
<li>Binarize data with fairseq</li>
</ol>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Combine prediction_file (.en) and mono.zh (.zh) into a new dataset.</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># hint: tokenize prediction_file with the spm model</span><br>!cp ./prediction.txt {mono_prefix}/<span class="hljs-string">'ted_zh_corpus.deduped.clean.en'</span><br>spm_encode(prefix, vocab_size, mono_prefix)<br><span class="hljs-comment"># spm_model.encode(line, out_type=str)</span><br><span class="hljs-comment"># output: ./DATA/rawdata/mono/mono.tok.en &amp; mono.tok.zh</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># hint: use fairseq to binarize these two files again</span><br>binpath = Path(<span class="hljs-string">'./DATA/data-bin/synthetic'</span>)<br>src_dict_file = <span class="hljs-string">'./DATA/data-bin/ted2020/dict.en.txt'</span><br>tgt_dict_file = src_dict_file<br>monopref = ./DATA/rawdata/mono/mono.tok <span class="hljs-comment"># or whatever path after applying subword tokenization, w/o the suffix (.zh/.en)</span><br><span class="hljs-keyword">if</span> binpath.exists():<br>     <span class="hljs-built_in">print</span>(binpath, <span class="hljs-string">"exists, will not overwrite!"</span>)<br><span class="hljs-keyword">else</span>:<br>     !python -m fairseq_cli.preprocess\<br>         	--source-lang <span class="hljs-string">'zh'</span>\<br>         	--target-lang <span class="hljs-string">'en'</span>\<br>         	--trainpref {monopref}\<br>	        --destdir {binpath}\<br>	        --srcdict {src_dict_file}\<br>            --tgtdict {tgt_dict_file}\<br>	        --workers <span class="hljs-number">2</span><br></code></pre></td></tr></tbody></table></figure>
<h1 id="generative-adversarial-networkgan">Generative Adversarial Network(GAN)</h1>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250609164348887.png" srcset="/img/loading.gif" lazyload alt="image-20250609164348887"><figcaption aria-hidden="true">image-20250609164348887</figcaption>
</figure>
<blockquote>
<p>Distribution makes the same input has different outputs, especially for the tasks needs <strong>“creativity”.</strong></p>
</blockquote>
<h2 id="unconditional-generation">Unconditional Generation</h2>
<p>Take Anime Face Generation as an example.</p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250609164706881.png" srcset="/img/loading.gif" lazyload alt="image-20250609164706881"><figcaption aria-hidden="true">image-20250609164706881</figcaption>
</figure>
<h3 id="discriminator">Discriminator</h3>
<ul>
<li>Discriminator is a neural network. In this example, the input is an image, the output is a <strong>Scalar</strong>. Larger scalar means real, smaller value fake.</li>
</ul>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250609164912819.png" srcset="/img/loading.gif" lazyload alt="image-20250609164912819"><figcaption aria-hidden="true">image-20250609164912819</figcaption>
</figure>
<h3 id="basic-idea-of-gan">Basic Idea of GAN</h3>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250609165356858.png" srcset="/img/loading.gif" lazyload alt="image-20250609165356858"><figcaption aria-hidden="true">image-20250609165356858</figcaption>
</figure>
<blockquote>
<p>This is where the term “adversarial” comes from.</p>
</blockquote>
<h4 id="algorithm">Algorithm</h4>
<ul>
<li><p>Initialize <strong>Generator</strong> and <strong>Discriminator</strong></p></li>
<li><p>In each training iteration:</p>
<ol type="1">
<li>Fix generator G, and update discriminator D.
<ul>
<li>Discriminator learns to assign high scores to real objects and low scores to generated objects.</li>
</ul></li>
<li>Fix discriminator D, and update generator G.
<ul>
<li>Generator learns to “fool” the Discriminator.</li>
</ul></li>
</ol>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250609165814803.png" srcset="/img/loading.gif" lazyload alt="image-20250609165814803"><figcaption aria-hidden="true">image-20250609165814803</figcaption>
</figure></li>
</ul>
<h3 id="theory-behind-gan">Theory behind GAN</h3>
<h4 id="our-objective">Our Objective</h4>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250609170005494.png" srcset="/img/loading.gif" lazyload alt="image-20250609170005494"><figcaption aria-hidden="true">image-20250609170005494</figcaption>
</figure>
<ul>
<li><span class="math inline"><em>G</em><sup>⋆</sup> = <em>a</em><em>r</em><em>g</em>&nbsp;<em>m</em><em>i</em><em>n</em><sub><em>G</em></sub>&nbsp;<em>D</em><em>i</em><em>v</em>(<em>P</em><sub><em>G</em></sub>, <em>P</em><sub><em>d</em><em>a</em><em>t</em><em>a</em></sub>)</span></li>
<li><span class="math inline"><em>D</em><em>i</em><em>v</em>(<em>P</em><sub><em>G</em></sub>, <em>P</em><sub><em>d</em><em>a</em><em>t</em><em>a</em></sub>)</span> is Divergence between distributions <span class="math inline"><em>P</em><sub><em>G</em></sub></span> and <span class="math inline"><em>P</em><sub><em>d</em><em>a</em><em>t</em><em>a</em></sub></span></li>
<li><strong>How to compute the divergence?</strong></li>
</ul>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250609170616047.png" srcset="/img/loading.gif" lazyload alt="image-20250609170616047"><figcaption aria-hidden="true">image-20250609170616047</figcaption>
</figure>
<p><strong>Training</strong>: <span class="math inline"><em>D</em><sub>⋆</sub> = <em>a</em><em>r</em><em>g</em>&nbsp;<em>m</em><em>a</em><em>x</em><sub><em>D</em></sub><em>V</em>(<em>D</em>, <em>G</em>)</span>, The value of <span class="math inline"><em>m</em><em>a</em><em>x</em><sub><em>D</em></sub><em>V</em>(<em>D</em>, <em>G</em>)</span> is related to JS divergence(JS<hanla></hanla>散度).</p>
<p><strong>Objective Function</strong> for D: <span class="math inline"><em>V</em>(<em>G</em>, <em>D</em>) = <em>E</em><sub><em>y</em> ∼ <em>P</em><sub><em>d</em><em>a</em><em>t</em><em>a</em></sub></sub>[<em>l</em><em>o</em><em>g</em><em>D</em>(<em>y</em>)] + <em>E</em><sub><em>y</em> ∼ <em>P</em><sub><em>G</em></sub></sub>[<em>l</em><em>o</em><em>g</em>(1 − <em>D</em>(<em>y</em>))]</span></p>
<p>the <span class="math inline"><em>V</em>(<em>D</em>, <em>G</em>)</span> is negative cross entropy, so <span class="math inline"><em>D</em><sub>⋆</sub> = <em>a</em><em>r</em><em>g</em>&nbsp;<em>m</em><em>a</em><em>x</em><sub><em>D</em></sub><em>V</em>(<em>D</em>, <em>G</em>)</span> is equal to minimizing cross entropy in Training classifier.</p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250609171418772.png" srcset="/img/loading.gif" lazyload alt="image-20250609171418772"><figcaption aria-hidden="true">image-20250609171418772</figcaption>
</figure>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250609171359287.png" srcset="/img/loading.gif" lazyload alt="image-20250609171359287"><figcaption aria-hidden="true">image-20250609171359287</figcaption>
</figure>
<blockquote>
<p>Other divergence can also be used, not just JS divergence.</p>
</blockquote>
<h3 id="tips-for-gan">Tips for GAN</h3>
<ul>
<li>JS divergence is not suitable, because in most cases, <span class="math inline"><em>P</em><sub><em>G</em></sub></span> and <span class="math inline"><em>P</em><sub><em>d</em><em>a</em><em>t</em><em>a</em></sub></span> are not overlapped.</li>
<li>Intuition: If two distributions do not overlap, binary classifier achieves 100% accuracy.</li>
<li>Its accuracy (or loss) means nothing during GAN training.</li>
</ul>
<h4 id="wasserstein-distance">Wasserstein distance</h4>
<ul>
<li>Considering one distribution P as a pile of earth, and another distribution Q as the target.</li>
<li>The average distance the earth mover has to move the earth.</li>
</ul>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250609171802480.png" srcset="/img/loading.gif" lazyload alt="image-20250609171802480"><figcaption aria-hidden="true">image-20250609171802480</figcaption>
</figure>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250609171819388.png" srcset="/img/loading.gif" lazyload alt="image-20250609171819388"><figcaption aria-hidden="true">image-20250609171819388</figcaption>
</figure>
<p>There are many possible “moving plans”. Using the “moving plan” with <strong>the smallest average distance</strong> to define the Wasserstein distance.</p>
<h4 id="wgan">WGAN</h4>
<ul>
<li>Evaluate Wasserstein distance between <span class="math inline"><em>P</em><sub><em>d</em><em>a</em><em>t</em><em>a</em></sub></span> and <span class="math inline"><em>P</em><sub><em>G</em></sub></span>: <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.667ex;" xmlns="http://www.w3.org/2000/svg" width="39.369ex" height="2.364ex" role="img" focusable="false" viewBox="0 -750 17401.1 1045"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(878,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="msub" transform="translate(1407,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(605,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"></path></g><g data-mml-node="mo" transform="translate(828,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(1606,0)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mi" transform="translate(2287,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(2632,0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mi" transform="translate(3135,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(3604,0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mi" transform="translate(4037,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mi" transform="translate(4613,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(4958,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(5319,0)"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g></g></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(6151.9,0)"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path></g><g data-mml-node="TeXAtom" transform="translate(771,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="msub" transform="translate(490,0)"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="TeXAtom" transform="translate(675,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(520,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(1049,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(1410,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g></g></g></g></g><g data-mml-node="mo" transform="translate(2649.6,0)"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path></g><g data-mml-node="mi" transform="translate(2927.6,0)"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"></path></g><g data-mml-node="mo" transform="translate(3755.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(4144.6,0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(4634.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(5023.6,0)"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path></g><g data-mml-node="mo" transform="translate(5523.9,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="msub" transform="translate(6524.1,0)"><g data-mml-node="mi"><path data-c="1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path></g><g data-mml-node="TeXAtom" transform="translate(771,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="msub" transform="translate(490,0)"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="TeXAtom" transform="translate(675,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D43A" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q492 659 471 656T418 643T357 615T294 567T236 496T189 394T158 260Q156 242 156 221Q156 173 170 136T206 79T256 45T308 28T353 24Q407 24 452 47T514 106Q517 114 529 161T541 214Q541 222 528 224T468 227H431Q425 233 425 235T427 254Q431 267 437 273H454Q494 271 594 271Q634 271 659 271T695 272T707 272Q721 272 721 263Q721 261 719 249Q714 230 709 228Q706 227 694 227Q674 227 653 224Q646 221 643 215T629 164Q620 131 614 108Q589 6 586 3Q584 1 581 1Q571 1 553 21T530 52Q530 53 528 52T522 47Q448 -22 322 -22Q201 -22 126 55T50 252Z"></path></g></g></g></g></g><g data-mml-node="mo" transform="translate(8597.2,0)"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path></g><g data-mml-node="mi" transform="translate(8875.2,0)"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"></path></g><g data-mml-node="mo" transform="translate(9703.2,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(10092.2,0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(10582.2,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(10971.2,0)"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path></g></g></g></g></svg></mjx-container></li>
<li><span class="math inline"><em>D</em> ∈ 1 − <em>L</em><em>i</em><em>p</em><em>s</em><em>c</em><em>h</em><em>i</em><em>t</em><em>z</em></span> means D has to be smooth enough. If without the constraint, the training of D will not converge. So we need to Keep the D smooth forces D(x) become ∞ and −∞.</li>
</ul>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250609172347710.png" srcset="/img/loading.gif" lazyload alt="image-20250609172347710"><figcaption aria-hidden="true">image-20250609172347710</figcaption>
</figure>
<h3 id="evaluation-of-generation">Evaluation of Generation</h3>
<blockquote>
<p>Human evaluation is expensive and sometimes unfair/unstable. How to evaluate the quality of the generated images <strong>automatically</strong>?</p>
</blockquote>
<h4 id="problems">Problems</h4>
<h5 id="mode-collapse">Mode Collapse</h5>
<p>生成器（Generator）只能生成数据分布中的<strong>极少数模式（mode）</strong>，而无法覆盖全部真实数据的多样性。例如，训练一个生成人脸图像的<hanla></hanla>GAN<hanla></hanla>时，如果发生模式坍塌，生成器可能只会输出同一张脸或少数几张脸，而无法生成其他不同的人脸。</p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250610190028832.png" srcset="/img/loading.gif" lazyload alt="image-20250610190028832"><figcaption aria-hidden="true">image-20250610190028832</figcaption>
</figure>
<h5 id="mode-dropping">Mode Dropping</h5>
<p>生成器<strong>部分忽略</strong>真实数据分布中的某些模式（mode），导致生成样本的多样性<strong>不足</strong>，但尚未完全坍塌到极少数模式。与<hanla></hanla><strong>Mode Collapse（模式坍塌）</strong> 相比，Mode Dropping<hanla></hanla>是更轻微的多样性缺失问题——生成器可能覆盖了大部分真实数据的模式，但仍遗漏了一些子类别或变体。相比于<hanla></hanla>Mode Collapse，Mode Dropping<hanla></hanla>更难被察觉。</p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250610190522643.png" srcset="/img/loading.gif" lazyload alt="image-20250610190522643"><figcaption aria-hidden="true">image-20250610190522643</figcaption>
</figure>
<h4 id="inception-scoreis">Inception Score(IS)</h4>
<blockquote>
<p>Good quality,large diversity means <strong>large</strong> IS.</p>
</blockquote>
<ul>
<li><strong>高质量图像</strong>应被预训练的分类模型<strong>明确分类</strong></li>
<li><strong>多样性高的生成集</strong>应覆盖多个类别</li>
<li><span class="math inline"><em>I</em><em>S</em> = ∑<sub><em>x</em></sub>∑<sub><em>y</em></sub><em>P</em>(<em>y</em>|<em>x</em>)<em>l</em><em>o</em><em>g</em><em>P</em>(<em>y</em>|<em>x</em>) − ∑<sub><em>y</em></sub><em>P</em>(<em>y</em>)<em>l</em><em>o</em><em>g</em><em>P</em>(<em>y</em>)</span></li>
</ul>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250610190703305.png" srcset="/img/loading.gif" lazyload alt="image-20250610190703305"><figcaption aria-hidden="true">image-20250610190703305</figcaption>
</figure>
<h4 id="fréchet-inception-distance-fid">Fréchet Inception Distance (FID)</h4>
<blockquote>
<ul>
<li><p><strong>Smaller</strong> is better.</p></li>
<li><p>A lot of samples is needed.</p></li>
</ul>
</blockquote>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250610191218207.png" srcset="/img/loading.gif" lazyload alt="image-20250610191218207"><figcaption aria-hidden="true">image-20250610191218207</figcaption>
</figure>
<h3 id="more-evaluations">More Evaluations</h3>
<p>Pros and cons of GAN evaluation measures:https://arxiv.org/abs/1802.03446</p>
<h2 id="conditional-generation">Conditional Generation</h2>
<h3 id="text-to-image">Text-to-image</h3>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250610193633852.png" srcset="/img/loading.gif" lazyload alt="image-20250610193633852"><figcaption aria-hidden="true">image-20250610193633852</figcaption>
</figure>
<h3 id="image-translationpix2pix">Image translation(pix2pix)</h3>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250610193751736.png" srcset="/img/loading.gif" lazyload alt="image-20250610193751736"><figcaption aria-hidden="true">image-20250610193751736</figcaption>
</figure>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250610194003693.png" srcset="/img/loading.gif" lazyload alt="image-20250610194003693"><figcaption aria-hidden="true">image-20250610194003693</figcaption>
</figure>
<h3 id="sound-to-image">Sound-to-image</h3>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250610194058087.png" srcset="/img/loading.gif" lazyload alt="image-20250610194058087"><figcaption aria-hidden="true">image-20250610194058087</figcaption>
</figure>
<p>While sound becomes louder…</p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250610194152345.png" srcset="/img/loading.gif" lazyload alt="image-20250610194152345"><figcaption aria-hidden="true">image-20250610194152345</figcaption>
</figure>
<h3 id="talk-head-generation">Talk Head Generation</h3>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250610194222160.png" srcset="/img/loading.gif" lazyload alt="image-20250610194222160"><figcaption aria-hidden="true">image-20250610194222160</figcaption>
</figure>
<h2 id="learning-from-unpaired-data">Learning from Unpaired Data</h2>
<blockquote>
<p>Unsupervised Conditional Generation can learn the mapping without any paired data.</p>
</blockquote>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250610195532702.png" srcset="/img/loading.gif" lazyload alt="image-20250610195532702"><figcaption aria-hidden="true">image-20250610195532702</figcaption>
</figure>
<h3 id="cycle-gan">Cycle GAN</h3>
<p><strong>无需成对训练数据</strong>，即可实现不同域（domain）之间的风格转换。</p>
<ol type="1">
<li><strong>循环一致性（Cycle Consistency）</strong>:<hanla></hanla>确保从域<hanla></hanla>A<hanla></hanla>转换到域<hanla></hanla>B<hanla></hanla>后，再转换回域<hanla></hanla>A<hanla></hanla>的图像与原始图像尽可能一致，解决了无监督训练中<hanla></hanla>“模式坍塌”<hanla></hanla>和<hanla></hanla>“无配对数据”<hanla></hanla>的挑战。</li>
<li><strong>对抗损失（Adversarial Loss）</strong>：使用两个生成器和两个判别器，分别判断生成图像是否属于目标域。</li>
</ol>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250610200802740.png" srcset="/img/loading.gif" lazyload alt="image-20250610200802740"><figcaption aria-hidden="true">image-20250610200802740</figcaption>
</figure>
<h3 id="other-application">Other Application</h3>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250610201722352.png" srcset="/img/loading.gif" lazyload alt="image-20250610201722352"><figcaption aria-hidden="true">image-20250610201722352</figcaption>
</figure>
<h2 id="homework-6-anime-face-generation">Homework 6: Anime face generation</h2>
<h3 id="task-introduction">Task introduction</h3>
<ol type="1">
<li>Input: random number</li>
<li>Output: Anime face</li>
<li>Implementation requirement: <strong>DCGAN</strong> &amp; <strong>WGAN</strong> &amp; <strong>WGAN-GP</strong></li>
<li>Target: generate 1000 anime face images</li>
</ol>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250610202249831.png" srcset="/img/loading.gif" lazyload alt="image-20250610202249831"><figcaption aria-hidden="true">image-20250610202249831</figcaption>
</figure>
<h3 id="evaluation-metrics">Evaluation metrics</h3>
<h4 id="fid-fréchet-inception-distance-score">FID (Fréchet Inception Distance) score</h4>
<ol type="1">
<li>Use another model to create features for real and fake images</li>
<li>Calculate the Fréchet distance between distribution of two features</li>
</ol>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250610202442479.png" srcset="/img/loading.gif" lazyload alt="image-20250610202442479"><figcaption aria-hidden="true">image-20250610202442479</figcaption>
</figure>
<h4 id="afd-anime-face-detection-rate">AFD (Anime face detection) rate</h4>
<ol type="1">
<li>To detect how many anime faces in your submission</li>
<li>The higher, the better</li>
</ol>
<h3 id="dataset">Dataset</h3>
<p>Crypko 1. Dataset link is in the colab 2. Dataset format 3. There are 71,314 pictures in the folder 4. <strong>You can use additional data to increase the performance</strong></p>
<h3 id="baselines-1">Baselines</h3>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250610214342255.png" srcset="/img/loading.gif" lazyload alt="image-20250610214342255"><figcaption aria-hidden="true">image-20250610214342255</figcaption>
</figure>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250610214424250.png" srcset="/img/loading.gif" lazyload alt="image-20250610214424250"><figcaption aria-hidden="true">image-20250610214424250</figcaption>
</figure>
<h3 id="useful-information">Useful information</h3>
<h4 id="dcgan">DCGAN</h4>
<ol type="1">
<li>Sample code implementation</li>
<li>Use several conv layers to generate image</li>
</ol>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250610214921145.png" srcset="/img/loading.gif" lazyload alt="image-20250610214921145"><figcaption aria-hidden="true">image-20250610214921145</figcaption>
</figure>
<h4 id="wgan-wgan-gp">WGAN &amp; WGAN-GP</h4>
<ol type="1">
<li>WGAN: Modify from DCGAN</li>
</ol>
<ol type="a">
<li>Remove the last sigmoid layer from the discriminator.</li>
<li>Do not take the logarithm when calculating the loss.</li>
<li>Clip the weights of the discriminator to a constant (1 ~ -1).</li>
<li>Use RMSProp or SGD as the optimizer.</li>
<li><a target="_blank" rel="noopener" href="https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/wgan/wgan.py">link</a></li>
</ol>
<ol start="2" type="1">
<li>WGAN-GP: Modify from WGAN</li>
</ol>
<ol type="a">
<li>Use gradient penalty to replace weight clipping</li>
<li>Gradient penalty accumulate gradient from an interpolated image</li>
<li><a target="_blank" rel="noopener" href="https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/wgan_gp/wgan_gp.py">link</a></li>
</ol>
<h4 id="stylegan">StyleGAN</h4>
<ol type="a">
<li>First transform latent variable z to w</li>
<li>Use w in different stage in generator(Deal with different resolutions)</li>
<li>Useful <a target="_blank" rel="noopener" href="https://github.com/lucidrains/stylegan2-pytorch">link</a></li>
</ol>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250610215258306.png" srcset="/img/loading.gif" lazyload alt="image-20250610215258306"><figcaption aria-hidden="true">image-20250610215258306</figcaption>
</figure>
<h3 id="simple-baseline-3">Simple Baseline</h3>
<blockquote>
<p>Run sample code.</p>
</blockquote>
<p>Output:</p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250612005735719.png" srcset="/img/loading.gif" lazyload alt="image-20250612005735719"><figcaption aria-hidden="true">image-20250612005735719</figcaption>
</figure>
<h3 id="medium-baseline-3">Medium Baseline</h3>
<blockquote>
<p>Use WGAN with more epochs</p>
</blockquote>
<p>Code: <a target="_blank" rel="noopener" href="https://github.com/Striver98/Machine-Learning-Homework/blob/master/HW6_medium.py">link</a></p>
<p>Output:</p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250612005703078.png" srcset="/img/loading.gif" lazyload alt="image-20250612005703078"><figcaption aria-hidden="true">image-20250612005703078</figcaption>
</figure>
<h3 id="strong-baseline-3">Strong Baseline</h3>
<blockquote>
<p>Use WGAN-GP</p>
</blockquote>
<p>Code: <a target="_blank" rel="noopener" href="https://github.com/Striver98/Machine-Learning-Homework/blob/master/HW6_strong.py">link</a></p>
<p>Output:</p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/Epoch_039.jpg" srcset="/img/loading.gif" lazyload alt="Epoch_039"><figcaption aria-hidden="true">Epoch_039</figcaption>
</figure>
<h1 id="self-supervised-learning">Self-Supervised Learning</h1>
<p>​ In self-supervised learning, the system learns to predict part of its input from other parts of it input. In other words a portion of the input is used as a supervisory signal to a predictor fed with the remaining portion of the input.</p>
<blockquote>
<p>在自监督学习中，系统学习如何根据输入的其他部分来预测输入的一部分。换句话说，输入的某一部分被用作监督信号，提供给一个由输入剩余部分所馈送的预测器。</p>
</blockquote>
<h2 id="bert-series">BERT series</h2>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250613181547163.png" srcset="/img/loading.gif" lazyload alt="image-20250613181547163"><figcaption aria-hidden="true">image-20250613181547163</figcaption>
</figure>
<p>参考文献：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></li>
</ul>
<h3 id="masking-input">Masking Input</h3>
<p>BERT 的核心思想之一是 <strong>双向上下文建模</strong>。传统的语言模型（如 ELMo）通常是单向的（从左到右或从右到左），只能根据前面的词预测后面的词，或反之。BERT 旨在让模型能够同时利用词语<strong>左右两侧</strong>的全部上下文信息来理解词语本身的意义和角色。Masked Language Modeling (MLM) 任务就是为了实现这个目标而设计的。</p>
<p>Masking Input 发生在 BERT 的 <strong>预训练 (Pre-training)</strong> 阶段。其过程可以分解为以下几个步骤：</p>
<ol type="1">
<li><strong>输入句子准备：</strong>
<ul>
<li>输入通常是一个或两个句子（对于 Next Sentence Prediction 任务）。</li>
<li>句子被分割成更小的单元，通常是 <strong>WordPieces</strong> 或 <strong>subwords</strong> (例如，“playing” -&gt; <code>["play", "##ing"]</code>)。这是为了解决词汇表过大和未登录词<hanla></hanla>(OOV)<hanla></hanla>问题。</li>
<li>句子前后会添加特殊的标记：<code>[CLS]</code> (分类) 在句首，<code>[SEP]</code> (分隔) 在句尾（或两个句子之间）。</li>
</ul></li>
<li><strong>随机选择遮盖目标：</strong>
<ul>
<li>对输入序列中的所有 token（除了 <code>[CLS]</code> 和 <code>[SEP]</code> 等特殊标记），<strong>随机选择大约 15%</strong> 的 token 作为需要进行预测的目标。这个比例是经过实验确定的平衡点：太低则学习信号不足，太高则破坏了句子的上下文，导致学习困难。</li>
<li>举例：假设输入句子是 <code>"The quick brown fox jumps over the lazy dog"</code>。可能选中的目标 token 是 <code>"brown"</code>, <code>"fox"</code>, <code>"the"</code>（后一个）。</li>
</ul></li>
<li><strong>执行遮盖操作：</strong>
<ul>
<li>对被选中的这 15% 的 token，进行具体的遮盖处理。这里 <strong>不是简单地用一个固定的 “MASK” 替换所有选中 token</strong>。BERT 采用了更精细的策略：
<ul>
<li><strong>80% 的概率：</strong> 将这个 token 替换成特殊的 <code>[MASK]</code> 标记。这是最典型的<hanla></hanla>“遮盖”<hanla></hanla>操作。</li>
<li><strong>10% 的概率：</strong> 将这个 token 替换成一个 <strong>随机</strong> 的 token（从整个词汇表中随机采样）。</li>
<li><strong>10% 的概率：</strong> <strong>保持不变</strong>，即保留原始的 token。</li>
</ul></li>
</ul></li>
</ol>
<blockquote>
<p>实行这种策略的原因推测：</p>
<ul>
<li><strong>80% MASK：</strong> 强制模型学习利用完整的上下文来预测被遮盖位置的真正内容。这是任务的核心。</li>
<li><strong>10% 随机替换：</strong> 防止模型过于依赖<hanla></hanla>“<code>[MASK]</code>”<hanla></hanla>标记本身的存在。因为在<strong>微调<hanla></hanla>(Fine-tuning)</strong> 阶段处理真实数据时，输入中是<strong>不会</strong>出现 <code>[MASK]</code> 的。如果模型在预训练时只见过 <code>[MASK]</code>，它会觉得微调时遇到的真实单词（未被遮盖的）都是<hanla></hanla>“新东西”<hanla></hanla>或与训练信号无关。加入随机替换让模型学会<hanla></hanla>“这个位置可能是任何词，我需要基于上下文来猜”。</li>
<li><strong>10% 保持不变：</strong> 提供一种<hanla></hanla>“一致性”<hanla></hanla>信号。模型知道某些时候选中了要预测，但其实词是对的（因为没被修改），这有助于模型学习对未被遮盖词的表示也保持准确和稳定，并训练模型理解上下文即使不需要修改目标词也有效。</li>
</ul>
</blockquote>
<ol start="4" type="1">
<li><strong>模型输入：</strong>
<ul>
<li>最终，这个经过遮盖操作处理后的序列（可能包含 <code>[MASK]</code>, 随机词, 原始词）被送入 BERT 的 <strong>Transformer Encoder</strong>。</li>
<li>模型在处理这个序列时，<strong>知道哪些位置是被选中需要预测的</strong>（这些位置在计算损失时会用到）。</li>
</ul></li>
<li><strong>模型预测：</strong>
<ul>
<li>BERT 模型（多层 Transformer Encoder）基于输入序列中 <strong>所有 token 的双向上下文</strong>，计算出序列中每个位置的 <strong>上下文嵌入<hanla></hanla>(Contextual Embedding)</strong>。</li>
<li>对于 <strong>所有被选中（遮盖、随机替换、保持原样）的 token 的位置</strong>，模型会将对应位置的顶层输出向量（即该位置的上下文表示）送入一个额外的 <strong>Softmax 分类层</strong>。</li>
<li>这个分类层的作用是：<strong>预测该位置原始未被修改的 token 是什么</strong>。</li>
<li>模型不会预测未被选中的位置（那 85%）。</li>
</ul></li>
<li><strong>损失计算：</strong>
<ul>
<li>损失函数通常是标准的 <strong>交叉熵损失<hanla></hanla>(Cross-Entropy Loss)</strong>。</li>
<li>损失的计算 <strong>仅针对那些在步骤 2 中被选中的 token 的位置</strong>（即那 15%）。</li>
<li>对于每个这样的位置，模型预测该位置原始 token 在整个词汇表上的概率分布，损失就是该分布与目标分布（即原始 token 的 one-hot 向量）之间的交叉熵。</li>
<li>将所有被选中位置的损失求和，得到最终的目标函数用于更新模型参数。</li>
</ul></li>
</ol>
<blockquote>
<p>通过<strong>人为遮蔽部分输入并迫使模型基于完整上下文进行复原</strong>，驱动模型学习通用的、强大的、双向的深度语言理解能力，为后续在各种下游任务的微调打下了坚实的基础。</p>
</blockquote>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250613180236231.png" srcset="/img/loading.gif" lazyload alt="image-20250613180236231"><figcaption aria-hidden="true">image-20250613180236231</figcaption>
</figure>
<h3 id="next-sentence-prediction">Next Sentence Prediction</h3>
<h4 id="goal">Goal</h4>
<p>很多重要的下游任务（如问答<hanla></hanla><code>QA</code>、自然语言推理<hanla></hanla><code>NLI</code>、文本蕴含<hanla></hanla><code>Textual Entailment</code>）都需要模型理解<strong>句子之间</strong>的关系。MLM 任务主要是学习词级别或短语级别的语境理解，但缺乏明确训练模型掌握跨句子连贯性的能力。NSP 就是为了弥补这一点而被设计出来的。</p>
<h4 id="theory">Theory</h4>
<p>NSP 是一个<strong>二分类任务</strong>。在预训练阶段，BERT 的输入由两个片段 <code>Sentence A</code> 和 <code>Sentence B</code> 组成。模型需要判断：<strong><code>Sentence B</code> 是不是 <code>Sentence A</code> 在原始文本中的实际下一句（即它们在上下文中是连贯、有逻辑顺序的）？</strong></p>
<ul>
<li><strong>标签 <code>IsNext</code> (正例)：</strong> <code>Sentence B</code> 是 <code>Sentence A</code> 在原始文档中真实的后续句子。</li>
<li><strong>标签 <code>NotNext</code> (负例)：</strong> <code>Sentence B</code> 是随机从语料库中挑选出来的一个句子，与 <code>Sentence A</code> 在语义和逻辑上<strong>没有连贯关系</strong>。</li>
</ul>
<p>通过迫使模型区分这两种情况，BERT 学习捕捉句子间的各种关系，如因果关系、转折关系、顺承关系等，并理解句子组合的整体含义。</p>
<h4 id="process">Process</h4>
<ol type="1">
<li><p><strong>输入构造：</strong></p>
<ul>
<li><p><strong>Sentence A：</strong>通常是取语料库中的一个完整句子或文本片段。</p></li>
<li><p><strong>Sentence B</strong> 存在两种情况：</p>
<ul>
<li><strong>正例 (50% 的概率)：</strong> <code>Sentence B</code> 就是从原始文档中紧接着 <code>Sentence A</code> 的下一个句子。</li>
<li><strong>负例 (50% 的概率)：</strong> <code>Sentence B</code> 是从语料库中<strong>随机挑选</strong>出来的一个句子（通常来自与 <code>Sentence A</code> 不同的文档）。关键点是它与 <code>Sentence A</code> <strong>没有上下文关联</strong>。</li>
</ul></li>
<li><p><strong>标记符：</strong></p>
<ul>
<li><p><code>[CLS]</code>： 放在整个输入序列的最开头。</p></li>
<li><p><code>[SEP]</code>： 放在 <code>Sentence A</code> 和 <code>Sentence B</code> 之间，用于分隔两个句子。如果输入只有 <code>Sentence A</code>（如某些分类任务），则 <code>[SEP]</code> 放在句尾。</p></li>
</ul></li>
</ul></li>
<li><p><strong>模型输入：</strong></p>
<ul>
<li>最终的输入序列是： <code>[CLS] + Token_1^A + Token_2^A + ... + [SEP] + Token_1^B + Token_2^B + ... + [SEP]</code></li>
<li>除了单词 token 的嵌入（Token Embeddings），BERT 的输入嵌入还包括：
<ul>
<li><strong>段落嵌入 (Segment Embeddings):</strong> 用于区分 <code>Sentence A</code> 和 <code>Sentence B</code>。所有属于 <code>Sentence A</code> 的 token 分配一个段嵌入类型（比如 0），所有属于 <code>Sentence B</code> 的 token 分配另一个段嵌入类型（比如 1）。<code>[CLS]</code> 和 <code>[SEP]</code> 通常也按位置或段逻辑分配。</li>
<li><strong>位置嵌入 (Position Embeddings):</strong> 表示每个 token 在序列中的位置信息。</li>
</ul></li>
</ul></li>
<li><p><strong>模型处理：</strong></p>
<ul>
<li>这个构造好的输入序列（包含 token 嵌入、段嵌入、位置嵌入）被送入 BERT 的 <strong>Transformer Encoder</strong>。</li>
<li>Transformer Encoder 处理整个序列，为输入序列中的 <strong>每个位置</strong>（包括每个 token 和 <code>[CLS]</code>）输出一个<strong>上下文感知的向量表示</strong>。</li>
</ul></li>
<li><p><strong>NSP 预测：</strong></p>
<ul>
<li>NSP 任务的预测<strong>只</strong>依赖于特殊标记 <code>[CLS]</code> 对应的输出向量 <code>C</code>。</li>
<li>将这个向量 <code>C</code> 送入一个额外的 <strong>分类层（Classification Layer）</strong>，通常是一个 <code>Linear + Softmax</code> 层。</li>
<li>这个分类层将<hanla></hanla><code>C</code><hanla></hanla>映射成一个维度为 2 的向量，代表两个类别的概率：
<ul>
<li><strong><code>P(IsNext | A, B)</code></strong>： <code>Sentence B</code> 是 <code>Sentence A</code> 的下一句的概率。</li>
<li><strong><code>P(NotNext | A, B)</code></strong>： <code>Sentence B</code> 不是 <code>Sentence A</code> 的下一句的概率。</li>
</ul></li>
</ul></li>
<li><p><strong>损失计算：</strong></p>
<ul>
<li>目标函数就是标准的 <strong>二元交叉熵损失 (Binary Cross-Entropy Loss)</strong>。</li>
<li>损失基于模型对 <code>[CLS]</code> 位置输出的预测概率和真实标签 (<code>IsNext</code> 或 <code>NotNext</code>) 进行计算。</li>
<li>这个损失与 MLM 任务的损失（基于被遮盖位置的预测）<strong>相加</strong>，作为 BERT 预训练的<strong>总损失</strong>，用于更新整个模型的参数。</li>
</ul></li>
</ol>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250613181046365.png" srcset="/img/loading.gif" lazyload alt="image-20250613181046365"><figcaption aria-hidden="true">image-20250613181046365</figcaption>
</figure>
<h4 id="problem">Problem</h4>
<ul>
<li><p>This approach is not helpful.</p>
<p>Robustly optimized BERT approach RoBERTa: https://arxiv.org/abs/1907.11692</p>
<blockquote>
<p>RoBERTa 发现，去掉 NSP 任务并且只使用更大的 batch size 和更多的数据训练 MLM，反而在多项任务上取得了更好的效果。他们认为 NSP 任务可能不够有挑战性，负例（随机选下一句）太容易被区分（正例来自同一文档，负例来自不同文档，模型可能主要区分文档来源，而不是精细的语义连贯性）。</p>
</blockquote></li>
<li><p>SOP : Sentence order prediction</p>
<p>Used in ALBERT: https://arxiv.org/abs/1909.11942</p>
<blockquote>
<p>它不区分句子是否来自同一文档，而是关注<strong>句子顺序逻辑</strong>。</p>
<ul>
<li><strong>正例：</strong> 两个<strong>连续</strong>句子按原始顺序呈现 (<code>Sentence A</code>, <code>True Next B</code>)。</li>
<li><strong>负例：</strong> 将<strong>相同</strong>的两个句子<strong>交换顺序</strong>呈现 (<code>Sentence B</code> 变成了第一句，<code>Sentence A</code> 变成了第二句)。</li>
<li>任务变为判断句子顺序是否正确。这要求模型理解句子间的内在因果关系、时间顺序等，比区分是否来自同一文档更具挑战性。实验表明 SOP 通常比 NSP 更有效。</li>
</ul>
</blockquote></li>
</ul>
<h3 id="benchmark">Benchmark</h3>
<p>General Language Understanding Evaluation (GLUE): <a target="_blank" rel="noopener" href="https://www.cluebenchmarks.com/">CLUE<hanla></hanla>中文语言理解基准测评</a></p>
<p>•Corpus of Linguistic Acceptability ( CoLA •Stanford Sentiment Treebank (SST 2) •Microsoft Research Paraphrase Corpus (MRPC) •Quora Question Pairs (QQP) •Semantic Textual Similarity Benchmark (STS B) •Multi Genre Natural Language Inference (MNLI) •Question answering NLI (QNLI) •Recognizing Textual Entailment (RTE) •Winograd NLI (WNLI)</p>
<h3 id="how-to-use-bert">How to use BERT</h3>
<h4 id="sentiment-analysis">Sentiment analysis</h4>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250613181953520.png" srcset="/img/loading.gif" lazyload alt="image-20250613181953520"><figcaption aria-hidden="true">image-20250613181953520</figcaption>
</figure>
<h4 id="pos-tagging">POS tagging</h4>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250613182037798.png" srcset="/img/loading.gif" lazyload alt="image-20250613182037798"><figcaption aria-hidden="true">image-20250613182037798</figcaption>
</figure>
<h4 id="natural-language-inference-nli">Natural Language Inference (NLI)</h4>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250613182117509.png" srcset="/img/loading.gif" lazyload alt="image-20250613182117509"><figcaption aria-hidden="true">image-20250613182117509</figcaption>
</figure>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250613182138731.png" srcset="/img/loading.gif" lazyload alt="image-20250613182138731"><figcaption aria-hidden="true">image-20250613182138731</figcaption>
</figure>
<h4 id="extraction-based-question-answering-qa">Extraction based Question Answering (QA)</h4>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250613182227705.png" srcset="/img/loading.gif" lazyload alt="image-20250613182227705"><figcaption aria-hidden="true">image-20250613182227705</figcaption>
</figure>
<h3 id="other-research">Other Research</h3>
<h4 id="bert-embryology">BERT Embryology</h4>
<p>Based on the BERT Embryology (胚胎学)，When does BERT know POS tagging（语义标注）, syntactic parsing（语法分析）, semantics（语义理解）?</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.02480">link</a></p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250614000249908.png" srcset="/img/loading.gif" lazyload alt="image-20250614000249908"><figcaption aria-hidden="true">image-20250614000249908</figcaption>
</figure>
<blockquote>
<p>由论文可知，重建（reconstruction）结果如图<hanla></hanla>1(a)<hanla></hanla>所示。ALBERT<hanla></hanla>首先学习重建功能词（function words），例如限定词（determiners）、介词（prepositions），随后逐步按<strong>动词→副词→形容词→名词→专有名词</strong>的顺序学习重建实义词（content words）。<strong>动词的不同形式和时态具有差异化学习进程</strong>：第三人称单数现在时（third-person singular present）重建最为容易，而现在分词（present participle）重建难度最高。图<hanla></hanla>1(b)<hanla></hanla>的预测（prediction）结果表明，<strong>掩码预测的学习难度普遍高于令牌重建</strong>。ALBERT<hanla></hanla>预测掩码令牌的顺序虽与重建相似，但<strong>速度显著较慢且准确率更低</strong>。</p>
</blockquote>
<blockquote>
<p>论文选择了四类探针任务（probing tasks）进行实验：<strong>词性标注（POS tagging）</strong>、<strong>成分标记（constituent tagging）</strong>、<strong>共指消解（coreference resolution）</strong> 和<strong>语义角色标注（semantic role labeling）</strong>。前两项任务用于探测标记嵌入（token embeddings）中隐含的<strong>语法知识（syntactic knowledge）</strong>，后两项任务则旨在检测标记嵌入所承载的<strong>语义知识（semantic knowledge）</strong>。</p>
<p>实验结果如图<hanla></hanla>2<hanla></hanla>所示。可以观察到所有四项任务在预训练期间呈现<strong>相似趋势</strong>，表明<strong>语法知识与语义知识在预训练过程中同步发展</strong>。具体而言：</p>
<ol type="1">
<li>语法相关任务（POS<hanla></hanla>标注与成分标记）：
<ul>
<li>在前<hanla></hanla>10<hanla></hanla>万步<strong>快速提升性能</strong></li>
<li>后续训练中<strong>不再显著改善</strong>，且表现持续波动</li>
</ul></li>
<li>语义角色标注（SRL）的异常现象：
<ul>
<li>约<hanla></hanla>15<hanla></hanla>万步达到性能峰值</li>
<li>此后<strong>逐步衰减</strong></li>
<li>这一现象可能表明，当<hanla></hanla>ALBERT<hanla></hanla>模型<strong>致力于优化预训练目标时</strong>，特定网络层中与任务相关的信息会逐渐减少（<em>信息衰减机制</em>）。</li>
</ul></li>
</ol>
</blockquote>
<h4 id="massbart">MASS/BART</h4>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1905.02450">MASS: Masked Sequence to Sequence Pre-training for Language Generation</a></p>
<blockquote>
<p>MASS is based on the sequence to sequence learning framework: its encoder takes a sentence with a masked fragment (several consecutive tokens) as input, and its decoder predicts this masked fragment conditioned on the encoder representations. Unlike BERT or a language model that pre-trains only the encoder or decoder, MASS is carefully designed to pre-train the encoder and decoder jointly in two steps: 1) By predicting the fragment of the sentence that is masked on the encoder side, MASS can force the encoder to understand the meaning of the unmasked tokens, in order to predict the masked tokens in the decoder side; 2) By masking the input tokens of the decoder that are unmasked in the source side, MASS can force the decoder rely more on the source representation other than the previous tokens in the target side for next token prediction, better facilitating the joint training between encoder and decoder.</p>
<p>该方法基于序列到序列学习框架：其编码器接收带有掩码片段（若干连续词元）的句子作为输入，解码器则基于编码器表征预测该掩码片段。与仅预训练编码器或解码器的<hanla></hanla>BERT<hanla></hanla>或语言模型不同，MASS<hanla></hanla>通过两个步骤精心设计以联合预训练编码器和解码器：1）通过预测编码器侧被掩码的句子片段，迫使编码器理解未掩码词元的语义，从而支持解码器对掩码词元的预测；2）通过掩码解码器输入中源端未遮盖的词元，迫使解码器在预测后续词元时更依赖源表征而非目标端历史词元，从而更好地促进编码器与解码器的联合训练。</p>
</blockquote>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1910.13461">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</a></p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/x3.png" srcset="/img/loading.gif" lazyload alt="Refer to caption"><figcaption aria-hidden="true">Refer to caption</figcaption>
</figure>
<blockquote>
<p>Inputs to the encoder need not be aligned with decoder outputs, allowing arbitrary noise transformations. Here, a document has been corrupted by replacing spans of text with mask symbols. The corrupted document (left) is encoded with a bidirectional model, and then the likelihood of the original document (right) is calculated with an autoregressive decoder. For fine-tuning, an uncorrupted document is input to both the encoder and decoder, and we use representations from the final hidden state of the decoder.</p>
<p>编码器的输入无需与解码器输出对齐，这允许进行任意噪声转换。本文操作中，通过将文本片段替换为掩码符号来损坏原始文档。左侧的损坏文档经由双向模型编码后，再由自回归解码器计算右侧原始文档的可能性。在微调阶段，原始文档同时输入编码器与解码器，我们使用解码器最终隐藏状态的表示。</p>
<p>BART is trained by corrupting documents and then optimizing a reconstruction loss—the cross-entropy between the decoder’s output and the original document. Unlike existing denoising autoencoders, which are tailored to specific noising schemes, BART allows us to apply any type of document corruption. In the extreme case, where all information about the source is lost, BART is equivalent to a language model.</p>
<p>BART<hanla></hanla>通过破坏文档再进行重构损失优化完成训练——具体优化的是解码器输出与原始文档间的交叉熵。与现有仅适配特定噪声方案的降噪自编码器不同，BART<hanla></hanla>允许采用任意类型的文档破坏方式。在极端情况下，当原始信息完全丢失时，BART<hanla></hanla>即相当于语言模型。</p>
</blockquote>
<blockquote>
<p>The transformations:</p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250614010826805.png" srcset="/img/loading.gif" lazyload alt="image-20250614010826805"><figcaption aria-hidden="true">image-20250614010826805</figcaption>
</figure>
<ol type="1">
<li><strong>Token Masking</strong>: random tokens are sampled and replaced with [MASK] elements.</li>
<li><strong>Token Deletion</strong>: Random tokens are deleted from the input. In contrast to token masking, the model must decide which positions are missing inputs.</li>
<li><strong>Text Infilling</strong>: A number of text spans are sampled, with span lengths drawn from a Poisson distribution (λ=3). Each span is replaced with a single [MASK] token. 0-length spans correspond to the insertion of [MASK] tokens. Text infilling teaches the model to predict how many tokens are missing from a span.</li>
<li><strong>Sentence Permutation</strong>: A document is divided into sentences based on full stops, and these sentences are shuffled in a random order.</li>
<li><strong>Document Rotation</strong>: A token is chosen uniformly at random, and the document is rotated so that it begins with that token. This task trains the model to identify the start of the document.</li>
</ol>
</blockquote>
<h3 id="why-does-bert-work">Why does BERT work?</h3>
<p><strong>Contextualized word embedding</strong>(上下文词嵌入): 通过<hanla></hanla>Transformer<hanla></hanla>的自注意力机制，每个词的嵌入向量由<strong>全句所有词共同计算生成</strong>，实现真正的语境感知，同一词在不同句子中生成不同向量（如<hanla></hanla>“bank”<hanla></hanla>在金融<hanla></hanla>/<hanla></hanla>地理语境中向量差异显著），解决多义词问题。</p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250614012124426.png" srcset="/img/loading.gif" lazyload alt="image-20250614012124426"><figcaption aria-hidden="true">image-20250614012124426</figcaption>
</figure>
<p>一个有趣的现象：即使在蛋白质、DNA<hanla></hanla>或音乐分类等非文本数据上，使用在文本上预训练的<hanla></hanla>BERT<hanla></hanla>模型，也比随机初始化的模型收敛更快、性能更好，而且只比专门针对该任务设计的模型稍差一点。</p>
<blockquote>
<p>原因猜测：</p>
<ol type="1">
<li>模型架构的普适性：Transformer<hanla></hanla>的自注意力机制（Self-Attention）本质是<strong>学习序列元素间的依赖关系</strong>，无论输入是文本、蛋白质序列还是音乐信号。</li>
<li>预训练获得的通用表示能力：文本预训练使模型学习到<strong>层级化特征提取能力</strong>，而这种能力能直接迁移至其他非文本任务中。并且文本预训练数据天然含噪声（如错别字、语法变异），使模型学会<strong>过滤无关信号</strong>，这一特性在非文本领域至关重要。</li>
<li>迁移学习的优化优势：预训练权重提供了<strong>接近最优的初始化点</strong>，避免随机初始化陷入局部最优；文本数据的规模（如千亿级<hanla></hanla>token）远超生物<hanla></hanla>/<hanla></hanla>音乐领域，预训练模型可以将<strong>大规模数据知识蒸馏至参数中</strong>。</li>
</ol>
</blockquote>
<h3 id="multi-lingual-bert">Multi-lingual BERT</h3>
<blockquote>
<p>Training a BERT model by many different languages.</p>
</blockquote>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250614013711437.png" srcset="/img/loading.gif" lazyload alt="image-20250614013711437"><figcaption aria-hidden="true">image-20250614013711437</figcaption>
</figure>
<p>在<hanla></hanla>BERT<hanla></hanla>的<hanla></hanla>Zero-shot Reading Comprehension<hanla></hanla>任务中，采用<hanla></hanla>104<hanla></hanla>种语言预训练（Pre-train），以中英双语微调（Fine-tune）反而优于纯中文预训练<hanla></hanla>+<hanla></hanla>中文微调的组合。</p>
<blockquote>
<p>原因猜测：</p>
<ol type="1">
<li>在<hanla></hanla>104<hanla></hanla>种语言的大规模语料上预训练时，模型通过共享的<hanla></hanla>Transformer<hanla></hanla>编码器学习不同语言之间的深层语义关联。例如，中文<hanla></hanla>“兔子”<hanla></hanla>和英文<hanla></hanla>“rabbit”<hanla></hanla>的嵌入向量在表示空间中高度接近。这种跨语言对齐能力使模型在未训练过的语言任务中（如中文零样本阅读理解）仍能通过语义映射进行推理。</li>
<li>104<hanla></hanla>种语言的训练数据量远超单一中文语料，覆盖更广泛的语法结构、文化背景和领域知识。这种多样性显著提升模型对语言共性与差异的鲁棒性，使其在面对中文零样本任务时更少受限于训练数据的偶然性。单一中文预训练<hanla></hanla>+<hanla></hanla>微调易在小规模中文数据集上过拟合；而多语言预训练通过大规模正则化抑制过拟合，微调时中英文数据进一步提供异构监督信号。</li>
<li>模型在预训练中学习到跨语言的通用模式（如指代消解、因果推理），这些模式在英文微调时被强化，并直接迁移至中文阅读理解中。</li>
</ol>
</blockquote>
<h2 id="gpt-series">GPT series</h2>
<h3 id="predict-next-token">Predict Next Token</h3>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250615210451323.png" srcset="/img/loading.gif" lazyload alt="image-20250615210451323"><figcaption aria-hidden="true">image-20250615210451323</figcaption>
</figure>
<p>GPT 的核心任务本质上是<strong>序列预测</strong>。给定一个已有的词序列（或更精确地说，Token 序列），模型的任务是预测在这个序列之后，<strong>最有可能出现的下一个 Token 是什么</strong>。这个过程是<strong>自回归</strong>的：模型预测出一个 Token 后，会将它添加到输入序列末尾，然后基于这个新的、更长的序列预测下一个 Token，如此循环往复，从而生成连贯的文本。</p>
<h4 id="basic-principle">Basic Principle</h4>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250615213043548.png" srcset="/img/loading.gif" lazyload alt="image-20250615213043548" style="zoom:200%;"></p>
<h3 id="in-context-learning-icl">In-context Learning (ICL)</h3>
<p><strong>模型无需更新其参数（即无需传统意义上的<hanla></hanla>“训练”<hanla></hanla>或<hanla></hanla>“微调”），仅通过向模型的输入中提供特定格式的文本示例（称为<hanla></hanla>“上下文”<hanla></hanla>或<hanla></hanla>“提示”）</strong>，就能理解并执行新的任务。</p>
<blockquote>
<p>可以将 ICL 理解为一种<strong>即时且临时的<hanla></hanla>“学习”<hanla></hanla>或<hanla></hanla>“适应”</strong>，完全依赖模型输入中提供的上下文信息来完成特定任务。</p>
</blockquote>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250615213744336.png" srcset="/img/loading.gif" lazyload alt="image-20250615213744336"><figcaption aria-hidden="true">image-20250615213744336</figcaption>
</figure>
<p><strong>为什么<hanla></hanla>GPT<hanla></hanla>能够进行<hanla></hanla>In-context Learning (ICL) ？</strong></p>
<blockquote>
<ol type="1">
<li>LLM 在庞大的文本语料库上进行预训练（目标是预测下一个词）。在这个过程中，它们不仅学习了语言知识（语法、词汇），还大量地、反复地<hanla></hanla>“看到了”<hanla></hanla>各种任务的文本描述和案例（因为互联网上充斥着教程、问答、代码注释等）。预训练让模型具备了对<hanla></hanla>“任务模式”<hanla></hanla>极其敏感的潜在能力。</li>
<li>Transformer 的自注意力机制能够高效地处理长距离依赖关系，并能在生成输出时充分考虑整个输入序列中的所有信息。这使得模型能够动态地将<hanla></hanla>“指令”<hanla></hanla>和<hanla></hanla>“演示示例”<hanla></hanla>与<hanla></hanla>“新输入”<hanla></hanla>联系起来，找出其中的模式和映射关系。</li>
<li>当提供演示示例时，模型会识别这些示例所体现的输入到输出的映射规则（如：“输入英语句子 -&gt; 输出法语句子”，“说好的 -&gt; 积极；说坏的 -&gt; 消极；陈述事实 -&gt; 中性”）。然后，它把新输入代入到推断出的规则中，生成符合该规则的输出序列（即预测下一个词）。</li>
</ol>
</blockquote>
<h3 id="to-learn-more">To learn more</h3>
<h4 id="simclrsimple-framework-for-contrastive-learning-of-visual-representations">SimCLR(Simple Framework for Contrastive Learning of Visual Representations)</h4>
<blockquote>
<p>一种自监督视觉表征学习方法，通过对比学习从未标注数据中学习高质量的特征表示，无需人工标注即可达到与监督学习相媲美的性能。</p>
</blockquote>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616000937024.png" srcset="/img/loading.gif" lazyload alt="image-20250616000937024"><figcaption aria-hidden="true">image-20250616000937024</figcaption>
</figure>
<ul>
<li><strong>Data Augmentation</strong>：对同一原始图像应用两次独立的随机增强（如随机裁剪、颜色抖动、高斯模糊、水平翻转），生成一对<strong>正样本</strong>。</li>
<li><strong>Encoder</strong>：使用卷积神经网络提取增强后图像的语义特征，输出高维特征向量 <span class="math inline"><em>h</em></span>（称为<strong>表示向量</strong>）。</li>
<li><strong>Projection Head</strong>：在编码器后添加一个小型多层感知机（MLP），将 <em><span class="math inline"><em>h</em></span></em> 映射到低维空间 <em><span class="math inline"><em>z</em></span></em>（称为<strong>投影向量</strong>），投影头通过非线性变换（如<hanla></hanla>ReLU）过滤冗余信息，优化特征空间以适应对比任务。</li>
<li><strong>NT-Xent Loss</strong>（<strong>比损失函数</strong>）：最大化正样本对的相似性，最小化负样本对的相似性。
<ul>
<li><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250615234510645.png" srcset="/img/loading.gif" lazyload title="fig:" alt="image-20250615234510645"></li>
<li><span class="math inline"><em>s</em><em>i</em><em>m</em>(<em>z</em><sub><em>i</em></sub>, <em>z</em><sub><em>j</em></sub>)</span>：余弦相似度</li>
<li><span class="math inline"><em>τ</em></span>：温度参数，控制相似度分布的尖锐程度（值越小，区分越严格）</li>
<li>分母包含同一批次内所有其他样本作为<strong>负样本</strong></li>
</ul></li>
</ul>
<p><strong>工作流程</strong>：一批 <em>N</em> 张未标注图像，每张图像生成两个增强视图，共 2<em>N</em> 个样本，进行特征提取（增强视图 → 编码器 → 表示向量 <em>h</em> → 投影头 → 投影向量 <em>z</em>），对每个样本 <em>i</em>，计算其正样本对的相似度，并对比所有负样本（2<em>N</em>−2 个）的相似度，通过反向传播更新编码器和投影头的参数，丢弃与任务无关的噪声特征，训练完成后移除投影头，仅用编码器提取的特征 <em>h</em> 进行迁移（如分类、检测）。</p>
<h4 id="byolbootstrap-your-own-latent">BYOL（Bootstrap your own latent）</h4>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2006.07733">Bootstrap your own latent: A new approach to self-supervised Learning</a></p>
<blockquote>
<p>是一种无需负样本的自监督对比学习方法，由<hanla></hanla>DeepMind<hanla></hanla>团队于<hanla></hanla>2020<hanla></hanla>年提出。它通过双网络结构和自蒸馏机制，实现高效的特征表示学习，在图像分类、目标检测等任务中达到与监督学习相当的性能。</p>
</blockquote>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616000949805.png" srcset="/img/loading.gif" lazyload alt="image-20250616000949805"><figcaption aria-hidden="true">image-20250616000949805</figcaption>
</figure>
<ul>
<li><strong>双网络架构</strong>：BYOL<hanla></hanla>的核心是<strong>在线网络（Online Network）</strong> 和 <strong>目标网络（Target Network）</strong>。在线网络包含编码器（如<hanla></hanla>ResNet）、投影头（MLP）和预测器（MLP）。参数通过梯度下降更新，负责学习当前任务；目标网络结构与在线网络相同（无预测器），参数通过<strong>指数移动平均（EMA）</strong> 更新，目标网络提供稳定的学习目标，避免模型坍塌。</li>
<li><strong>自蒸馏机制</strong>：对同一图像生成两个增强视图，由在线网络和目标网络分别处理，得出预测表示和投影表示，最小化二者的<hanla></hanla>loss，损失函数采用归一化余弦相似度损失，最小化同一图像不同视图表示的距离，无需负样本约束。</li>
</ul>
<blockquote>
<p>SimCLR<hanla></hanla>以来大量负样本防止坍塌，但负样本可能引入噪声或偏差，而<hanla></hanla>BYOL<hanla></hanla>通过目标网络的<hanla></hanla>EMA<hanla></hanla>更新和预测器，<strong>避免坍塌且简化训练</strong>，摆脱对负样本的依赖。</p>
</blockquote>
<h2 id="pre-trained-language-models">Pre-trained Language Models</h2>
<h3 id="background-knowledge">Background knowledge</h3>
<blockquote>
<p>Training a language model is self-supervised learning. Self-supervised learning is predicting any part of the input from any other part.</p>
</blockquote>
<ol type="1">
<li><p>Autoregressive Language Models (ALMs): Complete the sentence given its prefix.</p></li>
<li><p>Masked Language Models (MLMs): Use the unmasked words to predict the masked word.</p></li>
<li><p>Transformer-based ALMs: Composed of stacked layers of transformer layers.</p></li>
<li><p>Pre-trained Language Models (PLMs):</p>
<ul>
<li>Using a large corpora(语料库) to train a neural language model.
<ul>
<li>Autoregressive pre-trained: GPT<hanla></hanla>系列<hanla></hanla>(GPT, GPT-2, GPT-3)</li>
<li>MLM-based pre-trained: BERT<hanla></hanla>系列<hanla></hanla>(BERT, RoBERTa, ALBERT)</li>
<li>We believe that after pre-training, the PLM learns some knowledge, encoded in its hidden representations, that can transfer to downstream tasks</li>
<li><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616003835183.png" srcset="/img/loading.gif" lazyload title="fig:" alt="image-20250616003835183"></li>
</ul></li>
<li>(Standard) fine-tuning: Using the pre-trained weights of the PLM to initialize a model for a downstream task.
<ul>
<li><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616004150556.png" srcset="/img/loading.gif" lazyload title="fig:" alt="image-20250616004150556"></li>
</ul></li>
</ul></li>
</ol>
<h3 id="the-problems-of-plms">The Problems of PLMs</h3>
<ol type="1">
<li>Data scarcity in downstream tasks: A large amount of labeled data is not easy to obtain for each downstream task.</li>
<li>The PLM is too big, and they are still getting bigger.
<ul>
<li>Need a copy for each downstream task</li>
<li>Inference takes too long</li>
<li>Consume too much space</li>
</ul></li>
</ol>
<h3 id="the-solutions-of-those-problems">The Solutions of Those Problems</h3>
<h4 id="labeled-data-scarcity-data-efficient-fine-tuning">Labeled Data Scarcity → Data-Efficient Fine-tuning</h4>
<ol type="1">
<li>Prompt Tuning: By converting the data points in the dataset into natural language prompts, the model may be easier to know what it should do.</li>
</ol>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616004626601.png" srcset="/img/loading.gif" lazyload alt="image-20250616004626601"><figcaption aria-hidden="true">image-20250616004626601</figcaption>
</figure>
<blockquote>
<p>Format the downstream task as a language modelling task with predefined templates into natural language prompts.</p>
</blockquote>
<p>What you need in prompt tuning:</p>
<ul>
<li>A prompt template: convert data points into a natural language prompt.</li>
<li>A PLM: perform language modeling.</li>
<li>A verbalizer: A mapping between the label and the vocabulary</li>
</ul>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616005202355.png" srcset="/img/loading.gif" lazyload alt="image-20250616005202355"><figcaption aria-hidden="true">image-20250616005202355</figcaption>
</figure>
<blockquote>
<p>Prompt tuning has better performance under data scarcity because it incorporates human knowledge and introduces no new parameters.</p>
</blockquote>
<ol start="2" type="1">
<li>Few-shot Learning: We have some labeled training data.</li>
</ol>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616005533670.png" srcset="/img/loading.gif" lazyload alt="image-20250616005533670"><figcaption aria-hidden="true">image-20250616005533670</figcaption>
</figure>
<ol start="3" type="1">
<li>Semi-Supervised learning: We have some labeled training data and a large amount of unlabeled data.</li>
</ol>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616005751658.png" srcset="/img/loading.gif" lazyload alt="image-20250616005751658"><figcaption aria-hidden="true">image-20250616005751658</figcaption>
</figure>
<p>Pattern-Exploiting Training (PET):</p>
<p>(1)Use different prompts and verbalizer to prompt-tune different PLMs on the labeled dataset.</p>
<p>(2)Predict the unlabeled dataset and combine the predictions from different models.</p>
<p>(3)Use a PLM with classifier head to train on the soft-labeled data set.</p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616010057896.png" srcset="/img/loading.gif" lazyload alt="image-20250616010057896"><figcaption aria-hidden="true">image-20250616010057896</figcaption>
</figure>
<ol start="4" type="1">
<li>Zero-shot inference: inference on the downstream task without any training data.</li>
</ol>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616010155364.png" srcset="/img/loading.gif" lazyload alt="image-20250616010155364"><figcaption aria-hidden="true">image-20250616010155364</figcaption>
</figure>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616010243043.png" srcset="/img/loading.gif" lazyload alt="image-20250616010243043"><figcaption aria-hidden="true">image-20250616010243043</figcaption>
</figure>
<h4 id="plms-are-gigantic-reducing-the-number-of-parameters">PLMs Are Gigantic → Reducing the Number of Parameters</h4>
<p><strong>Parameter-efficient fine-tuning: Reduce the task-specific parameters in downstream task</strong></p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616010550353.png" srcset="/img/loading.gif" lazyload alt="image-20250616010550353"><figcaption aria-hidden="true">image-20250616010550353</figcaption>
</figure>
<blockquote>
<p>Fine-tuning = modifying the hidden representation based on a PLM</p>
</blockquote>
<ol type="1">
<li><strong>Adapter</strong>: Use special submodules to modify hidden representations.</li>
</ol>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616010753826.png" srcset="/img/loading.gif" lazyload alt="image-20250616010753826"><figcaption aria-hidden="true">image-20250616010753826</figcaption>
</figure>
<blockquote>
<p>During fine-tuning, only update the adapters and the classifier head. All downstream tasks share the PLM, the adapters in each layer and the classifier heads are the task-specific modules.</p>
</blockquote>
<ol start="2" type="1">
<li><strong>LoRA</strong>: Low-Rank Adaptation of Large Language Models.</li>
</ol>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616011042979.png" srcset="/img/loading.gif" lazyload alt="image-20250616011042979"><figcaption aria-hidden="true">image-20250616011042979</figcaption>
</figure>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616011107134.png" srcset="/img/loading.gif" lazyload alt="image-20250616011107134"><figcaption aria-hidden="true">image-20250616011107134</figcaption>
</figure>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616011127575.png" srcset="/img/loading.gif" lazyload alt="image-20250616011127575"><figcaption aria-hidden="true">image-20250616011127575</figcaption>
</figure>
<blockquote>
<p>All downstream tasks share the PLM; the LoRA in each layer and the classifier heads are the task-specific modules.</p>
</blockquote>
<ol start="3" type="1">
<li><strong>Prefix Tuning</strong>: Insert trainable prefix in each layer.</li>
</ol>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616011311868.png" srcset="/img/loading.gif" lazyload alt="image-20250616011311868"><figcaption aria-hidden="true">image-20250616011311868</figcaption>
</figure>
<blockquote>
<p>Only the prefix (key and value) are updated during finetuning.</p>
</blockquote>
<ol start="4" type="1">
<li><strong>Soft Prompting</strong>: Prepend the prefix embedding at the input layer.</li>
</ol>
<blockquote>
<p>Soft Prompts: vectors (can be initialized from some word embeddings)</p>
</blockquote>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616011427459.png" srcset="/img/loading.gif" lazyload alt="image-20250616011427459"><figcaption aria-hidden="true">image-20250616011427459</figcaption>
</figure>
<p><strong>Benefits</strong>:</p>
<ol type="1">
<li>Drastically <strong>decreases</strong> the task-specific parameters.</li>
<li><strong>Less easier to overfit</strong> on training data; better out-of-domain performance.</li>
<li>Fewer parameters to <strong>fine-tune</strong>; a good candidate when training with small dataset.</li>
</ol>
<p><strong>Early exit: Reduce the models that are involved during inference</strong></p>
<p>Add a <strong>classifier</strong> at each layer and use a <strong>confidence predictor</strong> to decide which classifier to use.</p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616011936043.png" srcset="/img/loading.gif" lazyload alt="image-20250616011936043"><figcaption aria-hidden="true">image-20250616011936043</figcaption>
</figure>
<h2 id="self-supervised-learning-for-speech-and-image">Self-supervised Learning for Speech and Image</h2>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616220614941.png" srcset="/img/loading.gif" lazyload alt="image-20250616220614941"><figcaption aria-hidden="true">image-20250616220614941</figcaption>
</figure>
<h3 id="generative-approaches">Generative Approaches</h3>
<h4 id="bert-series-1">BERT series</h4>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616220841973.png" srcset="/img/loading.gif" lazyload alt="image-20250616220841973"><figcaption aria-hidden="true">image-20250616220841973</figcaption>
</figure>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1910.12638">Mockingjay: Unsupervised Speech Representation Learning with Deep Bidirectional Transformer Encoders</a></p>
<p>Mockingjay 是一种针对<strong>语音信号</strong>的自监督学习模型，属于生成式方法（Generative Approaches）。它通过重构被破坏的语音帧（Audio Frames）来学习语音表征，显著提升了语音识别、语音情感分析等下游任务的性能。</p>
<p><strong>Masking</strong>:</p>
<ul>
<li><p>Smoothness of acoustic features: Masking consecutive features</p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616221646340.png" srcset="/img/loading.gif" lazyload alt="image-20250616221646340"><figcaption aria-hidden="true">image-20250616221646340</figcaption>
</figure></li>
<li><p>Masking strategies for speech: Masking specific dimensions(<strong>Learn more speaker</strong> <strong>information in this way</strong>)</p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616221732877.png" srcset="/img/loading.gif" lazyload alt="image-20250616221732877"><figcaption aria-hidden="true">image-20250616221732877</figcaption>
</figure></li>
</ul>
<h4 id="gpt-series-1">GPT series</h4>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616222011145.png" srcset="/img/loading.gif" lazyload alt="image-20250616222011145"><figcaption aria-hidden="true">image-20250616222011145</figcaption>
</figure>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1910.12607">Generative Pre-Training for Speech with Autoregressive Predictive Coding</a></p>
<p>Autoregressive Predictive Coding（自回归预测编码，APC）是一种结合<strong>自回归建模</strong>（Autoregressive Modeling）和<strong>预测编码理论</strong>（Predictive Coding）的机器学习方法。</p>
<blockquote>
<p>通过预测未来数据点来学习当前数据的低维表示，同时利用预测误差驱动模型优化。</p>
</blockquote>
<blockquote>
<p>对于图像来说，以上两种方法均适用，将图像转化为<hanla></hanla>vector<hanla></hanla>即可。</p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616222200467.png" srcset="/img/loading.gif" lazyload alt="image-20250616222200467"><figcaption aria-hidden="true">image-20250616222200467</figcaption>
</figure>
</blockquote>
<h3 id="predictive-approach">Predictive Approach</h3>
<blockquote>
<p>No negative examples</p>
</blockquote>
<h4 id="image---predicting-rotation">Image - Predicting Rotation</h4>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.07728">Unsupervised Representation Learning by Predicting Image Rotations</a></p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616223103939.png" srcset="/img/loading.gif" lazyload alt="image-20250616223103939"><figcaption aria-hidden="true">image-20250616223103939</figcaption>
</figure>
<h4 id="image---context-prediction">Image - Context Prediction</h4>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1505.05192">Unsupervised Visual Representation Learning by Context Prediction</a></p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616223151920.png" srcset="/img/loading.gif" lazyload alt="image-20250616223151920"><figcaption aria-hidden="true">image-20250616223151920</figcaption>
</figure>
<p>仅需使用大规模无标签图像集，从单张图像中随机抽取成对的图像块，通过训练卷积神经网络预测第二块相对于第一块的方位坐标。通过这种图像内部上下文关系学习到的特征表示，能够有效捕捉图像之间的视觉相似性。</p>
<h4 id="predict-simplified-objects">Predict Simplified Objects</h4>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616223620605.png" srcset="/img/loading.gif" lazyload alt="image-20250616223620605"><figcaption aria-hidden="true">image-20250616223620605</figcaption>
</figure>
<p><u>For Speech</u>:</p>
<ul>
<li><p>HuBERT: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.07447">HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units</a></p>
<p>通过离线聚类步骤为类<hanla></hanla>BERT<hanla></hanla>预测损失提供对齐的目标标签。本方法的关键设计在于预测损失仅作用于掩蔽区域，迫使模型在连续输入上学习声学与语言信息的联合建模。HuBERT<hanla></hanla>的核心优势依赖于无监督聚类步骤的稳定性，而非所分配聚类标签的固有质量。</p></li>
<li><p>BEST: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2202.01855">Self-supervised Learning with Random-projection Quantizer for Speech Recognition</a></p>
<p>本方法训练了一个预测掩蔽语音信号的模型，其预测目标形式为通过随机投影量化器生成的离散标签。该量化器使用随机初始化的矩阵对语音输入进行投影，并在随机初始化的码本中执行最近邻搜索匹配。</p></li>
</ul>
<p><u>For Image</u>:</p>
<ul>
<li><p>DeepCluster: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1807.05520">Deep Clustering for Unsupervised Learning of Visual Features</a></p>
<p>在每次迭代中通过标准聚类算法（k<hanla></hanla>均值）对特征执行聚类分组，并将后续分配结果作为监督信号，依此更新网络的权重参数。</p></li>
</ul>
<h3 id="contrastive-learning">Contrastive Learning</h3>
<h4 id="simclr">SimCLR</h4>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.13991">Speech SIMCLR: Combining Contrastive and Reconstruction Objective for Self-supervised Speech Representation Learning</a></p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616225101928.png" srcset="/img/loading.gif" lazyload alt="image-20250616225101928"><figcaption aria-hidden="true">image-20250616225101928</figcaption>
</figure>
<h4 id="moco">MoCo</h4>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1911.05722">Momentum Contrast for Unsupervised Visual Representation Learning</a></p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616225212681.png" srcset="/img/loading.gif" lazyload alt="image-20250616225212681"><figcaption aria-hidden="true">image-20250616225212681</figcaption>
</figure>
<p>MoCo(Momentum Contrast) 是一种用于无监督视觉表征学习的动量对比算法，基于将对比学习视为字典查询的视角，本方法通过队列机制与动量平均编码器构建动态字典。该设计实现实时构建大规模且一致的字典，从而显著提升对比式无监督学习效果。</p>
<h4 id="vq-wav2vec">VQ-wav2vec</h4>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1910.05453">vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations</a></p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616225622543.png" srcset="/img/loading.gif" lazyload alt="image-20250616225622543"><figcaption aria-hidden="true">image-20250616225622543</figcaption>
</figure>
<p><strong>VQ-wav2vec + BERT</strong>:</p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616225805703.png" srcset="/img/loading.gif" lazyload alt="image-20250616225805703"><figcaption aria-hidden="true">image-20250616225805703</figcaption>
</figure>
<p><strong>Wav2vec 2.0</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2006.11477">wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations</a></p>
<p>Continuous input is critical. Quantized target improves performance.</p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616230029573.png" srcset="/img/loading.gif" lazyload alt="image-20250616230029573"><figcaption aria-hidden="true">image-20250616230029573</figcaption>
</figure>
<p>Why not formulated as typical classification?</p>
<blockquote>
<p>原因猜测：音频作为输入经过编码器的输出纷繁复杂，不利于作为简单的分类问题进行解决。</p>
</blockquote>
<h3 id="bootstrapping-approaches">Bootstrapping Approaches</h3>
<blockquote>
<p>No negative examples</p>
</blockquote>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616230309585.png" srcset="/img/loading.gif" lazyload alt="image-20250616230309585"><figcaption aria-hidden="true">image-20250616230309585</figcaption>
</figure>
<blockquote>
<p>类似一种<hanla></hanla>Typical Knowledge Distillation<hanla></hanla>方式，Teacher Encoder<hanla></hanla>的参数保持不变，通过计算损失进行<hanla></hanla>gradient descent，更新<hanla></hanla>Student Encoder<hanla></hanla>的参数，然后将其复制回<hanla></hanla>Teacher Encoder<hanla></hanla>中，如此循环往复。</p>
</blockquote>
<ul>
<li>Image
<ul>
<li>Bootstrap your own latent (BYOL)：https://arxiv.org/abs/2006.07733</li>
<li>Simple Siamese (SimSiam)：https://arxiv.org/abs/2011.10566</li>
</ul></li>
<li>Speech
<ul>
<li>Data2vec: the student learns from multiple layers of the teacher：https://arxiv.org/abs/2202.03555</li>
</ul></li>
</ul>
<h3 id="simply-extra-regularization">Simply Extra Regularization</h3>
<blockquote>
<p>No negative examples</p>
</blockquote>
<h4 id="variance-invariance-covariance-regularization-vicreg">Variance-Invariance-Covariance Regularization ( VICReg )</h4>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2105.04906">VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning</a></p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250616231158232.png" srcset="/img/loading.gif" lazyload alt="image-20250616231158232"><figcaption aria-hidden="true">image-20250616231158232</figcaption>
</figure>
<blockquote>
<p>通过给<hanla></hanla>Encoder<hanla></hanla>的输出的某一<hanla></hanla>/<hanla></hanla>些维度的方差设置一个阈值，可以使其输出结果的分布不再趋同，增加多样性，避免坍缩问题的发生。</p>
</blockquote>
<h2 id="homework-7-extractive-question-answering">Homework 7: Extractive Question Answering</h2>
<h3 id="dataset-drcd-odsqa">Dataset: DRCD &amp; ODSQA</h3>
<p><strong>DRCD</strong>: Delta Reading Comprehension Dataset <strong>ODSQA</strong>: Open-Domain Spoken Question Answering Dataset</p>
<ul>
<li>train: DRCD + DRCD-TTS
<ul>
<li>10524 paragraphs, 31690 questions</li>
</ul></li>
<li>dev: DRCD + DRCD-TTS
<ul>
<li>1490 paragraphs, 4131 questions</li>
</ul></li>
<li>test: DRCD + ODSQA
<ul>
<li>1586 paragraphs, 4957 questions</li>
</ul></li>
</ul>
<p><strong>Chinese Reading Comprehension</strong></p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250617170926905.png" srcset="/img/loading.gif" lazyload alt="image-20250617170926905"><figcaption aria-hidden="true">image-20250617170926905</figcaption>
</figure>
<h3 id="baselines-2">Baselines</h3>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250617171035588.png" srcset="/img/loading.gif" lazyload alt="image-20250617171035588"><figcaption aria-hidden="true">image-20250617171035588</figcaption>
</figure>
<h3 id="simple-baseline-4">Simple Baseline</h3>
<p>Sample code: <a target="_blank" rel="noopener" href="https://github.com/Striver98/Machine-Learning-Homework/blob/master/HW7/HW7_simple.py">link</a></p>
<p>Output:</p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250617190346928.png" srcset="/img/loading.gif" lazyload alt="image-20250617190346928"><figcaption aria-hidden="true">image-20250617190346928</figcaption>
</figure>
<h3 id="medium-baseline-4">Medium Baseline</h3>
<h4 id="apply-linear-learning-rate-decay">Apply linear learning rate decay</h4>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 学习率调度器，线性衰减</span><br><span class="hljs-comment"># 计算总步数<hanla></hanla></span><br>total_steps = <span class="hljs-built_in">len</span>(train_loader) * num_epoch<br><span class="hljs-comment"># 学习率衰减函数，线性衰减到<hanla></hanla>0</span><br>lr_lambda = <span class="hljs-keyword">lambda</span> step: <span class="hljs-built_in">max</span>(<span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span> - step / total_steps)<br><span class="hljs-comment"># 创建学习率调度器<hanla></hanla></span><br>scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)<br></code></pre></td></tr></tbody></table></figure>
<h4 id="change-value-of-doc_stride">Change value of “doc_stride”</h4>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-variable language_">self</span>.doc_stride = <span class="hljs-number">75</span> <span class="hljs-comment"># 变为原来的一半</span><br></code></pre></td></tr></tbody></table></figure>
<p>Code: <a target="_blank" rel="noopener" href="https://github.com/Striver98/Machine-Learning-Homework/blob/master/HW7/HW7_medium.py">link</a></p>
<p>Output:</p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250617192845564.png" srcset="/img/loading.gif" lazyload alt="image-20250617192845564"><figcaption aria-hidden="true">image-20250617192845564</figcaption>
</figure>
<h3 id="strong-baseline-4">Strong Baseline</h3>
<h4 id="improve-preprocessing">Improve preprocessing</h4>
<p>训练的数据处理是直接以答案为中心选择文本窗口，这导致模型可能学到一个不该学习的模式：答案就在中间。</p>
<blockquote>
<p>采用<strong>随机窗口位置</strong>：在训练时，不要总以答案为中心截取窗口，而是让窗口的起点在包含答案的所有合法位置中<strong>随机采样</strong>。这样，答案在窗口中的位置是随机的，模型不会学到<hanla></hanla>“答案总在中间”。</p>
</blockquote>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.split == <span class="hljs-string">"train"</span>:<br>    answer_start_token = tokenized_paragraph.char_to_token(question[<span class="hljs-string">"answer_start"</span>])<br>    answer_end_token = tokenized_paragraph.char_to_token(question[<span class="hljs-string">"answer_end"</span>])<br>    L = <span class="hljs-built_in">len</span>(tokenized_paragraph)<br>    min_start = <span class="hljs-built_in">max</span>(<span class="hljs-number">0</span>, answer_end_token - <span class="hljs-variable language_">self</span>.max_paragraph_len + <span class="hljs-number">1</span>)<br>    max_start = <span class="hljs-built_in">min</span>(answer_start_token, L - <span class="hljs-variable language_">self</span>.max_paragraph_len)<br>    <span class="hljs-keyword">if</span> max_start &lt; min_start:<br>        paragraph_start = min_start  <span class="hljs-comment"># 兜底，窗口只能唯一确定</span><br>    <span class="hljs-keyword">else</span>:<br>        paragraph_start = random.randint(min_start, max_start)<br>    paragraph_end = paragraph_start + <span class="hljs-variable language_">self</span>.max_paragraph_len<br></code></pre></td></tr></tbody></table></figure>
<h4 id="try-other-pretrained-models">Try other pretrained models</h4>
<p><a target="_blank" rel="noopener" href="https://huggingface.co/models">Models - Hugging Face</a></p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250617213103560.png" srcset="/img/loading.gif" lazyload alt="image-20250617213103560"><figcaption aria-hidden="true">image-20250617213103560</figcaption>
</figure>
<p>Output: <a target="_blank" rel="noopener" href="https://github.com/Striver98/Machine-Learning-Homework/blob/master/HW7/HW7_strong.py">link</a></p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250617230753387.png" srcset="/img/loading.gif" lazyload alt="image-20250617230753387"><figcaption aria-hidden="true">image-20250617230753387</figcaption>
</figure>
<h1 id="auto-encoder">Auto-encoder</h1>
<h2 id="basic-idea-of-auto-encoder">Basic Idea of Auto-encoder</h2>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622152152202.png" srcset="/img/loading.gif" lazyload alt="image-20250622152152202"><figcaption aria-hidden="true">image-20250622152152202</figcaption>
</figure>
<p>自编码器（Auto-encoder）是一种<strong>无监督学习的神经网络模型</strong>，其核心思想是通过学习数据的低维表示（编码）来实现数据的压缩与重建。它的设计灵感来源于<hanla></hanla>“信息瓶颈”<hanla></hanla>概念，旨在捕捉数据中最关键的特征，同时忽略冗余信息。</p>
<blockquote>
<p>核心任务是让<strong>输出尽可能复现输入</strong>。</p>
</blockquote>
<ol type="1">
<li>Encoder: 将高维输入数据（如图像、文本）压缩为低维潜在表示（Latent Representation），称为<hanla></hanla>“瓶颈层”（Bottleneck Layer）。</li>
<li>Decoder: 从低维潜在表示中重建原始输入数据。</li>
</ol>
<blockquote>
<p>自编码器的训练无需人工标注，其标签即为输入数据本身。</p>
</blockquote>
<p><strong>自编码器的<hanla></hanla>Dimension reduction 与<hanla></hanla>PCA<hanla></hanla>有何区别？</strong></p>
<blockquote>
<ul>
<li>PCA<hanla></hanla>是线性降维，而自编码器通过非线性激活函数捕捉复杂模式，适用于非线性数据。</li>
</ul>
</blockquote>
<p><strong>Why Auto-encoder work?</strong></p>
<blockquote>
<ol type="1">
<li><p>低维瓶颈层强制模型保留关键信息，丢弃次要细节（如图像背景噪声）。</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622153041762.png" srcset="/img/loading.gif" lazyload></p></li>
<li><p>潜在表示<hanla></hanla><span class="math inline"><em>z</em></span><hanla></hanla>是数据的<hanla></hanla>“抽象特征”，可视为数据的嵌入式表示（Embedding），适用于下游任务（如分类、聚类）。</p></li>
</ol>
</blockquote>
<h3 id="de-noising-auto-encoderdae">De-noising Auto-encoder(DAE)</h3>
<p>De-noising Auto-encoder（降噪自编码器，DAE）是一种改进的自编码器（Autoencoder），通过在输入数据中主动添加噪声并学习恢复原始数据，提升模型的鲁棒性和特征表示能力。</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622153405707.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<p>The idea of DAE is also used in BERT.</p>
</blockquote>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622153841583.png" srcset="/img/loading.gif" lazyload></p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;"><strong>维度</strong></th>
<th style="text-align: center;"><strong>DAE</strong></th>
<th style="text-align: center;"><strong>BERT</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><strong>核心机制</strong></td>
<td style="text-align: center;">破坏-重建</td>
<td style="text-align: center;">掩码预测（MLM）</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>输入类型</strong></td>
<td style="text-align: center;">图像<hanla></hanla>/<hanla></hanla>文本向量</td>
<td style="text-align: center;">文本<hanla></hanla>Token<hanla></hanla>序列</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>架构</strong></td>
<td style="text-align: center;">编码器-解码器对称结构</td>
<td style="text-align: center;">Transformer<hanla></hanla>纯编码器</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>训练目标</strong></td>
<td style="text-align: center;">重建原始输入</td>
<td style="text-align: center;">预测掩码词 + 句子关系</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>生成能力</strong></td>
<td style="text-align: center;">支持（通过解码器）</td>
<td style="text-align: center;">不支持（无显式解码器）</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>上下文建模</strong></td>
<td style="text-align: center;">局部或无上下文依赖</td>
<td style="text-align: center;">全局双向上下文</td>
</tr>
</tbody>
</table>
<h2 id="feature-disentanglement">Feature Disentanglement</h2>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622154012626.png" srcset="/img/loading.gif" lazyload></p>
<p>在深度学习中，原始数据（如图像、语音或文本）经过神经网络编码后，得到的特征向量往往是混合的，即一个维度的变化可能对应多个因素的共同作用。</p>
<p><strong>Feature Disentanglement 的目标是</strong>： 将这种混合的特征表示分解成<strong>相互独立、互不干扰的子表示</strong>，每个子表示对应一个<strong>语义独立的因子</strong>，如：</p>
<ul>
<li>图像中<hanla></hanla>“姿态”、“光照”、“物体种类”</li>
<li>语音中<hanla></hanla>“说话人身份”、“情绪”、“内容”</li>
<li>文本中<hanla></hanla>“语气”、“主题”、“句式结构”</li>
</ul>
<p>这样可以使模型更具可解释性、泛化能力更强，并更容易进行控制与生成。</p>
<h3 id="application-voice-conversion">Application: Voice Conversion</h3>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622160528098.png" srcset="/img/loading.gif" lazyload></p>
<p>通过<hanla></hanla>Feature Disentanglement<hanla></hanla>可以将<hanla></hanla>speaker<hanla></hanla>与<hanla></hanla>content<hanla></hanla>分离，实现语音转换。</p>
<h2 id="discrete-latent-representation">Discrete Latent Representation</h2>
<p>在深度学习中，<strong>潜在表示（latent representation）</strong>是指模型从原始输入中提取的内部特征编码，通常位于自动编码器（Autoencoder）或生成模型（如<hanla></hanla>VAE、GAN）的中间层。而 <strong>“离散潜在表示”</strong> 则是指这些潜在特征在潜空间中是<strong>离散的值</strong>，而不是连续的实数向量。</p>
<h3 id="vector-quantized-variational-auto-encoder-vqvae">Vector Quantized Variational Auto encoder (VQVAE)</h3>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622161118850.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="structure">Structure</h4>
<ol type="1">
<li>Encoder: 输入<hanla></hanla><span class="math inline"><em>x</em></span><hanla></hanla>通过编码器得到一个连续的表示：<span class="math inline"><em>z</em><sub><em>e</em></sub>(<em>x</em>) ∈ ℝ<sup><em>d</em></sup></span></li>
<li>Vector Quantization: 引入一个<strong>码本（codebook）</strong>，<span class="math inline"><em>ε</em> = <em>e</em><sub>1</sub>, <em>e</em><sub>2</sub>, ..., <em>e</em><sub><em>k</em></sub> ⊂ ℝ<sup><em>d</em></sup></span>，包含<hanla></hanla><span class="math inline"><em>K</em></span><hanla></hanla>个向量。
<ul>
<li>编码器输出会被量化为码本中最接近的向量：<span class="math inline"><em>z</em><sub><em>q</em></sub>(<em>x</em>) = <em>Q</em><em>u</em><em>a</em><em>n</em><em>t</em><em>i</em><em>z</em><em>e</em>(<em>z</em><sub><em>e</em></sub>(<em>x</em>)) = <em>e</em><sub><em>k</em></sub></span>, where <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex;" xmlns="http://www.w3.org/2000/svg" width="24.416ex" height="2.363ex" role="img" focusable="false" viewBox="0 -750 10791.7 1044.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g><g data-mml-node="mo" transform="translate(798.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(1854.6,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(2383.6,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(2834.6,0)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mi" transform="translate(3311.6,0)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(4189.6,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="msub" transform="translate(4534.6,0)"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(633,-150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g><g data-mml-node="mo" transform="translate(5508.9,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="msub" transform="translate(5786.9,0)"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="mi" transform="translate(498,-150) scale(0.707)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g></g><g data-mml-node="mo" transform="translate(6664.4,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(7053.4,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(7625.4,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(8236.6,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="msub" transform="translate(9236.8,0)"><g data-mml-node="mi"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(499,-150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g><g data-mml-node="msub" transform="translate(10077.2,0)"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mn" transform="translate(311,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g></g></svg></mjx-container></li>
<li>所以<hanla></hanla><span class="math inline"><em>z</em><sub><em>q</em></sub>(<em>x</em>)</span><hanla></hanla>是一个离散向量。</li>
</ul></li>
<li>Decoder: 用离散后的<hanla></hanla><span class="math inline"><em>z</em><sub><em>q</em></sub>(<em>x</em>)</span><hanla></hanla>重建输入：<span class="math inline"><em>x̂</em> = <em>D</em><em>e</em><em>c</em><em>o</em><em>d</em><em>e</em><em>r</em>(<em>z</em><sub><em>q</em></sub>(<em>x</em>))</span></li>
</ol>
<p><strong>离散向量和连续向量有什么区别?</strong></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 连续向量示例：向量每一维都是实数，可以是语义混合的，比如人脸的<hanla></hanla>“年龄”、“光照”、“角度”<hanla></hanla>等<hanla></hanla></span><br>z = [<span class="hljs-number">0.12</span>, -<span class="hljs-number">0.53</span>, <span class="hljs-number">0.87</span>, <span class="hljs-number">0.03</span>]<br><br><span class="hljs-comment"># 离散向量示例：</span><br>z = [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>]  <span class="hljs-comment"># 一个 one-hot 向量，表示第 3 类<hanla></hanla></span><br>z = <span class="hljs-number">2</span>  <span class="hljs-comment"># 一个<hanla></hanla>“码本索引”，表示第 2 个嵌入向量</span><br></code></pre></td></tr></tbody></table></figure>
<p><strong>离散向量是不可导的，VQ-VAE<hanla></hanla>如何解决梯度问题？</strong></p>
<blockquote>
<p><strong>Straight-Through Estimator（STE）</strong>: 在前向传播时，使用量化结果，而在反向传播时，把梯度直接传回给未量化的<hanla></hanla><span class="math inline"><em>z</em><sub><em>e</em></sub>(<em>x</em>)</span>，忽略量化步骤的不可导性。</p>
</blockquote>
<h3 id="text-as-representation">Text as Representation</h3>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.02851">Learning to Encode Text as Human-Readable Summaries using Generative Adversarial Networks</a></p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622163628467.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<p>This is cycle GAN</p>
</blockquote>
<h3 id="tree-as-embedding">Tree as Embedding</h3>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622163729186.png" srcset="/img/loading.gif" lazyload></p>
<p><strong>Tree as Embedding（树作为嵌入）</strong> 是一种将<strong>树结构信息编码为向量表示（embedding）</strong>的方法，旨在将具有分层或结构化语义的数据（如语法树、抽象语法树<hanla></hanla>AST、XML、知识图谱子树）映射到一个固定维度的向量空间中，从而使树结构能作为神经网络的输入进行建模与学习。</p>
<blockquote>
<p>可以理解为将树结构转换成神经网络能处理的向量，同时保留其语义与结构特征。</p>
</blockquote>
<h2 id="more-applications">More Applications</h2>
<h3 id="generator">Generator</h3>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622164524166.png" srcset="/img/loading.gif" lazyload></p>
<p>With some modification, we have <strong>variational auto-encoder (VAE)</strong>.</p>
<p><strong>Variational Autoencoder（VAE）</strong> 是一种<strong>生成模型（Generative Model）</strong>，它结合了<strong>自编码器（Autoencoder）与变分推断（Variational Inference）</strong>，在保持端到端训练的同时，<strong>学习一个可控、连续的潜在空间（latent space）</strong>，从而可以生成新的、真实感很强的数据。</p>
<blockquote>
<p>用神经网络学习隐变量的概率分布，然后从中采样以重构输入，进而生成新数据。</p>
</blockquote>
<figure class="highlight css"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-selector-tag">Input</span> <span class="hljs-attribute">x</span><br>   ↓<br>Encoder: μ(x), σ(x)<br>   ↓       \<br>Sample z = μ + σ·ε  ← ε ~ <span class="hljs-built_in">N</span>(<span class="hljs-number">0</span>, I)<br>   ↓<br>Decoder: x̂ = <span class="hljs-built_in">f</span>(z)<br>   ↓<br>Compare x̂ vs. x → 重构损失 + KL 正则<br><br></code></pre></td></tr></tbody></table></figure>
<h3 id="compression">Compression</h3>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1703.00395">Lossy Image Compression with Compressive Autoencoders</a></p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622165031545.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="anomaly-detection">Anomaly Detection</h3>
<ul>
<li>Given a set of training data <span class="math inline">{<em>x</em><sup>1</sup>, <em>x</em><sup>2</sup>, ..., <em>x</em><sup><em>N</em></sup>}</span></li>
<li>Detecting input <span class="math inline"><em>x</em></span> is similar to training data or not.</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622165625175.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="case-1-with-classifier">Case 1: With Classifier</h4>
<p><strong>本质上是将问题</strong>转化为二分类任务。</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622170115475.png" srcset="/img/loading.gif" lazyload></p>
<p>Anomaly Detection:</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622170247135.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<p>Confidence: the maximum scores or negative Entropy.</p>
</blockquote>
<p><strong>Example Framework</strong></p>
<p><u><strong>Training Set</strong></u>: Images <span class="math inline"><em>x</em></span> of characters from Simpsons.</p>
<ul>
<li>Each image <span class="math inline"><em>𝑥</em></span> is labelled by its characters <span class="math inline"><em>ŷ</em></span></li>
<li>Train a classifier, and we can obtain confidence score <span class="math inline"><em>c</em>(<em>x</em>)</span> from the classifier.</li>
</ul>
<p><u><strong>Dev Set</strong></u>: Label each image <span class="math inline"><em>𝑥</em></span> is from Simpsons or not.</p>
<ul>
<li>We can compute the <strong>performance</strong> of <span class="math inline"><em>f</em>(<em>x</em>)</span></li>
<li>Using dev set to determine <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.027ex;" xmlns="http://www.w3.org/2000/svg" width="1.319ex" height="1.597ex" role="img" focusable="false" viewBox="0 -694 583 706"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D706" d="M166 673Q166 685 183 694H202Q292 691 316 644Q322 629 373 486T474 207T524 67Q531 47 537 34T546 15T551 6T555 2T556 -2T550 -11H482Q457 3 450 18T399 152L354 277L340 262Q327 246 293 207T236 141Q211 112 174 69Q123 9 111 -1T83 -12Q47 -12 47 20Q47 37 61 52T199 187Q229 216 266 252T321 306L338 322Q338 323 288 462T234 612Q214 657 183 657Q166 657 166 673Z"></path></g></g></g></svg></mjx-container> and other hyperparameters.</li>
</ul>
<p><u><strong>Testing Set</strong></u>: Images <span class="math inline"><em>𝑥</em></span> is from Simpsons or not.</p>
<p><strong>Evaluation</strong></p>
<ul>
<li>Accuracy is not a good measurement!</li>
<li>A system can have high accuracy, but do nothing.</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622171711720.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<p>第一类错误与第二类错误</p>
</blockquote>
<h4 id="case-2-without-labels">Case 2: Without Labels</h4>
<ul>
<li>Given a set of training data <span class="math inline">{<em>x</em><sup>1</sup>, <em>x</em><sup>2</sup>, ..., <em>x</em><sup><em>N</em></sup>}</span></li>
<li>We want to find a function detecting input <span class="math inline"><em>x</em></span> is similar to training data or not.</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622172156784.png" srcset="/img/loading.gif" lazyload></p>
<p><strong>Maximum Likelihood</strong></p>
<p>Assuming the data points is sampled from a probability density function <span class="math inline"><em>f</em><sub><em>θ</em></sub>(<em>x</em>)</span></p>
<ul>
<li><span class="math inline"><em>θ</em></span> determines the shape of <span class="math inline"><em>f</em><sub><em>θ</em></sub>(<em>x</em>)</span></li>
<li><span class="math inline"><em>θ</em></span> is unknown, to be found from data</li>
<li><span class="math inline"><em>L</em>(<em>θ</em>) = <em>f</em><sub><em>θ</em></sub>(<em>x</em><sup>1</sup>)<em>f</em><sub><em>θ</em></sub>(<em>x</em><sup>2</sup>)...<em>f</em><sub><em>θ</em></sub>(<em>x</em><sup><em>N</em></sup>)</span> : Likelihood</li>
<li><span class="math inline"><em>θ</em><sup>⋆</sup> = <em>a</em><em>r</em><em>g</em>&nbsp;<em>m</em><em>a</em><em>x</em><sub><em>θ</em></sub><em>L</em>(<em>θ</em>)</span></li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622172903448.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622172931921.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622173014790.png" srcset="/img/loading.gif" lazyload></p>
<p>The colors represents the value of <span class="math inline"><em>f</em><sub><em>μ</em><sup>⋆</sup>, ∑<sup>⋆</sup></sub>(<em>x</em>)</span></p>
<h4 id="concluding-remarks">Concluding Remarks</h4>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622173208811.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="homework-8-anomaly-detection">Homework 8: Anomaly Detection</h2>
<h3 id="task-introduction-1">Task introduction</h3>
<p><strong>Unsupervised anomaly detection</strong>: Training a model to determine whether the given image is similar with the training data.</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622194705557.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="data">Data</h3>
<ul>
<li>Training data
<ul>
<li>100000 human faces</li>
</ul></li>
<li>Testing data
<ul>
<li>About 10000 from the same distribution with training data (label 0)</li>
<li>About 10000 from another distribution (anomalies, label 1)</li>
</ul></li>
<li>Format
<ul>
<li>data/ |—– trainingset.npy |—– testingset.npy</li>
<li>Shape: (#images, 64, 64, 3) for each .npy file</li>
</ul></li>
</ul>
<h3 id="methodology">Methodology</h3>
<ul>
<li>Train an autoencoder with small reconstruction error.</li>
<li>During inference, we can use reconstruction error as anomaly score.
<ul>
<li>Anomaly score can be seen as the degree of abnormality of an image.</li>
<li>An image from unseen distribution should have higher reconstruction error.</li>
</ul></li>
<li>Anomaly scores are used as our predicted values.</li>
</ul>
<h3 id="evaluation---roc-auc-score">Evaluation - ROC AUC score</h3>
<ul>
<li>TPR = TP / (TP + FN)</li>
<li>FPR = FP / (FP + TN)</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622194951599.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="simple-baseline-5">Simple Baseline</h3>
<p>Sample code: <a target="_blank" rel="noopener" href="https://github.com/Striver98/Machine-Learning-Homework/blob/master/HW8/HW8_simple.py">link</a></p>
<p>Output:</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622195753771.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="medium-baseline-5">Medium Baseline</h3>
<p>Code: <a target="_blank" rel="noopener" href="https://github.com/Striver98/Machine-Learning-Homework/blob/master/HW8/HW8_medium.py">link</a></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 卷积自动编码器模型 - 改进版本（更少层数）</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">conv_autoencoder</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(conv_autoencoder, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-comment"># 简化的编码器 - 减少层数</span><br>        <span class="hljs-variable language_">self</span>.encoder = nn.Sequential(  <br>            nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">16</span>, <span class="hljs-number">4</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>),  <span class="hljs-comment"># 3<hanla></hanla>通道→16<hanla></hanla>通道 (64x64→32x32)</span><br>            nn.ReLU(),<br>            nn.Conv2d(<span class="hljs-number">16</span>, <span class="hljs-number">32</span>, <span class="hljs-number">4</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>),  <span class="hljs-comment"># 16<hanla></hanla>通道→32<hanla></hanla>通道 (32x32→16x16)</span><br>            nn.ReLU(),<br>            <span class="hljs-comment"># 移除第三个卷积层，减少模型复杂度</span><br>        )<br>        <span class="hljs-comment"># 简化的解码器 - 减少层数</span><br>        <span class="hljs-variable language_">self</span>.decoder = nn.Sequential(  <br>            nn.ConvTranspose2d(<span class="hljs-number">32</span>, <span class="hljs-number">16</span>, <span class="hljs-number">4</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>),  <span class="hljs-comment"># 32<hanla></hanla>通道→16<hanla></hanla>通道 (16x16→32x32)</span><br>            nn.ReLU(),<br>            nn.ConvTranspose2d(<span class="hljs-number">16</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>),  <span class="hljs-comment"># 16<hanla></hanla>通道→3<hanla></hanla>通道 (32x32→64x64)</span><br>            nn.Tanh(),  <span class="hljs-comment"># 输出范围限制在<hanla></hanla>[-1,1]</span><br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = <span class="hljs-variable language_">self</span>.encoder(x)  <span class="hljs-comment"># 通过编码器</span><br>        x = <span class="hljs-variable language_">self</span>.decoder(x)  <span class="hljs-comment"># 通过解码器</span><br>        <span class="hljs-keyword">return</span> x<br><br><span class="hljs-comment"># 训练配置 - 改进版本</span><br><span class="hljs-comment"># 训练超参数<hanla></hanla></span><br>num_epochs = <span class="hljs-number">50</span>  <span class="hljs-comment"># 训练轮数<hanla></hanla></span><br>batch_size = <span class="hljs-number">256</span>  <span class="hljs-comment"># 更小的批次大小（从<hanla></hanla>2000<hanla></hanla>减少到<hanla></hanla>256）</span><br>learning_rate = <span class="hljs-number">1e-3</span>  <span class="hljs-comment"># 学习率保持不变</span><br><br><span class="hljs-comment"># 构建训练数据加载器<hanla></hanla></span><br>x = torch.from_numpy(train)  <span class="hljs-comment"># 将<hanla></hanla>NumPy<hanla></hanla>数组转换为<hanla></hanla>PyTorch<hanla></hanla>张量<hanla></hanla></span><br>train_dataset = CustomTensorDataset(x)  <span class="hljs-comment"># 创建训练数据集<hanla></hanla></span><br><br>train_sampler = RandomSampler(train_dataset)  <span class="hljs-comment"># 创建随机采样器<hanla></hanla></span><br>train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size, num_workers=<span class="hljs-number">0</span>)  <span class="hljs-comment"># 使用更小的批次大小</span><br><br><span class="hljs-comment"># 模型选择 - 使用改进的<hanla></hanla>CNN</span><br>model_type = <span class="hljs-string">'cnn'</span>   <span class="hljs-comment"># 选择<hanla></hanla>CNN<hanla></hanla>模型类型<hanla></hanla></span><br>model_classes = {<span class="hljs-string">'fcn'</span>: fcn_autoencoder(), <span class="hljs-string">'cnn'</span>: conv_autoencoder(), <span class="hljs-string">'vae'</span>: VAE()}  <span class="hljs-comment"># 模型类型映射字典<hanla></hanla></span><br>model = model_classes[model_type].cuda()  <span class="hljs-comment"># 实例化选择的模型并移至<hanla></hanla>GPU</span><br><br><span class="hljs-comment"># 推理阶段 - 也使用更小的批次大小<hanla></hanla></span><br>eval_batch_size = <span class="hljs-number">64</span>  <span class="hljs-comment"># 更小的评估批次大小（从<hanla></hanla>200<hanla></hanla>减少到<hanla></hanla>64）</span><br><br><span class="hljs-comment"># 构建测试数据加载器<hanla></hanla></span><br>data = torch.tensor(test, dtype=torch.float32)  <span class="hljs-comment"># 将测试数据转换为<hanla></hanla>PyTorch<hanla></hanla>张量<hanla></hanla></span><br>test_dataset = CustomTensorDataset(data)  <span class="hljs-comment"># 创建测试数据集<hanla></hanla></span><br>test_sampler = SequentialSampler(test_dataset)  <span class="hljs-comment"># 创建顺序采样器<hanla></hanla></span><br>test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=eval_batch_size, num_workers=<span class="hljs-number">0</span>)<br></code></pre></td></tr></tbody></table></figure>
<p>Output:</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622200955334.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="strong-baseline-5">Strong Baseline</h3>
<p><strong>Adjust model structure</strong>: Try smaller models (less layers), Smaller batch size.</p>
<p>Code: <a target="_blank" rel="noopener" href="https://github.com/Striver98/Machine-Learning-Homework/blob/master/HW8/HW8_strong.py">link</a></p>
<p>Output:</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250622234910690.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="explainable-machine-learning">Explainable Machine Learning</h1>
<h2 id="why-we-need-explainable-ml">Why we need Explainable ML?</h2>
<ul>
<li>Loan issuers are required by law to explain their models. (金融贷款)</li>
<li>Medical diagnosis model is responsible for human life. Can it be a black box? (医疗诊断)</li>
<li>If a model is used at the court, we must make sure the model behaves in a nondiscriminatory manner. (法庭判决)</li>
<li>If a self driving car suddenly acts abnormally, we need to explain why. (自动驾驶)</li>
</ul>
<blockquote>
<p>We can improve ML model based on explanation.</p>
</blockquote>
<h2 id="interpretable-v.s.-powerful">Interpretable v.s. Powerful</h2>
<ul>
<li><p>Some models are intrinsically interpretable. For example, <strong>linear model</strong> (from weights, you know the importance of features), but not very powerful.</p></li>
<li><p>Deep network is difficult to interpretable. <strong>Deep networks</strong> are black boxes , but powerful than a linear model.</p></li>
<li><p>Are there some models interpretable and powerful at the same time?</p>
<ul>
<li><p>Maybe <strong>decision tree</strong>？A tree can still be terrible and we often use a forest instead of just one tree.</p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250623165028987.png" srcset="/img/loading.gif" lazyload alt="image-20250623165028987"><figcaption aria-hidden="true">image-20250623165028987</figcaption>
</figure></li>
</ul></li>
</ul>
<h2 id="goal-of-explainable-ml">Goal of Explainable ML</h2>
<ol type="1">
<li><p><strong>建立信任</strong>：让用户知道模型为什么这么预测，特别是在医疗、金融等高风险领域。</p></li>
<li><p><strong>发现问题</strong>：帮助识别数据中的偏差、错误或模型学到的<hanla></hanla>“伪规律”。</p></li>
<li><p><strong>辅助决策</strong>：让人能根据模型的解释更合理地做出决策，而不是盲目依赖模型。</p></li>
<li><p><strong>满足法规</strong>：符合法律要求，提供决策依据和解释。</p></li>
<li><p><strong>促进科研</strong>：通过解释模型，帮助发现新的知识和潜在规律。</p></li>
</ol>
<p>In a word, Make people (your customers, your boss, yourself) comfortable.</p>
<h2 id="explainable-ml">Explainable ML</h2>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250623170026200.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="local-explanation-explain-the-decision">Local Explanation: Explain the Decision</h3>
<p><strong>Local Explanation（局部解释）</strong> 是指：</p>
<blockquote>
<p><strong>在特定样本（某一条预测）附近，对模型做出的预测结果进行解释</strong>，而不是解释整个模型的整体行为。</p>
</blockquote>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250623170243265.png" srcset="/img/loading.gif" lazyload></p>
<p>Which component is critical for making decision?</p>
<p>Object <span class="math inline"><em>x</em></span> : Image, text, etc.</p>
<p>Components: <span class="math inline">{<em>X</em><sub>1</sub>, ..., <em>x</em><sub><em>n</em></sub>, ..., <em>x</em><sub><em>N</em></sub>}</span></p>
<blockquote>
<p>Image: pixel, segment, etc. Text: a word</p>
</blockquote>
<ul>
<li>Removing or modifying the components</li>
<li>Large decision change</li>
<li>means Important component</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250623170603943.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="saliency-map">Saliency Map</h4>
<p><strong>Saliency Map（显著性图）</strong> 是一种<strong>可视化技术</strong>，用于解释<strong>神经网络模型在预测某个样本时，输入中哪些部分最<hanla></hanla>“重要”<hanla></hanla>或<hanla></hanla>“敏感”</strong>。</p>
<blockquote>
<p>它告诉你模型<hanla></hanla>“在看哪里”。在图像分类任务中，Saliency Map 就是一个<strong>热点图</strong>，显示图像中哪些像素对模型输出影响最大。</p>
</blockquote>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250623170810050.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="limitations">Limitations</h4>
<h5 id="noisy-gradient">Noisy Gradient</h5>
<p>在最基本的 Saliency Map 方法中（即 <strong>Vanilla Gradient</strong>），通过求某类别预测结果<hanla></hanla><span class="math inline"><em>y</em></span><hanla></hanla>对输入图像<hanla></hanla><span class="math inline"><em>I</em></span><hanla></hanla>的梯度，来评估每个像素对模型预测的<hanla></hanla>“影响力”。但实际计算中会出现的问题是,<hanla></hanla>生成的梯度图（显著性图）非常<hanla></hanla>“杂乱”<hanla></hanla>或<hanla></hanla>“有噪声”:</p>
<ul>
<li><p>热区分布不连续；</p></li>
<li><p>没有结构性；</p></li>
<li><p>很难解释模型到底关注了哪里；</p></li>
<li><p>图像呈现出<hanla></hanla>“雪花状”<hanla></hanla>或<hanla></hanla>“闪电纹”，视觉上很模糊。</p></li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250623171120600.png" srcset="/img/loading.gif" lazyload></p>
<p><strong>Solution: SmoothGrad</strong></p>
<ul>
<li>Randomly add noises to the input image, get saliency maps of the noisy images and average them.</li>
<li><span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.817ex;" xmlns="http://www.w3.org/2000/svg" width="36.137ex" height="3.186ex" role="img" focusable="false" viewBox="0 -1047.1 15972.6 1408.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></g><g data-mml-node="mi" transform="translate(645,0)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(1523,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(2008,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(2493,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(2854,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mi" transform="translate(3430,0)"><path data-c="1D43A" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q492 659 471 656T418 643T357 615T294 567T236 496T189 394T158 260Q156 242 156 221Q156 173 170 136T206 79T256 45T308 28T353 24Q407 24 452 47T514 106Q517 114 529 161T541 214Q541 222 528 224T468 227H431Q425 233 425 235T427 254Q431 267 437 273H454Q494 271 594 271Q634 271 659 271T695 272T707 272Q721 272 721 263Q721 261 719 249Q714 230 709 228Q706 227 694 227Q674 227 653 224Q646 221 643 215T629 164Q620 131 614 108Q589 6 586 3Q584 1 581 1Q571 1 553 21T530 52Q530 53 528 52T522 47Q448 -22 322 -22Q201 -22 126 55T50 252Z"></path></g><g data-mml-node="mi" transform="translate(4216,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(4667,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(5196,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mo" transform="translate(5716,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(6105,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(6677,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(7343.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mfrac" transform="translate(8399.6,0)"><g data-mml-node="mn" transform="translate(255.4,394) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mi" transform="translate(220,-345) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><rect width="624.3" height="60" x="120" y="220"></rect></g><g data-mml-node="munderover" transform="translate(9430.5,0)"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path></g><g data-mml-node="TeXAtom" transform="translate(1089,477.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(1089,-285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1123,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="mo" transform="translate(11883.8,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mfrac" transform="translate(12161.8,0)"><g data-mml-node="mrow" transform="translate(220,516.8) scale(0.707)"><g data-mml-node="mi"><path data-c="1D715" d="M202 508Q179 508 169 520T158 547Q158 557 164 577T185 624T230 675T301 710L333 715H345Q378 715 384 714Q447 703 489 661T549 568T566 457Q566 362 519 240T402 53Q321 -22 223 -22Q123 -22 73 56Q42 102 42 148V159Q42 276 129 370T322 465Q383 465 414 434T455 367L458 378Q478 461 478 515Q478 603 437 639T344 676Q266 676 223 612Q264 606 264 572Q264 547 246 528T202 508ZM430 306Q430 372 401 400T333 428Q270 428 222 382Q197 354 183 323T150 221Q132 149 132 116Q132 21 232 21Q244 21 250 22Q327 35 374 112Q389 137 409 196T430 306Z"></path></g><g data-mml-node="mi" transform="translate(566,0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mo" transform="translate(1116,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1505,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(2077,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="msub" transform="translate(2855,0)"><g data-mml-node="mi"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g><g data-mml-node="mi" transform="translate(836,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(3985,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><g data-mml-node="mrow" transform="translate(1364.1,-345.6) scale(0.707)"><g data-mml-node="mi"><path data-c="1D715" d="M202 508Q179 508 169 520T158 547Q158 557 164 577T185 624T230 675T301 710L333 715H345Q378 715 384 714Q447 703 489 661T549 568T566 457Q566 362 519 240T402 53Q321 -22 223 -22Q123 -22 73 56Q42 102 42 148V159Q42 276 129 370T322 465Q383 465 414 434T455 367L458 378Q478 461 478 515Q478 603 437 639T344 676Q266 676 223 612Q264 606 264 572Q264 547 246 528T202 508ZM430 306Q430 372 401 400T333 428Q270 428 222 382Q197 354 183 323T150 221Q132 149 132 116Q132 21 232 21Q244 21 250 22Q327 35 374 112Q389 137 409 196T430 306Z"></path></g><g data-mml-node="mi" transform="translate(566,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g></g><rect width="3292.9" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(15694.6,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g></g></g></svg></mjx-container></span></li>
</ul>
<h5 id="gradient-saturation">Gradient Saturation</h5>
<p><strong>Gradient Saturation（梯度饱和）</strong> 是指在神经网络中，由于某些激活函数在特定输入区间会输出<strong>近似恒定的值</strong>，导致<strong>反向传播时梯度接近零</strong>，从而<strong>阻碍学习或解释过程</strong>。</p>
<p>在 <strong>Saliency Map（显著性图）</strong> 这类基于梯度的方法中：</p>
<ul>
<li>梯度饱和会导致输入像素的梯度接近 0；</li>
<li>即使某个区域对模型预测结果有重要影响，但其梯度很小；</li>
<li><strong>显著性图中该区域就不会被高亮，造成误导</strong>。</li>
</ul>
<blockquote>
<p>“重要的区域被忽略了”，因为梯度被<hanla></hanla>“淹没”<hanla></hanla>在网络中。</p>
</blockquote>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250623171649111.png" srcset="/img/loading.gif" lazyload></p>
<p><strong>Solution: Alternative: Integrated gradient(IG)</strong></p>
<ul>
<li>通过计算输入特征从<hanla></hanla>“参考点”<hanla></hanla>变化到实际输入过程中产生的<strong>累积梯度</strong>，来衡量每个输入特征对预测结果的贡献。</li>
<li>从参考点<hanla></hanla><span class="math inline"><em>x</em><sup>′</sup></span>线性地<hanla></hanla>“移动”<hanla></hanla>到输入点<hanla></hanla><span class="math inline"><em>x</em></span>，并沿这条路径计算梯度的积分。
<ul>
<li><span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.034ex;" xmlns="http://www.w3.org/2000/svg" width="41.065ex" height="3.436ex" role="img" focusable="false" viewBox="0 -1061.8 18150.6 1518.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43C" d="M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z"></path></g><g data-mml-node="msub" transform="translate(504,0)"><g data-mml-node="mi"><path data-c="1D43A" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q492 659 471 656T418 643T357 615T294 567T236 496T189 394T158 260Q156 242 156 221Q156 173 170 136T206 79T256 45T308 28T353 24Q407 24 452 47T514 106Q517 114 529 161T541 214Q541 222 528 224T468 227H431Q425 233 425 235T427 254Q431 267 437 273H454Q494 271 594 271Q634 271 659 271T695 272T707 272Q721 272 721 263Q721 261 719 249Q714 230 709 228Q706 227 694 227Q674 227 653 224Q646 221 643 215T629 164Q620 131 614 108Q589 6 586 3Q584 1 581 1Q571 1 553 21T530 52Q530 53 528 52T522 47Q448 -22 322 -22Q201 -22 126 55T50 252Z"></path></g><g data-mml-node="mi" transform="translate(819,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(1617,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2006,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(2578,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(3244.7,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mo" transform="translate(4300.5,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(4689.5,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(605,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(5810.7,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="msubsup" transform="translate(6810.9,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(605,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path></g></g><g data-mml-node="mi" transform="translate(605,-254) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(7709.9,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(8321.1,0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path></g><g data-mml-node="msubsup" transform="translate(8821.3,0)"><g data-mml-node="mo" transform="translate(0 0.5)"><path data-c="222B" d="M113 -244Q113 -246 119 -251T139 -263T167 -269Q186 -269 199 -260Q220 -247 232 -218T251 -133T262 -15T276 155T297 367Q300 390 305 438T314 512T325 580T340 647T361 703T390 751T428 784T479 804Q481 804 488 804T501 805Q552 802 581 769T610 695Q610 669 594 657T561 645Q542 645 527 658T512 694Q512 705 516 714T526 729T538 737T548 742L552 743Q552 745 545 751T525 762T498 768Q475 768 460 756T434 716T418 652T407 559T398 444T387 300T369 133Q349 -38 337 -102T303 -207Q256 -306 169 -306Q119 -306 87 -272T55 -196Q55 -170 71 -158T104 -146Q123 -146 138 -159T153 -195Q153 -206 149 -215T139 -230T127 -238T117 -242L113 -244Z"></path></g><g data-mml-node="TeXAtom" transform="translate(699.9,532.6) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(505,-340.9) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D6FC" d="M34 156Q34 270 120 356T309 442Q379 442 421 402T478 304Q484 275 485 237V208Q534 282 560 374Q564 388 566 390T582 393Q603 393 603 385Q603 376 594 346T558 261T497 161L486 147L487 123Q489 67 495 47T514 26Q528 28 540 37T557 60Q559 67 562 68T577 70Q597 70 597 62Q597 56 591 43Q579 19 556 5T512 -10H505Q438 -10 414 62L411 69L400 61Q390 53 370 41T325 18T267 -2T203 -11Q124 -11 79 39T34 156ZM208 26Q257 26 306 47T379 90L403 112Q401 255 396 290Q382 405 304 405Q235 405 183 332Q156 292 139 224T121 120Q121 71 146 49T208 26Z"></path></g><g data-mml-node="mo" transform="translate(640,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1418,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g></g><g data-mml-node="mfrac" transform="translate(10899.2,0)"><g data-mml-node="mrow" transform="translate(220,525.1) scale(0.707)"><g data-mml-node="mi"><path data-c="1D715" d="M202 508Q179 508 169 520T158 547Q158 557 164 577T185 624T230 675T301 710L333 715H345Q378 715 384 714Q447 703 489 661T549 568T566 457Q566 362 519 240T402 53Q321 -22 223 -22Q123 -22 73 56Q42 102 42 148V159Q42 276 129 370T322 465Q383 465 414 434T455 367L458 378Q478 461 478 515Q478 603 437 639T344 676Q266 676 223 612Q264 606 264 572Q264 547 246 528T202 508ZM430 306Q430 372 401 400T333 428Q270 428 222 382Q197 354 183 323T150 221Q132 149 132 116Q132 21 232 21Q244 21 250 22Q327 35 374 112Q389 137 409 196T430 306Z"></path></g><g data-mml-node="mi" transform="translate(566,0)"><path data-c="1D439" d="M48 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H742Q749 676 749 669Q749 664 736 557T722 447Q720 440 702 440H690Q683 445 683 453Q683 454 686 477T689 530Q689 560 682 579T663 610T626 626T575 633T503 634H480Q398 633 393 631Q388 629 386 623Q385 622 352 492L320 363H375Q378 363 398 363T426 364T448 367T472 374T489 386Q502 398 511 419T524 457T529 475Q532 480 548 480H560Q567 475 567 470Q567 467 536 339T502 207Q500 200 482 200H470Q463 206 463 212Q463 215 468 234T473 274Q473 303 453 310T364 317H309L277 190Q245 66 245 60Q245 46 334 46H359Q365 40 365 39T363 19Q359 6 353 0H336Q295 2 185 2Q120 2 86 2T48 1Z"></path></g><g data-mml-node="mo" transform="translate(1315,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msup" transform="translate(1704,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(605,363) scale(0.707)"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path></g></g><g data-mml-node="mo" transform="translate(2553.5,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(3331.5,0)"><path data-c="1D6FC" d="M34 156Q34 270 120 356T309 442Q379 442 421 402T478 304Q484 275 485 237V208Q534 282 560 374Q564 388 566 390T582 393Q603 393 603 385Q603 376 594 346T558 261T497 161L486 147L487 123Q489 67 495 47T514 26Q528 28 540 37T557 60Q559 67 562 68T577 70Q597 70 597 62Q597 56 591 43Q579 19 556 5T512 -10H505Q438 -10 414 62L411 69L400 61Q390 53 370 41T325 18T267 -2T203 -11Q124 -11 79 39T34 156ZM208 26Q257 26 306 47T379 90L403 112Q401 255 396 290Q382 405 304 405Q235 405 183 332Q156 292 139 224T121 120Q121 71 146 49T208 26Z"></path></g><g data-mml-node="mo" transform="translate(3971.5,0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path></g><g data-mml-node="mo" transform="translate(4249.5,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(4638.5,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(605,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(5537.4,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="msubsup" transform="translate(6315.4,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(605,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path></g></g><g data-mml-node="mi" transform="translate(605,-254) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(7214.4,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(7603.4,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><g data-mml-node="mrow" transform="translate(2527.8,-345.6) scale(0.707)"><g data-mml-node="mi"><path data-c="1D715" d="M202 508Q179 508 169 520T158 547Q158 557 164 577T185 624T230 675T301 710L333 715H345Q378 715 384 714Q447 703 489 661T549 568T566 457Q566 362 519 240T402 53Q321 -22 223 -22Q123 -22 73 56Q42 102 42 148V159Q42 276 129 370T322 465Q383 465 414 434T455 367L458 378Q478 461 478 515Q478 603 437 639T344 676Q266 676 223 612Q264 606 264 572Q264 547 246 528T202 508ZM430 306Q430 372 401 400T333 428Q270 428 222 382Q197 354 183 323T150 221Q132 149 132 116Q132 21 232 21Q244 21 250 22Q327 35 374 112Q389 137 409 196T430 306Z"></path></g><g data-mml-node="msub" transform="translate(566,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(605,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g><rect width="5851.5" height="60" x="120" y="220"></rect></g><g data-mml-node="mi" transform="translate(16990.6,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(17510.6,0)"><path data-c="1D6FC" d="M34 156Q34 270 120 356T309 442Q379 442 421 402T478 304Q484 275 485 237V208Q534 282 560 374Q564 388 566 390T582 393Q603 393 603 385Q603 376 594 346T558 261T497 161L486 147L487 123Q489 67 495 47T514 26Q528 28 540 37T557 60Q559 67 562 68T577 70Q597 70 597 62Q597 56 591 43Q579 19 556 5T512 -10H505Q438 -10 414 62L411 69L400 61Q390 53 370 41T325 18T267 -2T203 -11Q124 -11 79 39T34 156ZM208 26Q257 26 306 47T379 90L403 112Q401 255 396 290Q382 405 304 405Q235 405 183 332Q156 292 139 224T121 120Q121 71 146 49T208 26Z"></path></g></g></g></svg></mjx-container></span></li>
</ul></li>
</ul>
<h3 id="global-explanation-explain-the-whole-model">Global Explanation: Explain the Whole Model</h3>
<p><strong>Global Explanation（全局解释）</strong> 是指：</p>
<blockquote>
<p>解释整个机器学习模型的总体行为，而不仅仅是对某一个输入样本的预测结果进行解释（Local Explanation）。</p>
</blockquote>
<p>它关注的是：</p>
<ul>
<li>模型总体是如何<hanla></hanla>“思考”<hanla></hanla>的；</li>
<li>模型整体对哪些特征最敏感；</li>
<li>预测规则是否稳定、合理、公平；</li>
<li>特征之间的交互关系如何影响输出。</li>
</ul>
<h4 id="what-does-a-filter-detect">What does a filter detect?</h4>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250623181311356.png" srcset="/img/loading.gif" lazyload></p>
<p><span class="math inline"><em>X</em><sup>⋆</sup> = <em>a</em><em>r</em><em>g</em>max<sub><em>x</em></sub>∑<sub><em>i</em></sub>∑<sub><em>j</em></sub><em>a</em><sub><em>i</em><em>j</em></sub></span> (gradient ascent)</p>
<p>E.g., Digit classifier</p>
<p><span class="math inline"><em>x</em><sup>⋆</sup></span> for each filter</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250623181455092.png" srcset="/img/loading.gif" lazyload style="zoom:200%;"></p>
<h4 id="what-does-a-digit-look-like-for-cnn">What does a digit look like for CNN?</h4>
<p>Find the image that maximizes class probability : <span class="math inline"><em>X</em><sup>⋆</sup> = <em>a</em><em>r</em><em>g</em>&nbsp;<em>m</em><em>a</em><em>x</em><sub><em>X</em></sub><em>y</em><sub><em>i</em></sub></span></p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250623181814478.png" srcset="/img/loading.gif" lazyload style="zoom:200%;"></p>
<p>Obviously we can’t see digits——Consider adversarial attack.</p>
<p><strong>Solution: Add Constraint</strong></p>
<ul>
<li>The image should looks like a digit.
<ul>
<li><span class="math inline"><em>X</em><sup>⋆</sup> = <em>a</em><em>r</em><em>g</em>max<sub><em>X</em></sub><em>y</em><sub><em>i</em></sub> + <em>R</em>(<em>X</em>)</span></li>
<li><span class="math inline"><em>R</em>(<em>X</em>) =  − ∑<sub><em>i</em>, <em>j</em></sub>|<em>X</em><sub><em>i</em><em>j</em></sub>|</span>, represents How likely <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="1.928ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 852 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g></g></g></svg></mjx-container> is a digit.</li>
<li><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250623182242551.png" srcset="/img/loading.gif" lazyload style="zoom:200%;"></li>
</ul></li>
<li>Constraint from Generator
<ul>
<li><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250623182341673.png" srcset="/img/loading.gif" lazyload></li>
<li>从生成器的输入低维向量中寻找使其生成图片的<hanla></hanla><span class="math inline"><em>y</em><sub><em>i</em></sub></span><hanla></hanla>最大的向量。</li>
</ul></li>
</ul>
<h2 id="homework-9-explainable-ai">Homework 9: Explainable AI</h2>
<blockquote>
<p>这个作业是在<hanla></hanla>gradescope<hanla></hanla>上布置的，非台大的学生无法完成，因此我只是搜集了所提到的技术的相关信息。</p>
</blockquote>
<h3 id="topic-i-cnn-explanation">Topic I: CNN explanation</h3>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250717175702153.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="lime">Lime</h4>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250717181524098.png" srcset="/img/loading.gif" lazyload></p>
<p><strong>LIME（Local Interpretable Model-agnostic Explanations）</strong> 是一种用于<strong>模型可解释性</strong>的方法。它的核心思想是：通过对输入数据进行轻微扰动并观察模型输出的变化，从而学习一个<strong>局部的、易解释的线性模型</strong>，来近似原始复杂模型的行为，以解释模型对某个<strong>单个输入样本</strong>的预测原因。</p>
<p>LIME 在 CNN 图像中的应用流程：</p>
<ol type="1">
<li><strong>图像分割</strong>：用算法将图像划分为多个 Superpixels。</li>
<li><strong>随机遮挡</strong>：创建多个扰动版本，部分 superpixels 被灰色遮挡。</li>
<li><strong>CNN 预测</strong>：用 CNN 对每个扰动版本预测，记录输出概率。</li>
<li><strong>加权线性回归</strong>：用扰动图像和对应预测结果训练一个线性模型。</li>
<li><strong>可视化解释</strong>：线性模型的权重可视化后，形成热力图，突出最重要区域。</li>
</ol>
<blockquote>
<p>假设 CNN 模型对一张猫的图片预测为<hanla></hanla>“猫”，我们用 LIME 得到如下解释：</p>
<ul>
<li>红色区域表示对<hanla></hanla>“猫”<hanla></hanla>这个预测最有贡献的 superpixels（如耳朵、眼睛、胡须区域）。</li>
<li>蓝色区域表示负面贡献区域（如背景或其他不相关内容）。</li>
<li>用户看到这个解释，就能知道模型是否真的在关注<hanla></hanla>“猫”<hanla></hanla>的关键特征。</li>
</ul>
</blockquote>
<p>针对图像数据的分类解释，<strong>LIME</strong> 的核心库是 <a target="_blank" rel="noopener" href="https://github.com/marcotcr/lime"><code>lime</code></a>，其中处理图像分类解释的模块是：</p>
<blockquote>
<figure class="highlight stylus"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus">lime<span class="hljs-selector-class">.lime_image</span>.LimeImageExplainer<br></code></pre></td></tr></tbody></table></figure>
</blockquote>
<p>主要函数：</p>
<table>
<thead>
<tr class="header">
<th>函数</th>
<th>功能总结</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>LimeImageExplainer()</code></td>
<td>创建图像解释器</td>
</tr>
<tr class="even">
<td><code>explain_instance()</code></td>
<td>执行图像扰动、预测、拟合，生成解释对象</td>
</tr>
<tr class="odd">
<td><code>classifier_fn</code></td>
<td>用户自定义模型的预测接口（返回类别概率）</td>
</tr>
<tr class="even">
<td><code>segmentation_fn</code></td>
<td>图像切分为 superpixels（通常用 SLIC）</td>
</tr>
<tr class="odd">
<td><code>get_image_and_mask()</code></td>
<td>提取显著区域并可视化</td>
</tr>
</tbody>
</table>
<h4 id="saliency-map-1">Saliency Map</h4>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250717190902104.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Saliency Map 函数</span><br><span class="hljs-comment"># 将 tensor 图像归一化到 [0, 1] 区间，便于显示<hanla></hanla></span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">normalize</span>(<span class="hljs-params">image</span>):<br>  <span class="hljs-keyword">return</span> (image - image.<span class="hljs-built_in">min</span>()) / (image.<span class="hljs-built_in">max</span>() - image.<span class="hljs-built_in">min</span>())<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_saliency_maps</span>(<span class="hljs-params">x, y, model</span>):<br>  model.<span class="hljs-built_in">eval</span>()<br>  x = x.cuda()<br><br>  x.requires_grad_()<br>  <br>  y_pred = model(x)<br>  loss_func = torch.nn.CrossEntropyLoss()<br>  loss = loss_func(y_pred, y.cuda())<br>  loss.backward()<br><br>  saliencies, _ = torch.<span class="hljs-built_in">max</span>(x.grad.data.<span class="hljs-built_in">abs</span>().detach().cpu(),dim=<span class="hljs-number">1</span>)  <span class="hljs-comment">#  RGB 三个通道上取最大值 → 得到 shape: (batch_size, H, W)，生成单通道显著图（热图）</span><br><br>  saliencies = torch.stack([normalize(item) <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> saliencies]) <span class="hljs-comment"># 对每张 saliency map 归一化，使其像素值落在 [0, 1]，更适合显示和对比</span><br>  <span class="hljs-keyword">return</span> saliencies<br><span class="hljs-comment"># images, labels = train_set.getbatch(img_indices)</span><br>saliencies = compute_saliency_maps(images, labels, model)<br><br><span class="hljs-comment"># visualize</span><br>fig, axs = plt.subplots(<span class="hljs-number">2</span>, <span class="hljs-built_in">len</span>(img_indices), figsize=(<span class="hljs-number">15</span>, <span class="hljs-number">8</span>))<br><span class="hljs-keyword">for</span> row, target <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>([images, saliencies]):<br>  <span class="hljs-keyword">for</span> column, img <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(target):<br>    <span class="hljs-keyword">if</span> row==<span class="hljs-number">0</span>:<br>      axs[row][column].imshow(img.permute(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>).numpy())<br>    <span class="hljs-keyword">else</span>:<br>      axs[row][column].imshow(img.numpy(), cmap=plt.cm.hot)<br>    <br>plt.show()<br>plt.close()<br></code></pre></td></tr></tbody></table></figure>
<h4 id="smooth-grad">Smooth Grad</h4>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250717192224512.png" srcset="/img/loading.gif" lazyload></p>
<p>Smooth Grad<hanla></hanla>是画<hanla></hanla>Saliency Map<hanla></hanla>的一种技术，有的图像在可视化的时候会有很多额外的噪声。可以认为<hanla></hanla>Smooth Grad<hanla></hanla>技术就是为了减少神经网络所看到的一些噪声。所谓的<hanla></hanla>Smooth Grad<hanla></hanla>就是在图像上随机增加一些噪声，通过引入一定的噪声来达到降噪的效果。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 归一化函数：将图像的像素值映射到 [0,1] 区间，用于增强可视化效果<hanla></hanla></span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">normalize</span>(<span class="hljs-params">image</span>):<br>    <span class="hljs-keyword">return</span> (image - image.<span class="hljs-built_in">min</span>()) / (image.<span class="hljs-built_in">max</span>() - image.<span class="hljs-built_in">min</span>())<br><br><span class="hljs-comment"># SmoothGrad 主函数<hanla></hanla></span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">smooth_grad</span>(<span class="hljs-params">x, y, model, epoch, param_sigma_multiplier</span>):<br>    model.<span class="hljs-built_in">eval</span>()  <span class="hljs-comment"># 设置模型为评估模式（关闭 dropout、BN 等）</span><br><br>    <span class="hljs-comment"># 设置高斯噪声的均值为<hanla></hanla>0</span><br>    mean = <span class="hljs-number">0</span><br>    <span class="hljs-comment"># 根据输入张量范围设置噪声标准差（越大越模糊）</span><br>    sigma = param_sigma_multiplier / (torch.<span class="hljs-built_in">max</span>(x) - torch.<span class="hljs-built_in">min</span>(x)).item()<br><br>    <span class="hljs-comment"># 创建一个用于累加多个扰动梯度的数组（shape 和输入相同）</span><br>    smooth = np.zeros(x.cuda().unsqueeze(<span class="hljs-number">0</span>).size())<br><br>    <span class="hljs-comment"># 重复 epoch 次，生成多个扰动输入</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epoch):<br>        <span class="hljs-comment"># 创建与输入 x 同形状的高斯噪声张量</span><br>        noise = Variable(x.data.new(x.size()).normal_(mean, sigma**<span class="hljs-number">2</span>))  <span class="hljs-comment"># 均值为<hanla></hanla>0，方差为<hanla></hanla>sigma^2</span><br><br>        <span class="hljs-comment"># 将噪声加入原始输入图像中</span><br>        x_mod = (x + noise).unsqueeze(<span class="hljs-number">0</span>).cuda()  <span class="hljs-comment"># 增加 batch 维度并移动到 GPU</span><br>        x_mod.requires_grad_()  <span class="hljs-comment"># 启用梯度计算</span><br><br>        <span class="hljs-comment"># 前向传播得到模型预测</span><br>        y_pred = model(x_mod)<br>        loss_func = torch.nn.CrossEntropyLoss()<br>        <span class="hljs-comment"># 计算预测结果与标签的交叉熵损失</span><br>        loss = loss_func(y_pred, y.cuda().unsqueeze(<span class="hljs-number">0</span>))<br><br>        <span class="hljs-comment"># 反向传播以获得输入的梯度</span><br>        loss.backward()<br><br>        <span class="hljs-comment"># 累加每次的梯度绝对值，detach 去除图计算图，cpu 将张量移回主机，转换为 numpy 格式</span><br>        smooth += x_mod.grad.<span class="hljs-built_in">abs</span>().detach().cpu().data.numpy()<br><br>    <span class="hljs-comment"># 对累加的梯度图进行平均并归一化，得到最终的 SmoothGrad 热力图</span><br>    smooth = normalize(smooth / epoch)<br><br>    <span class="hljs-comment"># 返回形状为 (1, C, H, W) 的 numpy 数组</span><br>    <span class="hljs-keyword">return</span> smooth<br><br><span class="hljs-comment"># 示例使用代码：对多张图像进行可视化<hanla></hanla></span><br>smooth = []<br><span class="hljs-keyword">for</span> i, l <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(images, labels):<br>    <span class="hljs-comment"># 对每张图像调用 smooth_grad 函数，得到其平滑的显著性图</span><br>    smooth.append(smooth_grad(i, l, model, <span class="hljs-number">500</span>, <span class="hljs-number">0.4</span>))<br><br><span class="hljs-comment"># 将多个显著性图拼接成一个数组，shape: (N, 1, C, H, W)</span><br>smooth = np.stack(smooth)<br><br><span class="hljs-comment"># 可视化结果，显示原图和对应的显著性图<hanla></hanla></span><br>fig, axs = plt.subplots(<span class="hljs-number">2</span>, <span class="hljs-built_in">len</span>(img_indices), figsize=(<span class="hljs-number">15</span>, <span class="hljs-number">8</span>))<br><br><span class="hljs-comment"># 遍历原图和显著性图<hanla></hanla></span><br><span class="hljs-keyword">for</span> row, target <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>([images, smooth]):<br>    <span class="hljs-keyword">for</span> column, img <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(target):<br>        <span class="hljs-comment"># 对原图进行通道维度转换：(C, H, W) -&gt; (H, W, C)，方便 plt 显示</span><br>        <span class="hljs-keyword">if</span> row == <span class="hljs-number">0</span>:<br>            axs[row][column].imshow(np.transpose(img.numpy(), (<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>)))<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># 对 smooth grad 显示热力图（假设 shape 为 (1, C, H, W)，取出并转置）</span><br>            axs[row][column].imshow(np.transpose(img.reshape(<span class="hljs-number">3</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>), (<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>)))<br><br>plt.show()<br><br></code></pre></td></tr></tbody></table></figure>
<h4 id="filter-visualization">Filter Visualization</h4>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250717192915979.png" srcset="/img/loading.gif" lazyload></p>
<p>这个方法是想要获得在<hanla></hanla>CNN<hanla></hanla>网络中间某一层所观察到的特征。在训练模型的时候，大多数情况下会一<hanla></hanla>train<hanla></hanla>到底，所以我们想要获取模型中间某层所观察到的内容比较困难，但是<hanla></hanla>PyTorch<hanla></hanla>提供了<hanla></hanla>hook<hanla></hanla>方法，能够轻松的获取模型中间的层数，并且观察到<hanla></hanla>CNN<hanla></hanla>网络中间所观察到的输出内容。hook<hanla></hanla>函数会自动保存模型中的某一层，使用完再将其移出即可。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 图像归一化函数：将像素值线性映射到 [0,1] 区间，增强显示对比度<hanla></hanla></span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">normalize</span>(<span class="hljs-params">image</span>):<br>    <span class="hljs-keyword">return</span> (image - image.<span class="hljs-built_in">min</span>()) / (image.<span class="hljs-built_in">max</span>() - image.<span class="hljs-built_in">min</span>())<br><br><span class="hljs-comment"># 定义全局变量用于保存中间层激活结果<hanla></hanla></span><br>layer_activations = <span class="hljs-literal">None</span><br><br><span class="hljs-comment"># 滤波器可视化解释函数</span><br><span class="hljs-comment"># x：输入图像（Tensor）</span><br><span class="hljs-comment"># model：CNN 模型</span><br><span class="hljs-comment"># cnnid：卷积层在 model.cnn 中的索引（即第几层卷积层）</span><br><span class="hljs-comment"># filterid：要解释的滤波器在该层中的索引（即第几个通道）</span><br><span class="hljs-comment"># iteration：优化迭代次数</span><br><span class="hljs-comment"># lr：优化器学习率<hanla></hanla></span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">filter_explanation</span>(<span class="hljs-params">x, model, cnnid, filterid, iteration=<span class="hljs-number">100</span>, lr=<span class="hljs-number">1</span></span>):<br>    model.<span class="hljs-built_in">eval</span>()  <span class="hljs-comment"># 设置模型为评估模式，防止 dropout 和 batchnorm 干扰</span><br><br>    <span class="hljs-comment"># 定义钩子函数：当 model 在 cnn[cnnid] 层前向传播时，会执行该函数</span><br>    <span class="hljs-comment"># 此函数将该层的输出（激活值）保存到全局变量 layer_activations 中</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">hook</span>(<span class="hljs-params">model, <span class="hljs-built_in">input</span>, output</span>):<br>        <span class="hljs-keyword">global</span> layer_activations<br>        layer_activations = output<br><br>    <span class="hljs-comment"># 注册钩子函数到指定卷积层，获取该层的输出</span><br>    hook_handle = model.cnn[cnnid].register_forward_hook(hook)<br><br>    <span class="hljs-comment"># ---------- 第一阶段：获取原始输入图像在该滤波器下的激活图 ----------</span><br><br>    <span class="hljs-comment"># 对输入图像进行前向传播，触发 hook 函数，layer_activations 会被赋值</span><br>    model(x.cuda())<br><br>    <span class="hljs-comment"># 获取第 filterid 个通道的激活图（形状为 H x W），并从计算图中剥离 -&gt; CPU -&gt; NumPy</span><br>    filter_activations = layer_activations[:, filterid, :, :].detach().cpu()<br><br>    <span class="hljs-comment"># ---------- 第二阶段：寻找能最大激活该滤波器的图像 ----------</span><br><br>    x = x.cuda()                <span class="hljs-comment"># 将输入图像转移到 GPU</span><br>    x.requires_grad_()          <span class="hljs-comment"># 允许对输入图像进行梯度更新（以便优化）</span><br><br>    optimizer = Adam([x], lr=lr)  <span class="hljs-comment"># 使用 Adam 优化器直接优化输入图像</span><br><br>    <span class="hljs-comment"># 进行 iteration 次迭代优化</span><br>    <span class="hljs-keyword">for</span> <span class="hljs-built_in">iter</span> <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(iteration):<br>        optimizer.zero_grad()   <span class="hljs-comment"># 梯度清零</span><br>        model(x)                <span class="hljs-comment"># 前向传播，重新获得当前图像的激活结果</span><br><br>        <span class="hljs-comment"># 定义目标函数：最大化第 filterid 个通道的所有激活值的总和</span><br>        <span class="hljs-comment"># 加负号是因为 optimizer 默认是最小化目标函数</span><br>        objective = -layer_activations[:, filterid, :, :].<span class="hljs-built_in">sum</span>()<br><br>        <span class="hljs-comment"># 反向传播计算梯度</span><br>        objective.backward()<br><br>        <span class="hljs-comment"># 执行一次优化步骤，更新输入图像以更强地激活目标滤波器</span><br>        optimizer.step()<br><br>    <span class="hljs-comment"># 最终优化后的图像即为<hanla></hanla>“最能激活该滤波器的图像”</span><br>    <span class="hljs-comment"># 去除 batch 维度，移回 CPU</span><br>    filter_visualizations = x.detach().cpu().squeeze()<br><br>    <span class="hljs-comment"># 移除 hook，避免影响后续模型使用</span><br>    hook_handle.remove()<br><br>    <span class="hljs-comment"># 返回结果：</span><br>    <span class="hljs-comment"># - filter_activations：原图在该滤波器下的响应图</span><br>    <span class="hljs-comment"># - filter_visualizations：经过优化后能最大激活该滤波器的图像</span><br>    <span class="hljs-keyword">return</span> filter_activations, filter_visualizations<br><br></code></pre></td></tr></tbody></table></figure>
<h4 id="integrated-gradients">Integrated Gradients</h4>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250717192942916.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250717192931271.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">IntegratedGradients</span>():<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model</span>):<br>        <span class="hljs-variable language_">self</span>.model = model  <span class="hljs-comment"># 保存传入的模型</span><br>        <span class="hljs-variable language_">self</span>.gradients = <span class="hljs-literal">None</span>  <span class="hljs-comment"># 用于存储计算得到的梯度</span><br>        <span class="hljs-variable language_">self</span>.model.<span class="hljs-built_in">eval</span>()  <span class="hljs-comment"># 将模型设置为评估模式，防止 Dropout、BatchNorm 影响结果</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_images_on_linear_path</span>(<span class="hljs-params">self, input_image, steps</span>):<br>        <span class="hljs-comment"># 生成从全零图像到输入图像之间的线性插值图像（baseline -&gt; input）</span><br>        xbar_list = [input_image * step / steps <span class="hljs-keyword">for</span> step <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(steps)]<br>        <span class="hljs-keyword">return</span> xbar_list<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_gradients</span>(<span class="hljs-params">self, input_image, target_class</span>):<br>        <span class="hljs-comment"># 计算输入图像对目标类别的梯度</span><br>        input_image.requires_grad = <span class="hljs-literal">True</span>  <span class="hljs-comment"># 启用对输入图像的梯度追踪</span><br>        model_output = <span class="hljs-variable language_">self</span>.model(input_image)  <span class="hljs-comment"># 前向传播，得到模型输出</span><br>        <span class="hljs-variable language_">self</span>.model.zero_grad()  <span class="hljs-comment"># 清除模型中已有的梯度（防止累积）</span><br>        <br>        <span class="hljs-comment"># 创建与模型输出同形状的 one-hot 向量，表示我们只关心 target_class 的输出</span><br>        one_hot_output = torch.FloatTensor(<span class="hljs-number">1</span>, model_output.size()[-<span class="hljs-number">1</span>]).zero_().cuda()<br>        one_hot_output[<span class="hljs-number">0</span>][target_class] = <span class="hljs-number">1</span>  <span class="hljs-comment"># 对应目标类别位置置为<hanla></hanla>1</span><br><br>        <span class="hljs-comment"># 反向传播：计算输入图像对目标类别的梯度</span><br>        model_output.backward(gradient=one_hot_output)<br>        <span class="hljs-variable language_">self</span>.gradients = input_image.grad  <span class="hljs-comment"># 保存得到的梯度</span><br><br>        <span class="hljs-comment"># 转换为 numpy 数组，并去掉 batch 维度</span><br>        gradients_as_arr = <span class="hljs-variable language_">self</span>.gradients.data.cpu().numpy()[<span class="hljs-number">0</span>]<br>        <span class="hljs-keyword">return</span> gradients_as_arr<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_integrated_gradients</span>(<span class="hljs-params">self, input_image, target_class, steps</span>):<br>        <span class="hljs-comment"># 计算积分梯度</span><br>        xbar_list = <span class="hljs-variable language_">self</span>.generate_images_on_linear_path(input_image, steps)<br>        <span class="hljs-comment"># 初始化积分梯度数组，形状同 input_image</span><br>        integrated_grads = np.zeros(input_image.size())<br><br>        <span class="hljs-keyword">for</span> xbar_image <span class="hljs-keyword">in</span> xbar_list:<br>            <span class="hljs-comment"># 对路径上每个插值图像计算梯度</span><br>            single_integrated_grad = <span class="hljs-variable language_">self</span>.generate_gradients(xbar_image, target_class)<br>            <span class="hljs-comment"># 将梯度按步数进行平均，加和每一步的贡献</span><br>            integrated_grads = integrated_grads + single_integrated_grad / steps<br><br>        <span class="hljs-comment"># 返回去掉 batch 维度后的结果，只返回第一个通道（如果是批量输入）</span><br>        <span class="hljs-keyword">return</span> integrated_grads[<span class="hljs-number">0</span>]<br></code></pre></td></tr></tbody></table></figure>
<h3 id="topic-ii-bert-explanation">Topic II: BERT explanation</h3>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250717193319389.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="attention-visualization">Attention Visualization</h4>
<p>https://huggingface.co/exbert/</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250717193454181.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="embedding-visualization">Embedding Visualization</h4>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250717193509971.png" srcset="/img/loading.gif" lazyload></p>
<p>加载训练好的模型权重，去迭代这个权重，通过<hanla></hanla>PCA<hanla></hanla>降维，然后可视化每一个隐藏层的状态。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 将问题和上下文编码成模型输入格式（input_ids, attention_mask<hanla></hanla>等）</span><br>inputs = Tokenizer(questions[QUESTION-<span class="hljs-number">1</span>], contexts[QUESTION-<span class="hljs-number">1</span>], return_tensors=<span class="hljs-string">'pt'</span>)<br><br><span class="hljs-comment"># 获取问题和上下文在输入序列中的位置索引（用于可视化时区分不同来源的<hanla></hanla>token）</span><br>question_start, question_end = <span class="hljs-number">1</span>, inputs[<span class="hljs-string">'input_ids'</span>][<span class="hljs-number">0</span>].tolist().index(<span class="hljs-number">102</span>) - <span class="hljs-number">1</span>  <span class="hljs-comment"># [SEP] 的<hanla></hanla>token id<hanla></hanla>是<hanla></hanla>102</span><br>context_start, context_end = question_end + <span class="hljs-number">2</span>, <span class="hljs-built_in">len</span>(inputs[<span class="hljs-string">'input_ids'</span>][<span class="hljs-number">0</span>]) - <span class="hljs-number">2</span>    <span class="hljs-comment"># 跳过<hanla></hanla>[SEP]<hanla></hanla>后紧跟的<hanla></hanla>context<hanla></hanla>开始直到倒数第二个<hanla></hanla>token（前一个<hanla></hanla>[SEP]）</span><br><br><span class="hljs-comment"># 载入模型预测后保存的<hanla></hanla>hidden states（模型第<hanla></hanla>QUESTION<hanla></hanla>题的输出，包含所有层的隐藏状态）</span><br>outputs_hidden_states = torch.load(<span class="hljs-string">f"hw9_bert/output/model_q<span class="hljs-subst">{QUESTION}</span>"</span>)<br><br><span class="hljs-comment">##### 遍历每一层的<hanla></hanla>hidden states<hanla></hanla>进行可视化 #####</span><br><span class="hljs-comment"># outputs_hidden_states<hanla></hanla>是一个包含<hanla></hanla>13<hanla></hanla>个元素的元组：</span><br><span class="hljs-comment"># 第<hanla></hanla>0<hanla></hanla>个是词嵌入（embedding layer 输出），其余<hanla></hanla>12<hanla></hanla>个是<hanla></hanla>transformer<hanla></hanla>各层的输出<hanla></hanla></span><br><span class="hljs-keyword">for</span> layer_index, embeddings <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(outputs_hidden_states[<span class="hljs-number">1</span>:]):  <span class="hljs-comment"># 跳过第一个<hanla></hanla>embedding<hanla></hanla>输出，从第<hanla></hanla>1<hanla></hanla>层到第<hanla></hanla>12<hanla></hanla>层</span><br><br>    <span class="hljs-comment"># 当前层的 embeddings 维度为 [1, seq_len, 768]，我们使用 PCA 将其降到二维</span><br>    reduced_embeddings = PCA(n_components=<span class="hljs-number">2</span>, random_state=<span class="hljs-number">0</span>).fit_transform(embeddings[<span class="hljs-number">0</span>])  <span class="hljs-comment"># embeddings[0]<hanla></hanla>形状为<hanla></hanla>[seq_len, 768]</span><br><br>    <span class="hljs-comment">##### 逐<hanla></hanla>token<hanla></hanla>绘制嵌入向量的二维表示 #####</span><br>    <span class="hljs-keyword">for</span> i, token_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(inputs[<span class="hljs-string">'input_ids'</span>][<span class="hljs-number">0</span>]):<br>        x, y = reduced_embeddings[i]              <span class="hljs-comment"># 获取该<hanla></hanla>token<hanla></hanla>的二维坐标</span><br>        word = Tokenizer.decode(token_id)         <span class="hljs-comment"># 将<hanla></hanla>token id<hanla></hanla>转回单词（不一定是词，有可能是子词）</span><br><br>        <span class="hljs-comment"># 根据该<hanla></hanla>token<hanla></hanla>的类别（答案<hanla></hanla>/<hanla></hanla>问题<hanla></hanla>/<hanla></hanla>上下文）用不同颜色绘制</span><br>        <span class="hljs-keyword">if</span> word <span class="hljs-keyword">in</span> answers[QUESTION-<span class="hljs-number">1</span>].split():   <span class="hljs-comment"># 如果<hanla></hanla>token<hanla></hanla>在答案中，用蓝色标记</span><br>            plt.scatter(x, y, color=<span class="hljs-string">'blue'</span>, marker=<span class="hljs-string">'d'</span>)<br>        <span class="hljs-keyword">elif</span> question_start &lt;= i &lt;= question_end: <span class="hljs-comment"># 如果<hanla></hanla>token<hanla></hanla>属于问题部分，用红色标记</span><br>            plt.scatter(x, y, color=<span class="hljs-string">'red'</span>)<br>        <span class="hljs-keyword">elif</span> context_start &lt;= i &lt;= context_end:   <span class="hljs-comment"># 如果<hanla></hanla>token<hanla></hanla>属于上下文部分，用绿色标记</span><br>            plt.scatter(x, y, color=<span class="hljs-string">'green'</span>)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">continue</span>  <span class="hljs-comment"># 跳过<hanla></hanla>CLS、SEP<hanla></hanla>等特殊<hanla></hanla>token</span><br><br>        <span class="hljs-comment"># 在点旁边标注<hanla></hanla>token<hanla></hanla>对应的文本</span><br>        plt.text(x + <span class="hljs-number">0.1</span>, y + <span class="hljs-number">0.2</span>, word, fontsize=<span class="hljs-number">12</span>)<br><br>    <span class="hljs-comment"># 为图例添加空点（只是为了让图例出现，不实际绘制）</span><br>    plt.plot([], label=<span class="hljs-string">'answer'</span>, color=<span class="hljs-string">'blue'</span>, marker=<span class="hljs-string">'d'</span>)<br>    plt.plot([], label=<span class="hljs-string">'question'</span>, color=<span class="hljs-string">'red'</span>, marker=<span class="hljs-string">'o'</span>)<br>    plt.plot([], label=<span class="hljs-string">'context'</span>, color=<span class="hljs-string">'green'</span>, marker=<span class="hljs-string">'o'</span>)<br><br>    plt.legend(loc=<span class="hljs-string">'best'</span>)                      <span class="hljs-comment"># 显示图例</span><br>    plt.title(<span class="hljs-string">'Layer '</span> + <span class="hljs-built_in">str</span>(layer_index + <span class="hljs-number">1</span>))  <span class="hljs-comment"># 添加标题：当前是第几层的可视化</span><br>    plt.show()                                  <span class="hljs-comment"># 显示图像</span><br><br></code></pre></td></tr></tbody></table></figure>
<h4 id="embedding-analysis">Embedding Analysis</h4>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250717194211814.png" srcset="/img/loading.gif" lazyload></p>
<p><strong>TODO</strong>: Compare output embedding of BERT using:</p>
<ul>
<li>Euclidean distance</li>
<li>Cosine similarity</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 每个句子中选择要比较的词的下标索引（按 word-level 选）</span><br><span class="hljs-comment"># 举例：句子 "蘋果茶真難喝"，若 index = 0，表示选择的是"蘋"</span><br><span class="hljs-comment"># 此处为<hanla></hanla>10<hanla></hanla>个句子中各自要选取比较的词在词级别的索引（如第<hanla></hanla>0<hanla></hanla>个句子的第<hanla></hanla>4<hanla></hanla>个词）</span><br>select_word_index = [<span class="hljs-number">4</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">8</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">4</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]<br><br><span class="hljs-comment"># 计算两个向量之间的欧几里得距离（L2<hanla></hanla>范数）</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">euclidean_distance</span>(<span class="hljs-params">a, b</span>):<br>    <span class="hljs-comment"># np.linalg.norm(a - b) 计算的是 √((a1 - b1)^2 + ... + (an - bn)^2)</span><br>    <span class="hljs-keyword">return</span> np.linalg.norm(a - b)<br><br><span class="hljs-comment"># 计算两个向量之间的余弦相似度： cos(θ) = (a·b) / (||a||*||b||)</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">cosine_similarity</span>(<span class="hljs-params">a, b</span>):<br>    <span class="hljs-keyword">return</span> np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))<br><br><span class="hljs-comment"># 设置用于词向量比较的相似度<hanla></hanla>/<hanla></hanla>距离度量方法</span><br><span class="hljs-comment"># 可选项：euclidean_distance 或 cosine_similarity</span><br>METRIC = euclidean_distance  <span class="hljs-comment"># 如果你想改为余弦相似度，可以改为 cosine_similarity</span><br><br><span class="hljs-comment"># 获取指定词在指定层的 BERT 输出中的嵌入表示<hanla></hanla></span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_select_embedding</span>(<span class="hljs-params">output, tokenized_sentence, select_word_index</span>):<br>    LAYER = <span class="hljs-number">12</span>  <span class="hljs-comment"># 选择第几层的输出向量（0 为 embedding 层，1~12 为 Transformer 层）</span><br><br>    <span class="hljs-comment"># output.hidden_states 是一个包含<hanla></hanla>13<hanla></hanla>层输出的元组 (embedding + 12 layers)</span><br>    <span class="hljs-comment"># [LAYER][0] 取出对应层的 [batch, seq_len, hidden_size]，我们只处理 batch_size = 1 的情况</span><br>    hidden_state = output.hidden_states[LAYER][<span class="hljs-number">0</span>]<br><br>    <span class="hljs-comment"># 将词级别索引映射为 token 级别的起始下标</span><br>    <span class="hljs-comment"># .word_to_tokens(index) 会返回一个 Span，其中包含该词所对应的 token 区间 [start, end)</span><br>    select_token_span = tokenized_sentence.word_to_tokens(select_word_index)<br><br>    <span class="hljs-keyword">if</span> select_token_span <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">f"The word index <span class="hljs-subst">{select_word_index}</span> could not be mapped to tokens."</span>)<br><br>    <span class="hljs-comment"># 取该词对应的第一个 token 的索引来代表整个词（可选策略：也可以取平均）</span><br>    select_token_index = select_token_span.start<br><br>    <span class="hljs-comment"># 返回该 token 的向量，并转为 numpy</span><br>    <span class="hljs-keyword">return</span> hidden_state[select_token_index].detach().cpu().numpy()<br><br></code></pre></td></tr></tbody></table></figure>
<h1 id="attacks-in-nlp">Attacks in NLP</h1>
<h2 id="introduction">Introduction</h2>
<p>In the past, we only focus on attacks in computer vision or audio. The input space for image or audio are vectors in <span class="math inline">ℝ<sup><em>n</em></sup></span>, but the input space in NLP are words/tokens. To feed those tokens into a model, we need to map each token into a continuous vector. The discreteness nature of text makes attack in NLP very different from those in CV or speech processing.</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250625171708840.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="evasion-attacks-and-defenses">Evasion Attacks and Defenses</h2>
<h3 id="introduction-1">Introduction</h3>
<h4 id="evasion-attacks-in-computer-vision">Evasion Attacks in Computer Vision</h4>
<p>Adding imperceptible noise on an image can change the prediction of a model.</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250625171918716.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="evasion-attacks-in-nlp">Evasion Attacks in NLP</h4>
<p>For a task, modify the input such that the model’s prediction corrupts while the modified input and the original input should not change the prediction for human.</p>
<blockquote>
<p>对于一个任务，修改输入使得模型的预测结果被干扰（或破坏），而修改后的输入与原始输入在人类看来不应改变其预测结果。</p>
</blockquote>
<ul>
<li>Sentiment Analysis</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250625172141329.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>Dependency Parsing</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250625172155024.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>Machine Translation</li>
</ul>
<p>Anything that makes the model behave from what we expect can be considered as an adversarial example.</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250625172251955.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="four-ingredients-in-evasion-attacks">Four Ingredients in Evasion Attacks</h3>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250625172401227.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="goal-1">Goal</h4>
<blockquote>
<p>What the attack aims to achieve.</p>
</blockquote>
<ul>
<li><strong>Untargeted classification</strong>: Make the model misclassify the input example.</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250625172527110.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li><strong>Targeted classification</strong>: Make the model to classify samples having ground truth of class 𝐴 into another class 𝐵.</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250625172608947.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li><strong>Universal suffix dropper(通用后缀去除器)</strong>: Make the translated sentence to drop some suffix.</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250625172752316.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>Wrong parse tree in dependency parsing</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250625172821408.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="transformations">Transformations</h4>
<blockquote>
<p>How to construct perturbations(扰动) for possible adversaries.</p>
</blockquote>
<h5 id="word-level">Word Level</h5>
<ul>
<li>Word substitution by WordNet synonyms(通过<hanla></hanla>WordNet<hanla></hanla>同义词进行词语替换)</li>
</ul>
<blockquote>
<p><strong>WordNet</strong> 是一个英语词汇数据库，由普林斯顿大学开发，它将词汇（名词、动词、形容词、副词）组织成同义词集合（synsets），并用语义关系（如同义、反义、上下位）将它们连接起来。</p>
<p>将原始句子中的某个单词，用其在 WordNet 中的同义词进行替换，以保持句子原意的前提下，生成一个<strong>语义等价但表述不同的句子</strong>。</p>
</blockquote>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250625173844578.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>Word substitution by 𝑘NN or <span class="math inline"><em>𝜀</em></span>-ball in counter-fitted GloVe embedding space(利用通过<hanla></hanla>counter-fitting<hanla></hanla>技术增强的 GloVe 向量）进行语义保持的对抗词替换)</li>
</ul>
<blockquote>
<p>GloVe（Global Vectors for Word Representation）是一种无监督学习的词嵌入方法，通过统计词与词之间的共现频率来学习词向量，使得<strong>语义上相近的词具有相近的向量表示</strong>。</p>
<p>Counter-fitted GloVe 是一种经过<strong>后处理（post-processing）</strong>的 GloVe 向量，目的在于：</p>
<ul>
<li><strong>拉近语义相似词（synonyms）的向量距离</strong></li>
<li><strong>推远语义相反词（antonyms）的向量距离</strong></li>
</ul>
</blockquote>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250625174139363.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>Word substitution by BERT masked language modeling (MLM) prediction</li>
</ul>
<blockquote>
<p>利用 BERT 的预测能力进行<strong>上下文敏感的词替换</strong>，主要过程如下：</p>
<ol type="1">
<li><strong>选中目标词</strong>，将其替换为 <code>[MASK]</code>。</li>
<li><strong>使用 BERT 模型</strong>对 <code>[MASK]</code> 位置进行预测，输出 Top-k 可能的词。</li>
<li><strong>从中筛选</strong>语义相近或语法合适的词作为替换词，构造新句子。</li>
</ol>
</blockquote>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250625174330891.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>Word substitution by BERT reconstruction (no masking)</li>
</ul>
<blockquote>
<p>利用 BERT 对句子中每个单词的上下文感知向量表示，判断该词是否与其上下文一致，并选择可能的替代词。</p>
</blockquote>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250625180216964.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>Word substitution by changing the inflectional form of verbs, nouns and adjectives</li>
</ul>
<blockquote>
<p>Inflectional morpheme: an affix that never changes the basic meaning of a word, and are indicative/characteristic of the part of speech (POS)</p>
<p><strong>屈折词缀</strong>：一种<strong>不改变词语基本含义</strong>的附加成分（词缀），它通常<strong>用来表示或标示词性（词类）特征</strong>。</p>
</blockquote>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250625180357804.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>Word substitution by gradient of the word embedding</li>
</ul>
<blockquote>
<p><strong>通过计算模型对输入词嵌入的梯度，找到最<hanla></hanla>“敏感”<hanla></hanla>的词，并替换为能引起模型输出变化的相似词</strong>，从而探测或攻击模型的鲁棒性。</p>
</blockquote>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250625180700355.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>Word insertion based on BERT MLM</li>
</ul>
<blockquote>
<p>利用 BERT 预训练语言模型的上下文预测能力，在文本中插入新词</p>
</blockquote>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250625180839647.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>Word deletion</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250625180924442.png" srcset="/img/loading.gif" lazyload></p>
<h5 id="char-level">Char Level</h5>
<ul>
<li>Swap</li>
<li>Substitution</li>
<li>Deletion</li>
<li>Insertion</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250625181017272.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="constrains">Constrains</h4>
<blockquote>
<p>What a valid adversarial example should satisfy.</p>
</blockquote>
<h5 id="overlap-between-the-transformed-sample-and-the-original-sample">Overlap between the transformed sample and the original sample</h5>
<ul>
<li>Levenshtein edit distance</li>
</ul>
<blockquote>
<p>Levenshtein Edit Distance（莱文斯坦编辑距离）常被用作约束条件（Constraints）之一，以控制对输入文本进行扰动（perturbation）的幅度，使得修改后的文本保持原始语义或形式的可接受性，而又能欺骗目标模型。</p>
</blockquote>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250626214227668.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>Maximum percentage of modified words</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250626214418668.png" srcset="/img/loading.gif" lazyload></p>
<h5 id="grammaticality-of-the-perturbed-sample">Grammaticality of the perturbed sample</h5>
<ul>
<li>Part of speech (POS,<hanla></hanla>词性) consistency</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250626214549362.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>Number of grammatical errors (evaluated by some toolkit)</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250626214616088.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>Fluency scored by the perplexity of a pre-trained language model</li>
</ul>
<blockquote>
<p>通过预训练语言模型的困惑度评估文本流畅性。</p>
</blockquote>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250626214738233.png" srcset="/img/loading.gif" lazyload></p>
<h5 id="semantic-preserving">Semantic preserving</h5>
<blockquote>
<p>变换后的样本与原始样本的语义相似性。</p>
</blockquote>
<ul>
<li>Distance of the swapped word’s embedding and the original word’s embedding</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250626214956056.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>Similarity between the transformed sample’s sentence embedding and the original sample’s sentence embedding</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250626215211019.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="search-method">Search Method</h4>
<blockquote>
<p>How to find an adversarial example from the transformations that satisfies the constrains and meets the goal.</p>
</blockquote>
<h5 id="greedy-search">Greedy Search</h5>
<p>Score the each transformation at each position, and then replace the words in decreasing order of the score until the prediction flips.</p>
<blockquote>
<p>为输入中每个位置的所有变换打分，并按照得分从高到低依次替换词语，直到模型预测发生变化。</p>
</blockquote>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250626215451625.png" srcset="/img/loading.gif" lazyload></p>
<h5 id="greedy-search-with-word-importance-ranking-wir">Greedy search with word importance ranking (WIR)</h5>
<ul>
<li>Step 1: Score each word’s importance</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250626215529982.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>Step 2: Swap the words from the most important to the least important</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250626215659779.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>Word Importance ranking by <strong>leave-one-out (LOO)</strong>: see how the ground truth probability decreases when the word is removed from the input.</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250626215757898.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>Word Importance ranking by the <strong>gradient of the word embedding</strong></li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250626215902106.png" srcset="/img/loading.gif" lazyload></p>
<h5 id="genetic-algorithm">Genetic Algorithm</h5>
<blockquote>
<p>evolution and selection based on fitness</p>
</blockquote>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250626215958442.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250626220009072.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<p>选择，交叉，变异操作</p>
<p>速度太慢，很少使用</p>
</blockquote>
<h3 id="examples-of-evasion-attacks">Examples of Evasion Attacks</h3>
<h4 id="synonym-substitution-attack">Synonym Substitution Attack</h4>
<blockquote>
<p>同义词替换攻击</p>
</blockquote>
<h5 id="textfooler">TextFooler</h5>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250626221813138.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>Algorithm</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250626221903170.png" srcset="/img/loading.gif" lazyload></p>
<ol type="1">
<li>Word Importance Ranking (WIR)：评估输入句子中<strong>每个词对模型预测结果的重要性</strong>。</li>
<li>生成同义词候选集：对于排名靠前的每个重要词，构造一组可选的同义词替换候选。</li>
<li>贪心替换并检测攻击是否成功：遍历词汇替换候选，每次选择当前对模型输出影响最大（最可能使其预测错误）的候选词进行替换，替换顺序按照步骤<hanla></hanla>1<hanla></hanla>的词重要性排名，一旦模型的预测结果改变，攻击即视为成功（early stopping）。</li>
</ol>
<h5 id="pwws">PWWS</h5>
<ul>
<li><strong>Probability weighted word saliency</strong>: consider LOO <span class="math inline"><em>Δ</em><em>p</em><sub><em>p</em><em>o</em><em>s</em><em>i</em><em>t</em><em>i</em><em>v</em><em>e</em></sub></span> and <span class="math inline"><em>Δ</em><em>p</em><sub><em>p</em><em>o</em><em>s</em><em>i</em><em>t</em><em>i</em><em>v</em><em>e</em></sub></span> in word substitution together to obtain the WIR</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250626222353976.png" srcset="/img/loading.gif" lazyload></p>
<h5 id="bert-attack">BERT-Attack</h5>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250626222422819.png" srcset="/img/loading.gif" lazyload></p>
<h5 id="genetic-algorithm-1">Genetic Algorithm</h5>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250626222458915.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<p>Even with those constrains, the adversarial samples may still be human perceptible.</p>
</blockquote>
<h4 id="morpheus">Morpheus</h4>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250626222620225.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<p>Morpheus 的攻击方法是<strong>基于搜索的贪心策略</strong>，与 TextFooler 类似，但加入了对<strong>检测器行为的建模</strong>，形成了一种<strong>双目标攻击（dual-objective attack）</strong>。</p>
</blockquote>
<h4 id="universal-trigger">Universal Trigger</h4>
<ul>
<li>A trigger string that is not related to the task but can perform targeted attack when add to the original string.</li>
</ul>
<blockquote>
<p>一组固定的、可通用的扰动（通常是词或子词），插入到任意输入文本中，即可显著影响模型预测，使其发生错误。</p>
<p>它是 <strong>无输入依赖的（input-agnostic）对抗攻击</strong>，即：</p>
<ul>
<li>不需要针对每个输入样本单独生成对抗扰动</li>
<li>相同的 trigger 可用于攻击任意输入句子</li>
<li>可视为自然语言中的<hanla></hanla>“对抗咒语”（adversarial spell）</li>
</ul>
</blockquote>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250626223134998.png" srcset="/img/loading.gif" lazyload></p>
<p><strong>Steps:</strong></p>
<ol type="1">
<li>Determine how many words the trigger needs and initialize them with some words. (such as <code>[the, the, the]</code>)</li>
<li>Backward and obtain the gradient of each trigger word’s embedding and find the token that minimize the objective function <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.604ex;" xmlns="http://www.w3.org/2000/svg" width="23.001ex" height="2.301ex" role="img" focusable="false" viewBox="0 -750 10166.4 1017.1"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(529,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(980,0)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mi" transform="translate(1457,0)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(2335,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="msub" transform="translate(2680,0)"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="TeXAtom" transform="translate(633,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(345,0)"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g><g data-mml-node="mi" transform="translate(1114,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(1599,0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mi" transform="translate(2032,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(2561,0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g></g></g><g data-mml-node="mo" transform="translate(5477.2,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(5866.2,0)"><g data-mml-node="mi"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(499,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(6881.4,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="msub" transform="translate(7881.6,0)"><g data-mml-node="mi"><path data-c="1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path></g><g data-mml-node="mn" transform="translate(771,-150) scale(0.707)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g><g data-mml-node="msub" transform="translate(9056.2,0)"><g data-mml-node="mo"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="TeXAtom" transform="translate(422,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mn" transform="translate(499,-150) scale(0.707)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g></g></g></g></g></svg></mjx-container>.</li>
<li>Update the trigger with the newly find words.</li>
<li>Continue step 1~3 until convergence</li>
</ol>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250626223602651.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="crafting-adversaries-by-auto-encoder">Crafting Adversaries by Auto-Encoder</h4>
<ul>
<li>Train a generator (auto-encoder) to generate the adversarial samples
<ul>
<li>Goal of generator: make the text classifier predict wrongly</li>
<li>Goal of the classifier: predict correctly</li>
<li>Iterate between attack and defense</li>
</ul></li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250627230016177.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>Attack step</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250627230250550.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>Defense step</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250627230328645.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>Problem during backward: cannot directly backward the argmax in AE</li>
</ul>
<blockquote>
<p>在训练过程中，如何从一个离散分布中采样，并且保持梯度可传播？</p>
</blockquote>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250627230402337.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>A closer look into non-differentiability of the AE output</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250627230500234.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>Gumbel-SoftMax reparameterization trick: using SoftMax with temperature scaling as approximation of argmax</li>
</ul>
<blockquote>
<p>以可导方式模拟 one-hot 向量采样过程</p>
</blockquote>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250627230743450.png" srcset="/img/loading.gif" lazyload></p>
<p>我们将 Gumbel-Max 改成 SoftMax 形式，得到一个可导的<hanla></hanla>“近似<hanla></hanla>one-hot”<hanla></hanla>向量。</p>
<p>温度 <span class="math inline"><em>τ</em></span> 的作用</p>
<ul>
<li>当 <span class="math inline"><em>τ</em> → 0</span>，输出趋近于 one-hot</li>
<li>当 <span class="math inline"><em>τ</em> → ∞</span>，输出趋近于均匀分布</li>
</ul>
<h3 id="defenses-against-evasion-attacks">Defenses against Evasion Attacks</h3>
<h4 id="training-a-more-robust-model">Training a More Robust Model</h4>
<ul>
<li>Adversarial training: <strong>generate the adversarial samples</strong> using the current model every <span class="math inline"><em>N</em></span> epochs</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250627231542540.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>Adversarial training in the <strong>word embedding space</strong> by <span class="math inline"><em>𝜀</em></span>-ball
<ul>
<li>Motivation: A word’s synonym may be within its neighborhood</li>
</ul></li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250627231728846.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li><strong>ASCC-defense</strong> (Adversarial Sparse Convex Combination): Adversarial training in the word embedding space by the <strong>convex hull</strong> form by the synonym set</li>
</ul>
<blockquote>
<p>在词嵌入空间中，利用同义词形成的凸包（convex hull），通过稀疏凸组合构造鲁棒表示，以增强模型对对抗攻击的抵抗力。</p>
</blockquote>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250627231840624.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250627232155711.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li><p><strong>Adversarial data augmentation</strong>: use a trained (unrobust) text classifier to pre-generate the adversarial samples, and then add them to the training dataset to train a new text classifier</p></li>
<li><p><strong>Adversarial and Mixup Data Augmentation</strong></p>
<ul>
<li>Adversarial data augmentation</li>
<li>Mixup the samples in the training set (including benign and adversarial)</li>
</ul></li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250627232440212.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="detecting-adversaries-during-inference">Detecting Adversaries during Inference</h4>
<ul>
<li><strong>Discriminate perturbations (DISP)</strong>: detect adversarial samples and convert them to benign ones</li>
</ul>
<blockquote>
<p>利用扰动本身的结构信息，定位并消除对抗性修改。</p>
<ol type="1">
<li><p>扰动判别器：一个用于判断某个标记是否被扰动的分类器；</p></li>
<li><p>嵌入估计器：通过回归方式估计被扰动标记的嵌入表示；</p></li>
<li><p>标记恢复：利用估计出的嵌入表示，在嵌入词库中查找并恢复被扰动的标记。</p></li>
</ol>
</blockquote>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250627232801690.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li><strong>Frequency-Guided Word Substitutions (FGWS)</strong>: Swap low frequency words with higher frequency counterparts with a three-stepped pipeline.</li>
</ul>
<blockquote>
<ol type="1">
<li><p>找出输入中在训练数据中出现频率低于预设阈值 <span class="math inline"><em>δ</em></span> 的词语；</p></li>
<li><p>将第<hanla></hanla>1<hanla></hanla>步中识别出的所有低频词，替换为它们中出现频率最高的同义词；</p></li>
<li><p>如果替换前后模型对原预测类别的概率差异大于预设阈值 <span class="math inline"><em>γ</em></span>，则将该输入标记为对抗样本。</p></li>
</ol>
</blockquote>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250627233403870.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="imitation-attacks-and-defenses">Imitation Attacks and Defenses</h2>
<h3 id="imitation-attacks">Imitation Attacks</h3>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250628195104936.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>Imitation attack aims to stole a trained model by querying it</li>
<li>Training a model requires significant resources, both time and money</li>
<li>Training data may be proprietary（独有的）</li>
<li>Factors that may affect how well a model can be stolen
<ul>
<li>Architecture mismatch</li>
<li>Data mismatch</li>
</ul></li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250628195247879.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="imitation-attacks-in-machine-translation">Imitation Attacks in Machine Translation</h4>
<blockquote>
<p>通过查询黑盒翻译<hanla></hanla>API（如 Google Translate、DeepL、ChatGPT 等）获取输入-输出对，并据此训练一个行为相似的模型，从而窃取其翻译能力。</p>
</blockquote>
<p><strong>Pipeline:</strong></p>
<ol type="1">
<li><strong>数据构造</strong>：
<ul>
<li>攻击者准备大量源语言句子（例如英语句子），这些可以从公开语料库中收集</li>
</ul></li>
<li><strong>黑盒查询</strong>：
<ul>
<li>将源语言句子发送给目标翻译系统（如 Google Translate）</li>
<li>收集目标模型返回的翻译结果（目标语言句子，如德语）</li>
</ul></li>
<li><strong>训练仿制模型</strong>：
<ul>
<li>使用这些<hanla></hanla>“源句-译文”<hanla></hanla>对训练一个神经机器翻译模型</li>
<li>模仿目标模型的翻译行为和风格</li>
</ul></li>
<li><strong>评估与攻击</strong>：
<ul>
<li>评估仿制模型与目标模型之间的<hanla></hanla>BLEU<hanla></hanla>相似度</li>
<li>或在仿制模型上设计对抗输入，并将其转移到原模型进行攻击</li>
</ul></li>
</ol>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250628195711071.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>Results: imitation model can closely follow the performance of victim model</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250628195747942.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="imitation-attacks-in-text-classification">Imitation Attacks in Text Classification</h4>
<blockquote>
<p>攻击者在不了解模型内部结构的前提下，通过查询文本分类模型（如情感分析器、垃圾邮件检测器、意图识别模型等）并收集其输出结果，构造一个仿制模型以复制原模型的行为。</p>
</blockquote>
<h3 id="adversarial-transferability">Adversarial Transferability</h3>
<blockquote>
<p><strong>对抗样本的可迁移性</strong>: 对于输入样本 <span class="math inline"><em>x</em></span>，攻击者在模型 <span class="math inline"><em>f</em><sub><em>s</em></sub></span>（源模型）上生成的对抗样本 <span class="math inline"><em>x</em><sub>adv</sub></span>，在不访问目标模型 <span class="math inline"><em>f</em><sub><em>t</em></sub></span> 的前提下，也能使 <span class="math inline"><em>f</em><sub><em>t</em></sub>(<em>x</em><sub>adv</sub>) ≠ <em>f</em><sub><em>t</em></sub>(<em>x</em>)</span>。</p>
<p>在一个模型上生成的对抗样本，在未修改的情况下也能欺骗另一个模型。</p>
</blockquote>
<ul>
<li>After we train the imitator model, we can (white-box) attack the imitator model to obtain adversarial samples, and use those samples to attack the victim model</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250628200205510.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="adversarial-transferability-in-machine-translation-mt">Adversarial transferability in machine translation (MT)</h4>
<ul>
<li>Adversarial examples can successfully transfer to production MT system</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250628200254174.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="adversarial-transferability-in-text-classification">Adversarial transferability in text classification</h4>
<blockquote>
<p><strong>从模仿模型发起的对抗攻击，其效果有时甚至比直接攻击目标模型更强。</strong></p>
<p><strong>模仿攻击</strong>：攻击者通过查询 victim 模型 API，收集大量<hanla></hanla>“文本-标签”<hanla></hanla>对，训练一个 imitator 模型 <span class="math inline"><em>M</em>′</span>，模仿 victim 的行为。</p>
<p><strong>白盒攻击</strong>：攻击者在 imitator <span class="math inline"><em>M</em>′</span> 上使用梯度导向攻击方法生成对抗样本 <span class="math inline"><em>x</em><sub>adv</sub></span>。</p>
<p><strong>迁移测试</strong>：将 <span class="math inline"><em>x</em><sub>adv</sub></span> 输入到目标 victim 模型 <span class="math inline"><em>M</em></span>，结果显示 <span class="math inline"><em>M</em>(<em>x</em><sub>adv</sub>) ≠ <em>M</em>(<em>x</em>)</span>，即迁移攻击成功。</p>
</blockquote>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250628200750040.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="defense-against-imitation-attacks">Defense against Imitation Attacks</h3>
<h4 id="add-noise-on-the-victim-output">Add noise on the victim output</h4>
<blockquote>
<p>Defense in text classification</p>
</blockquote>
<ul>
<li>With the cost of undermining the original performance</li>
<li>使用加入噪声的输出作为<hanla></hanla>imitator<hanla></hanla>的训练资料</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250628200906736.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="train-an-undistillable-victim-model">Train an undistillable victim model</h4>
<p>训练一个不可蒸馏的受害者模型，使得攻击者即使能够访问模型的输出，也无法有效地模仿（distill）该模型的行为。</p>
<p><strong>模型蒸馏（Knowledge Distillation）</strong>是一种模型压缩<hanla></hanla>/<hanla></hanla>迁移学习方法：</p>
<blockquote>
<p>学生模型（student）通过模仿教师模型（teacher）的输出（通常是 soft label，即每个类别的概率分布），学会其行为模式。</p>
</blockquote>
<p>在<strong>模仿攻击中</strong>，攻击者充当学生，通过查询受害者模型（victim）API<hanla></hanla>获得 soft label 或 hard label，用来训练 imitator 模型。</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250628201125737.png" srcset="/img/loading.gif" lazyload></p>
<ol type="1">
<li>Train a clean teacher normally</li>
<li>Train a nasty teacher whose objectives are
<ul>
<li><strong>Minimizing the cross entropy (CE) loss</strong> of classification</li>
<li><strong>Maximizing the KL-divergence (KLD)</strong> between the nasty teacher and the clean teacher</li>
</ul></li>
<li>Release the nasty teacher</li>
</ol>
<blockquote>
<p><strong>KL<hanla></hanla>散度衡量的是两个概率分布 P（真实分布）和 Q（预测分布）之间的差异程度</strong>，数值越大，表示 Q<hanla></hanla>和 P 差别越大，模型预测的分布偏离真实分布越远。</p>
<p>交叉熵表示的是：用模型预测的分布 Q 对真实分布 P 的编码长度（信息量）的期望。数值越小，说明模型预测的分布越好，能够更有效地<hanla></hanla>“压缩”<hanla></hanla>真实分布数据，分类准确率越高。</p>
</blockquote>
<h2 id="backdoor-attacks-and-defenses">Backdoor Attacks and Defenses</h2>
<h3 id="backdoor-attacks">Backdoor Attacks</h3>
<ul>
<li>an attack that aims to insert some backdoors during model training that will make the model misbehave when encountering certain triggers</li>
<li>The model should have normal performance when the trigger is not presented</li>
<li>The model deployer is not aware of the backdoor</li>
</ul>
<blockquote>
<p>模型在正常使用时表现正常，但只要输入中包含攻击者设计的<hanla></hanla>“后门触发器”，模型就会输出攻击者期望的结果。</p>
</blockquote>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250628202043747.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="data-poisoning">Data Poisoning</h4>
<p>Assume that we can manipulate the training dataset</p>
<ol type="1">
<li>Construct poisoning dataset</li>
<li>Use the poisoning dataset to train a model</li>
<li>Activate the backdoor with trigger</li>
</ol>
<p>常见的<hanla></hanla>poisoning data:</p>
<table>
<thead>
<tr class="header">
<th>类型</th>
<th>说明</th>
<th>示例</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>像素贴纸</strong></td>
<td>在图像某固定位置添加小块颜色或图案</td>
<td>小红点、水印</td>
</tr>
<tr class="even">
<td><strong>颜色变换</strong></td>
<td>改变图像某部分颜色</td>
<td>特定颜色滤镜</td>
</tr>
<tr class="odd">
<td><strong>语义触发词</strong></td>
<td>文本中插入特定词汇或字符</td>
<td>“cf123”</td>
</tr>
<tr class="even">
<td><strong>语法结构</strong></td>
<td>特殊句式或词序</td>
<td>特定排列顺序</td>
</tr>
</tbody>
</table>
<h4 id="backdoored-plm">Backdoored PLM</h4>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250628202420935.png" srcset="/img/loading.gif" lazyload></p>
<p>Assumption:</p>
<ul>
<li>We aims to release a pre-trained language model (PLM) with backdoor. The PLM will be further fine-tuned</li>
<li>We have no knowledge of the downstream task.</li>
</ul>
<ol type="1">
<li>Select the triggers</li>
<li>Pre-training:
<ul>
<li>For those inputs without triggers, train with MLM as usual</li>
<li>For those inputs with trigger, their MLM prediction target is some random word in the vocabulary</li>
</ul></li>
<li>Release the PLM for downstream fine-tuning</li>
</ol>
<h3 id="defense">Defense</h3>
<h4 id="onion-backdoor-defense-with-outlier-word-detection">ONION (backd<strong>O</strong>or defe<strong>N</strong>se with outl<strong>I</strong>er w<strong>O</strong>rd detectio<strong>N</strong>)</h4>
<blockquote>
<p>通过检测并剔除输入文本中的<hanla></hanla>“异常词”（Outlier Words），从而<strong>有效破坏后门触发器并恢复模型正常行为</strong>。</p>
</blockquote>
<ul>
<li>For each word in the sentence, remove it to see the change in PPL of GPT-2</li>
<li>If the change of PPL is lower than a pre-defined threshold 𝑡, flag the word as outlier(trigger)</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250628202819224.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="bypassing-onion-defense">Bypassing ONION Defense</h4>
<p><strong>Insert multiple repeating triggers</strong>: Removing one trigger will not cause the GPT-2 PPL to significantly lower</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250628202954873.png" srcset="/img/loading.gif" lazyload></p>
<p>如何解决？</p>
<blockquote>
<ul>
<li>连续多轮检测和剔除异常词，直到文本<hanla></hanla>PPL<hanla></hanla>显著下降或无更多异常词</li>
<li>结合文本整体流畅度、语义连贯性指标判断是否存在多个触发词，例如用语言模型对句子整体评分，检测异常波动，提示存在多触发器。</li>
<li>重复触发词在文本中形成特定模式（频繁出现、位置固定等），可设计规则或统计方法检测异常重复模式</li>
</ul>
</blockquote>
<h1 id="adversarial-attack">Adversarial Attack</h1>
<h2 id="example-of-attack">Example of Attack</h2>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250718195915205.png" srcset="/img/loading.gif" lazyload></p>
<p><strong>Targeted</strong>: Anything other than “Cat”</p>
<p><strong>Non-targeted</strong>: Misclassified as a specific class (e.g., “Star Fish”)</p>
<blockquote>
<ul>
<li><p>相同的攻击对于不同模型的影响不同</p></li>
<li><p>加入<hanla></hanla>Noise<hanla></hanla>的数量不同，模型识别的结果也不同</p></li>
</ul>
</blockquote>
<h2 id="how-to-attack">How to Attack</h2>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250718200622943.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<p>与原图的输出尽可能远，与目标尽可能近（对于<hanla></hanla>Targeted Attack<hanla></hanla>而言）</p>
</blockquote>
<h3 id="non-perceivable">Non-perceivable</h3>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250718200723460.png" srcset="/img/loading.gif" lazyload></p>
<p>对于图片而言，<strong>L-infinity</strong> 也许会更加适用，但对于语音、文字等，也许需要其他的准则。</p>
<h3 id="attack-approach">Attack Approach</h3>
<h4 id="gradient-descent">Gradient Descent</h4>
<p>Update <strong>input</strong>, not <strong>parameters</strong></p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250718205424157.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="fast-gradient-sign-method-fgsm">Fast Gradient Sign Method (FGSM)</h4>
<p>https://arxiv.org/abs/1412.6572</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250718205543420.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="iterative-fgsm">Iterative FGSM</h4>
<p>https://arxiv.org/abs/1607.02533</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250718205954373.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="white-box-v.s.-black-box">White Box v.s. Black Box</h2>
<ul>
<li>In the previous attack, we know the network parameters θ</li>
<li>This is called <strong>White Box Attack</strong>.</li>
<li>You cannot obtain model parameters in most online API.</li>
<li>Are we safe if we do not release model?</li>
<li>No, because <strong>Black Box Attack</strong> is possible.</li>
</ul>
<h3 id="black-box-attack">Black Box Attack</h3>
<p><strong>Black Box Attack（黑盒攻击）指的是攻击者无法访问模型的内部结构或参数</strong>，只能<strong>通过输入和输出之间的交互</strong>来对模型进行攻击或欺骗。</p>
<blockquote>
<p>攻击者只能看到输入输出，例如只能调用模型的 API</p>
</blockquote>
<p>If you have the training data of the target network——Train a <strong>proxy</strong> network yourself&amp;Using the <strong>proxy</strong> network to generate attacked objects</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250718211204391.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<p>如果没有目标模型的<hanla></hanla>training data<hanla></hanla>呢？</p>
<ul>
<li>基于梯度估计（score-based）：使用模型输出（如 softmax 分布）估计梯度</li>
<li>合成数据训练替代模型（zero-data transfer attack）：
<ul>
<li><strong>随机生成输入样本</strong>（如图像中的随机噪声、文本中的模板句子）</li>
<li>将这些样本提交给黑盒模型，获得输出标签</li>
<li>用这些（输入, 输出）对，训练 substitute model</li>
<li>在<hanla></hanla>substitute model<hanla></hanla>上执行白盒攻击（如 FGSM、PGD），再用生成的对抗样本攻击原模型</li>
</ul></li>
<li>基于标签的决策边界搜索（Decision-based）：
<ul>
<li>找到一个与 x 相似但预测错误的初始样本 x_adv</li>
<li>沿着 x_adv → x 的方向，逐步减小差异，逼近边界</li>
<li>找到刚好改变分类的最小扰动</li>
</ul></li>
</ul>
</blockquote>
<h3 id="universal-adversarial-attack">Universal Adversarial Attack</h3>
<p>https://arxiv.org/abs/1610.08401</p>
<p>定义：在不改变特定样本的基础上，找到一个通用扰动向量，使得加上它后，<strong>大多数输入样本的模型预测都会被误导</strong>。</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250718214224598.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="one-pixel-attack">One pixel attack</h3>
<p>定义：找到图像中仅一个像素的 RGB 值，使模型对该图像的预测类别发生改变。</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250718214531222.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="attack-in-the-physical-world">Attack in the Physical World</h3>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250718214628815.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>An attacker would need to find perturbations that generalize beyond a single image.</li>
<li>Extreme differences between adjacent pixels in the perturbation are unlikely to be accurately captured by cameras.</li>
<li>It is desirable to craft perturbations that are comprised mostly of colors reproducible by the printer.</li>
</ul>
<blockquote>
<ul>
<li>攻击者需要找到<strong>不仅仅对单张图像有效</strong>的扰动（具有泛化能力的扰动）。</li>
<li>扰动中<strong>相邻像素间的极端差异</strong>不太可能被摄像头准确捕捉。</li>
<li>理想情况下，所构造的扰动应<strong>主要由打印机可再现的颜色组成</strong>。</li>
</ul>
</blockquote>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250718214759930.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="adversarial-reprogramming">Adversarial Reprogramming</h3>
<p>https://arxiv.org/abs/1806.11146</p>
<p><strong>定义</strong>：对一个已有模型（通常是训练在任务 A 上的模型）输入经过特别设计的<strong>对抗性前缀或扰动函数</strong>，使其在<strong>不修改模型结构或参数</strong>的情况下，被<strong>重定向去完成任务 B</strong>。</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250718215041674.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="backdoor-in-model">“Backdoor” in Model</h3>
<p>https://arxiv.org/abs/1804.00792</p>
<p>Attack happens at the training phase</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250718215130158.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<p>be careful of unknown dataset</p>
</blockquote>
<h2 id="defense-1">Defense</h2>
<h3 id="passive-defense">Passive Defense</h3>
<p>定义：在不修改模型架构或权重的前提下，通过<strong>输入分析、扰动检测、预处理或后处理</strong>来防范对抗攻击的一类方法。</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250718222007637.png" srcset="/img/loading.gif" lazyload></p>
<table>
<thead>
<tr class="header">
<th>类别</th>
<th>方法</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>输入预处理</td>
<td>JPEG 压缩、图像平滑、去噪、自编码器重构等</td>
<td>降低对抗扰动对模型的影响</td>
</tr>
<tr class="even">
<td>输入检测</td>
<td>基于统计、模型输出分布或特征空间</td>
<td>检测输入是否为对抗样本</td>
</tr>
<tr class="odd">
<td>特征分析</td>
<td>PCA、SVD、频域分析</td>
<td>分离正常与对抗样本的特征分布</td>
</tr>
<tr class="even">
<td>多模型一致性</td>
<td>检查多个模型对输入预测是否一致</td>
<td>检测输入是否异常</td>
</tr>
<tr class="odd">
<td>后处理校正</td>
<td>软标签平滑、置信度调整</td>
<td>降低对抗样本对最终结果的干扰</td>
</tr>
</tbody>
</table>
<h3 id="proactive-defense">Proactive Defense</h3>
<p>定义：主动防御是一类<strong>通过修改模型结构、训练过程或损失函数</strong>，来<strong>增强模型对对抗扰动的内在鲁棒性</strong>的防御方法。</p>
<p>与 Passive Defense 不同，Proactive Defense 并不试图<hanla></hanla>“检测”<hanla></hanla>或<hanla></hanla>“修复”<hanla></hanla>输入，而是训练模型具备对抗能力，让模型<strong>自己更强</strong>。</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250718221030010.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<p>将生成的对抗样本加入训练数据中，通过暴露模型于<hanla></hanla>“敌意环境”，迫使其学会对抗。</p>
</blockquote>
<h2 id="homework-10-adversarial-attack">Homework 10: Adversarial Attack</h2>
<h3 id="task-description">Task Description</h3>
<h4 id="prerequisite">Prerequisite</h4>
<p>Those are methodologies which you should be familiar with first</p>
<ul>
<li>Attack objective: Non-targeted attack</li>
<li>Attack constraint: L-infinity norm and Parameter <span class="math inline"><em>ε</em></span></li>
<li>Attack algorithm: FGSM/I-FGSM</li>
<li>Attack schema: Black box attack (perform attack on proxy network)</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250719192635778.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="black-box-attack-1">Black-box attack</h4>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250719192744359.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="todo">TODO</h4>
<ol type="1">
<li>Choose any proxy network to attack the black box model from TA</li>
<li>Implement non-targeted adversarial attack method
<ul>
<li>FGSM</li>
<li>I-FGSM</li>
<li><u>MI-FGSM</u></li>
</ul></li>
<li><u>Increase attack transferability by Diverse input (DIM)</u></li>
<li><u>Attack more than one proxy model - <strong>Ensemble attack</strong></u></li>
</ol>
<p><strong>FGSM</strong>: Fast Gradient Sign Method</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250719193002062.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250719193009904.png" srcset="/img/loading.gif" lazyload></p>
<p><strong>I-FGSM</strong>: Iterative Fast Gradient Sign Method</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250719193052867.png" srcset="/img/loading.gif" lazyload></p>
<p><strong>MI-FGSM</strong>: Use momentum to stabilize update directions and escape from poor local maxima</p>
<p>https://arxiv.org/pdf/1710.06081.pdf</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250719193129111.png" srcset="/img/loading.gif" lazyload></p>
<p>Diverse Input (<strong>DIM</strong>):</p>
<ol type="1">
<li><p><strong>Random resizing</strong> (resizes the input images to a random size)</p></li>
<li><p><strong>Random padding</strong> (pads zeros around the input images in a random manner)</p></li>
</ol>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250719193319101.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<p>e.g.&nbsp;DIM + MI-FGSM</p>
</blockquote>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250719193340631.png" srcset="/img/loading.gif" lazyload></p>
<p><strong>Ensemble Attack</strong>:</p>
<ul>
<li>Choose a list of proxy models</li>
<li>Choose an attack algorithm (FGSM, I-FGSM, and so on)</li>
<li>Attack <strong>multiple proxy models at the same time</strong></li>
<li>[paper A] Ensemble adversarial attack:<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1611.02770">Delving into Transferable Adversarial Examples and Black-box Attacks</a></li>
<li>[paper B] How to choose suitable proxy models for black-box attack: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2007.00806">Query-Free Adversarial Transfer via Undertrained Surrogates</a></li>
</ul>
<h4 id="evaluation-metrics-1">Evaluation Metrics</h4>
<ul>
<li>Parameter <strong><span class="math inline"><em>ε</em></span> is fixed as 8</strong></li>
<li>Distance measurement: <strong>L-inf. norm</strong></li>
<li><strong>Model Accuracy</strong> is the only evaluation metrics</li>
</ul>
<h3 id="data-format">Data Format</h3>
<ul>
<li>Download link: <a target="_blank" rel="noopener" href="https://github.com/DanielLin94144/ML-attack-dataset/releases/download/v1.0.0/data.zip">link</a></li>
<li>Images:
<ul>
<li><a target="_blank" rel="noopener" href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10</a> images</li>
<li>(32 * 32 RGB images) * <strong>200</strong>
<ul>
<li>airplane/airplane1.png, …, airplane/airplane20.png</li>
<li>…</li>
<li>truck/truck1.png, …, truck/truck20.png</li>
</ul></li>
<li>10 classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck)</li>
<li>20 images for each class</li>
</ul></li>
</ul>
<h3 id="pre-trained-model">Pre-trained model</h3>
<ul>
<li>In this homework, we can perform attack on pretrained models</li>
<li><a target="_blank" rel="noopener" href="https://pypi.org/project/pytorchcv/">Pytorchcv</a> provides multiple models pretrained on CIFAR-10</li>
<li>A model list is provided <a target="_blank" rel="noopener" href="https://github.com/osmr/imgclsmob/blob/master/pytorch/pytorchcv/model_provider.py">here</a></li>
</ul>
<blockquote>
<p>原作业<hanla></hanla>PDF<hanla></hanla>提供的列表链接已失效，这条是现在可见的：<a target="_blank" rel="noopener" href="https://github.com/osmr/pytorchcv/blob/47b542244ef3244bff8ddca456340d1a80d8de84/pytorchcv/model_provider.py">Model List</a></p>
</blockquote>
<h3 id="baselines-3">Baselines</h3>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250719194003703.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<p>由于本次作业是在<hanla></hanla>judgeBoi<hanla></hanla>上提交的，非台大学生无法得出成绩，所以仅给出了代码。</p>
</blockquote>
<h3 id="simple-baseline-6">Simple baseline</h3>
<p>Just run Sample Code.</p>
<p><a target="_blank" rel="noopener" href="https://github.com/Striver98/Machine-Learning-Homework/blob/master/HW10/HW10_simple.py">link</a></p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250719195519998.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="medium-baseline-6">Medium baseline</h3>
<p>Ensemble Attack + random few model + IFGSM</p>
<p><a target="_blank" rel="noopener" href="https://github.com/Striver98/Machine-Learning-Homework/blob/master/HW10/HW10_medium.py">link</a></p>
<p>Ensemble Attack<hanla></hanla>的实现：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 集成攻击<hanla></hanla></span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ensembleNet</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model_names</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-comment"># 创建多个预训练模型的模块列表</span><br>        <span class="hljs-variable language_">self</span>.models = nn.ModuleList([ptcv_get_model(name, pretrained=<span class="hljs-literal">True</span>) <span class="hljs-keyword">for</span> name <span class="hljs-keyword">in</span> model_names])<br>        <span class="hljs-variable language_">self</span>.softmax = nn.Softmax(dim=<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># 初始化集成<hanla></hanla>logits<hanla></hanla>为<hanla></hanla>None</span><br>        ensemble_logits = <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">for</span> i, m <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(<span class="hljs-variable language_">self</span>.models):<br>            <span class="hljs-comment"># 获取当前模型的<hanla></hanla>logits<hanla></hanla>输出</span><br>            logits = m(x)<br>            <span class="hljs-comment"># 如果是第一个模型，直接赋值；否则累加<hanla></hanla>logits</span><br>            <span class="hljs-keyword">if</span> ensemble_logits <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                ensemble_logits = logits<br>            <span class="hljs-keyword">else</span>:<br>                ensemble_logits += logits<br>        <span class="hljs-comment"># 返回集成后的平均<hanla></hanla>logits</span><br>        <span class="hljs-keyword">return</span> ensemble_logits / <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.models)<br><br><span class="hljs-comment"># 构建集成模型<hanla></hanla></span><br>model_names = [<br>    <span class="hljs-string">'nin_cifar10'</span>,       <br>    <span class="hljs-string">'resnet20_cifar10'</span>,   <br>    <span class="hljs-string">'preresnet20_cifar10'</span> <br>]<br>ensemble_model = ensembleNet(model_names).to(device)<br>ensemble_model.<span class="hljs-built_in">eval</span>()<br></code></pre></td></tr></tbody></table></figure>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250719204113405.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="strong-baseline-6">Strong baseline</h3>
<p>Ensemble Attack + many models + MIFGSM</p>
<p><a target="_blank" rel="noopener" href="https://github.com/Striver98/Machine-Learning-Homework/blob/master/HW10/HW10_strong.py">link</a></p>
<p>MIFGSM<hanla></hanla>的实现：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># MI-FGSM（Momentum Iterative FGSM）攻击<hanla></hanla></span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">mifgsm</span>(<span class="hljs-params">model, x, y, loss_fn, epsilon=epsilon, alpha=alpha, num_iter=<span class="hljs-number">20</span>, decay=<span class="hljs-number">1.0</span></span>):<br>    x_adv = x<br>    <span class="hljs-comment"># 初始化动量张量</span><br>    momentum = torch.zeros_like(x).detach().to(device)<br>    <span class="hljs-comment"># 执行<hanla></hanla>num_iter<hanla></hanla>次迭代</span><br>    grad = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_iter):<br>        x_adv = x_adv.detach().clone()<br>        x_adv.requires_grad = <span class="hljs-literal">True</span><br>        loss = loss_fn(model(x_adv), y)<br>        loss.backward()<br>        <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 动量计算</span><br>        grad =  decay * grad + x_adv.grad.detach() / torch.norm(x_adv.grad.detach())<br>        x_adv = x_adv + alpha * grad.sign()<br>        x_adv = torch.<span class="hljs-built_in">max</span>(torch.<span class="hljs-built_in">min</span>(x_adv, x+epsilon), x-epsilon)<br>    <span class="hljs-keyword">return</span> x_adv<br></code></pre></td></tr></tbody></table></figure>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250719205131127.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="boss-baseline-2">Boss Baseline</h3>
<p><a target="_blank" rel="noopener" href="https://github.com/Striver98/Machine-Learning-Homework/blob/master/HW10/HW10_boss.py">link</a></p>
<p>DIM-MIFGSM<hanla></hanla>的实现：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># DIM-MIFGSM (Diverse Input Method MI-FGSM) 攻击<hanla></hanla></span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">dim_mifgsm</span>(<span class="hljs-params">model, x, y, loss_fn, epsilon=epsilon, alpha=alpha, num_iter=<span class="hljs-number">20</span>, decay=<span class="hljs-number">1.0</span>, prob=<span class="hljs-number">0.5</span></span>):<br>    <span class="hljs-string">"""</span><br><span class="hljs-string">    DIM-MIFGSM<hanla></hanla>攻击函数，结合了动量和多样化输入变换</span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        model: 目标模型</span><br><span class="hljs-string">        x: 输入图像</span><br><span class="hljs-string">        y: 真实标签</span><br><span class="hljs-string">        loss_fn: 损失函数</span><br><span class="hljs-string">        epsilon: 扰动范围</span><br><span class="hljs-string">        alpha: 步长</span><br><span class="hljs-string">        num_iter: 迭代次数</span><br><span class="hljs-string">        decay: 动量衰减因子</span><br><span class="hljs-string">        prob: 应用变换的概率</span><br><span class="hljs-string">    """</span><br>    x_adv = x<br>    <span class="hljs-comment"># 初始化动量</span><br>    momentum = torch.zeros_like(x).detach().to(device)<br>    <br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_iter):<br>        x_adv = x_adv.detach().clone()<br>        x_adv.requires_grad = <span class="hljs-literal">True</span><br>        <br>        <span class="hljs-comment"># 应用多样化输入变换</span><br>        x_transformed = diverse_input_transform(x_adv, prob)<br>        <br>        loss = loss_fn(model(x_transformed), y)<br>        loss.backward()<br>        <br>        <span class="hljs-comment"># 动量更新</span><br>        grad = x_adv.grad.detach()<br>        momentum = decay * momentum + grad / torch.norm(grad, p=<span class="hljs-number">1</span>)<br>        <br>        <span class="hljs-comment"># 更新对抗样本</span><br>        x_adv = x_adv + alpha * momentum.sign()<br>        <br>        <span class="hljs-comment"># 裁剪到允许范围</span><br>        x_adv = torch.<span class="hljs-built_in">max</span>(torch.<span class="hljs-built_in">min</span>(x_adv, x + epsilon), x - epsilon)<br>    <br>    <span class="hljs-keyword">return</span> x_adv<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">diverse_input_transform</span>(<span class="hljs-params">x, prob=<span class="hljs-number">0.5</span></span>):<br>    <span class="hljs-string">"""</span><br><span class="hljs-string">    多样化输入变换函数</span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        x: 输入图像张量，形状为 (batch_size, channels, height, width)</span><br><span class="hljs-string">        prob: 应用变换的概率</span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">        变换后的图像张量</span><br><span class="hljs-string">    """</span><br>    batch_size, channels, height, width = x.shape<br>    <br>    <span class="hljs-comment"># 随机决定是否应用变换</span><br>    <span class="hljs-keyword">if</span> torch.rand(<span class="hljs-number">1</span>).item() &gt; prob:<br>        <span class="hljs-keyword">return</span> x<br>    <br>    <span class="hljs-comment"># 随机选择变换类型：0-resize, 1-padding</span><br>    transform_type = torch.randint(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, (<span class="hljs-number">1</span>,)).item()<br>    <br>    <span class="hljs-keyword">if</span> transform_type == <span class="hljs-number">0</span>:<br>        <span class="hljs-comment"># Random resizing</span><br>        <span class="hljs-keyword">return</span> random_resize(x)<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-comment"># Random padding</span><br>        <span class="hljs-keyword">return</span> random_padding(x)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">random_resize</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-string">"""</span><br><span class="hljs-string">    随机调整图像大小</span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        x: 输入图像张量，形状为 (batch_size, channels, height, width)</span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">        调整大小后的图像张量</span><br><span class="hljs-string">    """</span><br>    batch_size, channels, height, width = x.shape<br>    <br>    <span class="hljs-comment"># 随机选择新的尺寸（在原尺寸的<hanla></hanla>80%-120%<hanla></hanla>之间）</span><br>    resize_ratio = <span class="hljs-number">0.8</span> + <span class="hljs-number">0.4</span> * torch.rand(<span class="hljs-number">1</span>).item()  <span class="hljs-comment"># [0.8, 1.2]</span><br>    new_size = <span class="hljs-built_in">int</span>(height * resize_ratio)<br>    <br>    <span class="hljs-comment"># 确保新尺寸至少为<hanla></hanla>16，且不超过原尺寸的<hanla></hanla>1.5<hanla></hanla>倍</span><br>    new_size = <span class="hljs-built_in">max</span>(<span class="hljs-number">16</span>, <span class="hljs-built_in">min</span>(new_size, <span class="hljs-built_in">int</span>(height * <span class="hljs-number">1.5</span>)))<br>    <br>    <span class="hljs-comment"># 使用双线性插值调整大小</span><br>    x_resized = torch.nn.functional.interpolate(<br>        x, size=(new_size, new_size), mode=<span class="hljs-string">'bilinear'</span>, align_corners=<span class="hljs-literal">False</span><br>    )<br>    <br>    <span class="hljs-comment"># 如果图像变小了，需要用零填充回原尺寸</span><br>    <span class="hljs-keyword">if</span> new_size &lt; height:<br>        pad_size = (height - new_size) // <span class="hljs-number">2</span><br>        x_resized = torch.nn.functional.pad(<br>            x_resized, <br>            (pad_size, height - new_size - pad_size, pad_size, height - new_size - pad_size),<br>            mode=<span class="hljs-string">'constant'</span>, value=<span class="hljs-number">0</span><br>        )<br>    <span class="hljs-comment"># 如果图像变大了，需要裁剪回原尺寸</span><br>    <span class="hljs-keyword">elif</span> new_size &gt; height:<br>        start = (new_size - height) // <span class="hljs-number">2</span><br>        x_resized = x_resized[:, :, start:start+height, start:start+width]<br>    <br>    <span class="hljs-keyword">return</span> x_resized<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">random_padding</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-string">"""</span><br><span class="hljs-string">    随机在图像周围填充零</span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        x: 输入图像张量，形状为 (batch_size, channels, height, width)</span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">        填充后再裁剪回原尺寸的图像张量</span><br><span class="hljs-string">    """</span><br>    batch_size, channels, height, width = x.shape<br>    <br>    <span class="hljs-comment"># 随机选择填充大小（最多为原尺寸的<hanla></hanla>10%）</span><br>    max_pad = height // <span class="hljs-number">10</span><br>    pad_left = torch.randint(<span class="hljs-number">0</span>, max_pad + <span class="hljs-number">1</span>, (<span class="hljs-number">1</span>,)).item()<br>    pad_right = torch.randint(<span class="hljs-number">0</span>, max_pad + <span class="hljs-number">1</span>, (<span class="hljs-number">1</span>,)).item()<br>    pad_top = torch.randint(<span class="hljs-number">0</span>, max_pad + <span class="hljs-number">1</span>, (<span class="hljs-number">1</span>,)).item()<br>    pad_bottom = torch.randint(<span class="hljs-number">0</span>, max_pad + <span class="hljs-number">1</span>, (<span class="hljs-number">1</span>,)).item()<br>    <br>    <span class="hljs-comment"># 应用填充</span><br>    x_padded = torch.nn.functional.pad(<br>        x, (pad_left, pad_right, pad_top, pad_bottom), mode=<span class="hljs-string">'constant'</span>, value=<span class="hljs-number">0</span><br>    )<br>    <br>    <span class="hljs-comment"># 随机裁剪回原尺寸</span><br>    padded_height, padded_width = x_padded.shape[<span class="hljs-number">2</span>], x_padded.shape[<span class="hljs-number">3</span>]<br>    start_h = torch.randint(<span class="hljs-number">0</span>, padded_height - height + <span class="hljs-number">1</span>, (<span class="hljs-number">1</span>,)).item()<br>    start_w = torch.randint(<span class="hljs-number">0</span>, padded_width - width + <span class="hljs-number">1</span>, (<span class="hljs-number">1</span>,)).item()<br>    <br>    x_cropped = x_padded[:, :, start_h:start_h+height, start_w:start_w+width]<br>    <br>    <span class="hljs-keyword">return</span> x_cropped<br><br><span class="hljs-comment"># 使用集成模型执行<hanla></hanla>DIM-MIFGSM<hanla></hanla>攻击<hanla></hanla></span><br>ensemble_dim_mifgsm_examples, ensemble_dim_mifgsm_acc, ensemble_dim_mifgsm_loss = gen_adv_examples(ensemble_model, adv_loader, dim_mifgsm, loss_fn)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f'ensemble_dim_mifgsm_acc = <span class="hljs-subst">{ensemble_dim_mifgsm_acc:<span class="hljs-number">.5</span>f}</span>, ensemble_dim_mifgsm_loss = <span class="hljs-subst">{ensemble_dim_mifgsm_loss:<span class="hljs-number">.5</span>f}</span>'</span>)<br><br><span class="hljs-comment"># 创建集成<hanla></hanla>DIM-MIFGSM<hanla></hanla>对抗样本目录并保存图像<hanla></hanla></span><br>create_dir(root, <span class="hljs-string">'ensemble_dim_mifgsm'</span>, ensemble_dim_mifgsm_examples, adv_names)<br></code></pre></td></tr></tbody></table></figure>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250719213421792.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="domain-adaptation">Domain Adaptation</h1>
<blockquote>
<p><strong>Domain Adaptation（领域自适应）</strong> 是迁移学习（Transfer Learning）中的一个重要分支，其目标是<strong>将源领域（source domain）学到的知识迁移到目标领域（target domain）</strong>，尤其是在目标领域的标注数据非常稀缺甚至没有的情况下，仍然能在目标领域中保持良好的模型性能。</p>
</blockquote>
<h2 id="domain-shift">Domain Shift</h2>
<p>Training and testing data have different distributions.</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250723175005711.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="basic-idea">Basic Idea</h2>
<p>Source Domain(with labeled data)</p>
<p>Knowledge of target domain:</p>
<ol type="1">
<li><p>Little but labeled</p>
<ul>
<li><strong>Idea</strong>: training a model by source data, then <strong>fine-tune</strong> the model by target data</li>
<li><strong>Challenge</strong>: only limited target data, so be careful about <strong>overfitting</strong></li>
</ul></li>
<li><p>Large amount of unlabeled data</p>
<ul>
<li><p><strong>Idea</strong>: Learn to ignore the differences</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250723175529548.png" srcset="/img/loading.gif" lazyload></p></li>
</ul></li>
<li><p>little &amp; unlabeled</p>
<ul>
<li><p>Idea: Domain Generalization</p></li>
<li><p>https://ieeexplore.ieee.org/document/8578664</p></li>
<li><p>https://arxiv.org/abs/2003.13216</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250723233701685.png" srcset="/img/loading.gif" lazyload></p></li>
</ul></li>
</ol>
<h2 id="domain-adversarial-training">Domain Adversarial Training</h2>
<blockquote>
<p><strong>Domain Adversarial Training（领域对抗训练）</strong>是领域自适应（Domain Adaptation）中最有代表性和有效的方法之一，其核心思想是：<strong>通过对抗学习方式，使源领域和目标领域的特征在判别器面前不可区分，从而实现分布对齐</strong>。</p>
</blockquote>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250723175753361.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250723175950564.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="basic-idea-1">Basic Idea</h3>
<p>领域对抗训练借鉴了 GAN（生成对抗网络）中的<hanla></hanla>“对抗”<hanla></hanla>机制，引入一个<strong>领域判别器（Domain Discriminator）</strong>，并与特征提取器<hanla></hanla>“对抗”<hanla></hanla>训练：</p>
<ul>
<li><strong>特征提取器（Feature Extractor）</strong>：学习输入的中间表示，希望让下游任务（如分类）更容易。</li>
<li><strong>标签分类器（Label Classifier）</strong>：利用特征进行任务预测（如情感分类、目标识别）。<span class="math inline"><em>L</em></span><hanla></hanla>是分类器输出与源域数据真实标签<hanla></hanla>l<hanla></hanla>的<hanla></hanla>cross-entropy。</li>
<li><strong>领域判别器（Domain Discriminator）</strong>：判断一个样本是来自源领域还是目标领域。</li>
</ul>
<blockquote>
<p>特征提取器既要为主任务提取有判别力的特征，又要<hanla></hanla>“欺骗”<hanla></hanla>领域判别器，让源域和目标域的特征<strong>不可区分</strong>，从而实现<strong>领域对齐</strong>。</p>
</blockquote>
<h3 id="model-architecture">Model Architecture</h3>
<figure class="highlight css"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs css"><hanla></hanla>输入图像<br>   ↓<br><span class="hljs-selector-attr">[Feature Extractor]</span><br>   ↓                  ↘<br><span class="hljs-selector-attr">[Label Classifier]</span>    <span class="hljs-selector-attr">[Domain Discriminator]</span><br>   ↓                      ↓<br>  <span class="hljs-attribute">y</span>̂（任务预测）        d̂（域判别）<br><br></code></pre></td></tr></tbody></table></figure>
<h2 id="limitation">Limitation</h2>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250723182740927.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="considering-decision-boundary">Considering Decision Boundary</h3>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250723182823410.png" srcset="/img/loading.gif" lazyload></p>
<p>Reference：</p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1802.08735">Used in Decision-boundary Iterative Refinement Training with a Teacher (DIRT T)</a></p></li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1712.02560">Maximum Classifier Discrepancy</a></p></li>
</ul>
<h4 id="constraining-via-conditional-entropy-minimization">Constraining via Conditional Entropy Minimization</h4>
<blockquote>
<p>Virtual Adversarial Domain Adaptation (VADA) model: a basic combination of <strong>domain adversarial training</strong> and <strong>semi-supervised training objectives</strong>.</p>
</blockquote>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/x1.png" srcset="/img/loading.gif" lazyload></p>
<p>The cluster assumption states that the input distribution <span class="math inline"><em>X</em></span> contains clusters and that points in the same cluster come from the same class. If the cluster assumption holds, the optimal decision boundaries should occur far away from data-dense regions in the space of 𝒳. we achieve this behavior via <strong>minimization of the conditional entropy</strong> with respect to the target distribution.</p>
<blockquote>
<p>聚类假设（Cluster Assumption）认为：输入分布 X 存在若干聚类簇，且同一簇内的数据点应属于相同类别。若聚类假设成立，最优决策边界应当位于输入空间 𝒳 中数据密度较低的区域。我们通过对<strong>目标分布的条件熵最小化</strong>来实现这一特性。</p>
</blockquote>
<p>Intuitively, minimizing the conditional entropy forces the classifier to be confident on the unlabeled target data, thus driving the classifier’s decision boundaries away from the target data. However,this approximation breaks down if the classifier <span class="math inline"><em>h</em></span> is not locally-Lipschitz. Without the locally-Lipschitz constraint, the classifier is allowed to abruptly change its prediction in the vicinity of the training data points, which 1) results in a unreliable empirical estimate of conditional entropy and 2) allows placement of the classifier decision boundaries close to the training samples even when the empirical conditional entropy is minimized. To prevent this, we propose to explicitly incorporate the <strong>locally-Lipschitz constraint</strong> via <strong>virtual adversarial training</strong>.</p>
<blockquote>
<p>直观而言，最小化条件熵会迫使分类器对未标注目标数据做出高置信度预测，从而将决策边界推离目标数据分布区域。然而，如果分类器 h 不满足局部<hanla></hanla>Lipschitz<hanla></hanla>连续性条件，这一近似方法就会失效。在没有局部<hanla></hanla>Lipschitz<hanla></hanla>约束的情况下，分类器可以在训练数据点附近突然改变其预测结果，这将导致：1) 条件熵的经验估计变得不可靠；2) 即使经验条件熵被最小化，分类器的决策边界仍可能被放置在靠近训练样本的位置。为防止这种情况，我们提出通过<strong>虚拟对抗训练</strong>显式地引入<strong>局部<hanla></hanla>Lipschitz<hanla></hanla>约束</strong>。</p>
</blockquote>
<h4 id="decision-boundary-iterative-refinement-training">Decision-boundary Iterative Refinement Training</h4>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/x2.png" srcset="/img/loading.gif" lazyload></p>
<p>Initialize with the VADA model and then further minimize the cluster assumption violation in the target domain. In particular, we first use VADA to learn an initial classifier <span class="math inline"><em>h</em><sub><em>θ</em><sub>0</sub></sub></span>. Next, we incrementally push the classifier’s decision boundaries away from data-dense regions by minimizing the target-side cluster assumption violation loss <span class="math inline"><em>ℒ</em><sub><em>t</em></sub></span>. We denote this procedure <strong>Decision-boundary Iterative Refinement Training (DIRT)</strong>.</p>
<blockquote>
<p>先用<hanla></hanla>VADA<hanla></hanla>模型初始化，再进一步最小化目标域的聚类假设违背程度。具体而言，我们首先通过<hanla></hanla>VADA<hanla></hanla>学习初始分类器<hanla></hanla><span class="math inline"><em>h</em><sub><em>θ</em><sub>0</sub></sub></span>，随后通过最小化目标域聚类假设违背损失ℒₜ，逐步将分类器的决策边界从数据密集区域推离。我们将该方法称为决策边界迭代优化训练（Decision-boundary Iterative Refinement Training，DIRT）。</p>
</blockquote>
<h4 id="maximum-classifier-discrepancy-for-unsupervised-domain-adaptation">Maximum Classifier Discrepancy for Unsupervised Domain Adaptation</h4>
<blockquote>
<p>通过利用分类器对目标样本的预测结果，来更好地对齐源域和目标域的特征分布。</p>
</blockquote>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250723231015943.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250723231737968.png" srcset="/img/loading.gif" lazyload></p>
<p>为有效检测源域支持集外的目标样本，本文提出训练判别器（F1<hanla></hanla>和<hanla></hanla>F2）以最大化目标特征的预测差异度。若无此操作，两个分类器可能过于相似而无法识别支持集外的目标样本。随后训练生成器通过最小化该差异度来<hanla></hanla>“欺骗”<hanla></hanla>判别器，此举促使目标样本特征被生成在源域支持集内。这种对抗学习步骤在我们的方法中循环执行，最终目标是获得目标域支持集被源域支持集包含的特征分布。</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250723232827616.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<p><strong>场景设定</strong></p>
<ul>
<li><strong>源超市（日本店）</strong>：已有完整的商品标签系统（比如<hanla></hanla>“寿司”“清酒”<hanla></hanla>分类明确）</li>
<li><strong>目标超市（美国店）</strong>：新开业，商品未分类，但有些商品日本店从未见过（比如<hanla></hanla>“汉堡套餐”“奶昔”）</li>
</ul>
<p><strong>传统方法的问题</strong></p>
<p>就像让一个在日本店培训的店员直接给美国店商品贴标签：</p>
<ul>
<li>店员只关心<hanla></hanla>“商品摆放位置像不像日本店”（特征分布对齐）</li>
<li>结果：把汉堡放在寿司区（因为都是圆形），奶昔误标为清酒（都是液体）</li>
<li><strong>根本缺陷</strong>：完全没考虑商品本身的类别特性</li>
</ul>
<p><strong>创新方法</strong></p>
<p>派两个在日本店培训的专家店员 + 一个商品摆放员：</p>
<ol type="1">
<li><strong>第一阶段：找问题商品</strong>
<ul>
<li>专家<hanla></hanla>A<hanla></hanla>说汉堡该放快餐区，专家<hanla></hanla>B<hanla></hanla>坚持该放冷冻区 → 出现分类分歧（高差异度）</li>
<li>这意味着汉堡在美国店的摆放位置（特征）超出了日本店的经验范围（源域支持集外）</li>
</ul></li>
<li><strong>第二阶段：调整摆放策略</strong>
<ul>
<li>商品摆放员根据分歧反馈：
<ul>
<li>把汉堡移到两个专家都认可的位置（比如新设<hanla></hanla>“美式快餐”<hanla></hanla>区）</li>
<li>让奶昔的摆放方式既不像清酒也不像果汁（最小化差异度）</li>
</ul></li>
</ul></li>
<li><strong>持续优化</strong>
<ul>
<li>反复进行<hanla></hanla>“专家找茬→摆放员调整”<hanla></hanla>的循环</li>
<li>最终效果：
<ul>
<li>美国店商品区包含日本店所有分类（寿司<hanla></hanla>/<hanla></hanla>清酒区保持不变）</li>
<li>新增区域（汉堡<hanla></hanla>/<hanla></hanla>奶昔）与原有系统和谐共存</li>
</ul></li>
</ul></li>
</ol>
<p><strong>为什么需要两个专家？</strong></p>
<ul>
<li>单个专家容易固执己见（单分类器陷入局部最优）</li>
<li>两个专家不同视角：
<ul>
<li>专家<hanla></hanla>A<hanla></hanla>关注食品原料（分类器<hanla></hanla>F1<hanla></hanla>侧重纹理特征）</li>
<li>专家<hanla></hanla>B<hanla></hanla>关注包装形状（分类器<hanla></hanla>F2<hanla></hanla>侧重几何特征）</li>
</ul></li>
<li>只有当他们都觉得<hanla></hanla>“这商品没见过”<hanla></hanla>时，才真正需要调整系统</li>
</ul>
</blockquote>
<h2 id="homework-11-domain-adaptation">Homework 11: Domain Adaptation</h2>
<h3 id="task-description-1">Task Description</h3>
<p>Given real images (with labels) and drawing images (without labels), please use domain adaptation technique to make your network predict the drawing images correctly.</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250724212042984.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="dataset-1">Dataset</h3>
<ul>
<li>Label: 10 classes (numbered from 0 to 9), as following pictures described.</li>
<li>Training : 5000 (32, 32) RGB real images (with label).</li>
<li>Testing : 100000 (28, 28) gray scale drawing images.</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250724212202764.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="baseline-guides">Baseline Guides</h3>
<table>

<thead>
<tr class="header">
<th style="text-align: center;">Baseline</th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: left;">Hints</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Simple</td>
<td style="text-align: center;">0.41962</td>
<td style="text-align: left;">Just run the code and submit answer.</td>
</tr>
<tr class="even">
<td style="text-align: center;">Medium</td>
<td style="text-align: center;">0.59980</td>
<td style="text-align: left;">Set proper λ in DaNN algorithm&amp;Training more epochs.</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Strong</td>
<td style="text-align: center;">0.71874</td>
<td style="text-align: left;">The Test data is label-balanced, can you make use of this additional information?</td>
</tr>
<tr class="even">
<td style="text-align: center;">Boss</td>
<td style="text-align: center;">0.77956</td>
<td style="text-align: left;">○ All the techniques you’ve learned in CNN.<br>■ Change optimizer, learning rate, set lr_scheduler, etc…<br>■ Ensemble the model or output you tried.<br>○ Implement other advanced adversarial training.<br>■ For example, MCD MSDA DIRT-T<br>○ Huh, semi-supervised learning may help, isn’t it?<br>○ What about unsupervised learning? (like Universal Domain Adaptation?)</td>
</tr>
</tbody>
</table>
<h3 id="boss-baseline-3">Boss Baseline</h3>
<blockquote>
<p>由于时间原因，我没有从<hanla></hanla>Simple<hanla></hanla>开始训练。</p>
</blockquote>
<p><a target="_blank" rel="noopener" href="https://github.com/Striver98/Machine-Learning-Homework/blob/master/HW11/HW11.py">Code Link</a></p>
<p>我采用了<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1505.07818">论文</a>中对于<span class="math inline"><em>λ</em></span>的<hanla></hanla>Adaptation：</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250725120807978.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">adaptive_lambda</span>(<span class="hljs-params">epoch, num_epoch</span>):<br>    p = epoch / num_epoch<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">2.</span> / (<span class="hljs-number">1</span>+np.exp(-<span class="hljs-number">10</span>*p)) - <span class="hljs-number">1</span>    <br></code></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">num_epoch = <span class="hljs-number">2000</span><br><br><span class="hljs-comment"># train num_epoch</span><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epoch):<br>    <span class="hljs-comment"># You should chooose lamnda cleverly.</span><br>    lamb = adaptive_lambda(epoch, num_epoch)<br>    train_D_loss, train_F_loss, train_acc = train_epoch(source_dataloader, target_dataloader, lamb=lamb)<br><br>    torch.save(feature_extractor.state_dict(), <span class="hljs-string">f'extractor_model.bin'</span>)<br>    torch.save(label_predictor.state_dict(), <span class="hljs-string">f'predictor_model.bin'</span>)<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">'epoch {:&gt;3d}: train D loss: {:6.4f}, train F loss: {:6.4f}, acc {:6.4f}'</span>.<span class="hljs-built_in">format</span>(epoch, train_D_loss, train_F_loss, train_acc))<br></code></pre></td></tr></tbody></table></figure>
<p>Output:</p>
<figure>
<img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250726095908765.png" srcset="/img/loading.gif" lazyload alt="image-20250726095908765"><figcaption aria-hidden="true">image-20250726095908765</figcaption>
</figure>
<h1 id="reinforcement-learning">Reinforcement Learning</h1>
<h2 id="what-is-rl-three-steps-in-ml">What is RL? (Three steps in ML)</h2>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250727225004655.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="step-1-function-with-unknown">Step 1: Function with Unknown</h3>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250727225224325.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>Input of neural network: the <strong>observation</strong> of machine represented as a vector or a matrix</li>
<li>Output neural network : each <strong>action</strong> corresponds to a neuron in output layer</li>
</ul>
<blockquote>
<p>在强化学习中，我们通常需要一个函数来指导智能体的决策。这个函数的参数最初是未知的，需要通过数据学习得到。</p>
</blockquote>
<h3 id="step-2-define-loss">Step 2: Define “Loss”</h3>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250727225759167.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<p>为了优化函数参数，需要定义一个衡量当前表现与目标差距的指标，即损失函数（或目标函数）。</p>
</blockquote>
<h3 id="step-3-optimization">Step 3: Optimization</h3>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250727230855214.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<p>把<hanla></hanla>Actor<hanla></hanla>看作<hanla></hanla>Generator，Environment<hanla></hanla>看作<hanla></hanla>Discriminator，强化学习的优化训练过程就像<hanla></hanla>GAN<hanla></hanla>的思想。</p>
</blockquote>
<h2 id="policy-gradient">Policy Gradient</h2>
<h3 id="how-to-control-your-actor">How to control your actor</h3>
<p>Make it take (or don’t take) a specific action <span class="math inline"><em>â</em></span> given specific observation <span class="math inline"><em>𝑠</em></span>.</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250727231759618.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250727232031577.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250727232013371.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<p><span class="math inline"><em>A</em><sub><em>i</em></sub></span><hanla></hanla>表示做每一个<hanla></hanla>Action<hanla></hanla>的<hanla></hanla>“鼓励<hanla></hanla>“程度</p>
</blockquote>
<h4 id="version-0">Version 0</h4>
<p>令<hanla></hanla><span class="math inline"><em>A</em><sub><em>i</em></sub></span><hanla></hanla>等于执行当前<hanla></hanla>Action<hanla></hanla>产生的<hanla></hanla>Reward</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250727232218580.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>An action <strong>affect the subsequent observations and thus subsequent rewards.</strong></li>
<li><strong>Reward delay</strong>: Actor has to sacrifice immediate reward to gain more long term reward.（短视）</li>
<li>Example: In space invader, only “fire” obtains reward, so vision 0 will learn an actor that <strong>always “fire”</strong>(开火成瘾症)</li>
</ul>
<h4 id="version-1">Version 1</h4>
<p>令<hanla></hanla><span class="math inline"><em>A</em><sub><em>i</em></sub></span><hanla></hanla>等于执行当前<hanla></hanla>Action<hanla></hanla>产生的后续<hanla></hanla>Reward<hanla></hanla>的总和</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250727232757521.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="version-2">Version 2</h4>
<p>在<hanla></hanla>Version 1<hanla></hanla>的基础上，给执行当前<hanla></hanla>Action<hanla></hanla>后产生的后续<hanla></hanla><span class="math inline"><em>r</em><sub><em>i</em></sub></span><hanla></hanla>加上超参数<span class="math inline"><em>𝛾</em></span>，使得离当前<hanla></hanla>observation<hanla></hanla>近产生的<hanla></hanla>reward<hanla></hanla>影响比重大，远的影响比重小。</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250727233327907.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="version-3">Version 3</h4>
<p>在<hanla></hanla>Version 2<hanla></hanla>基础上对<hanla></hanla><span class="math inline"><em>A</em><sub><em>i</em></sub></span><hanla></hanla>进行<hanla></hanla>Normalization</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250727233731290.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="policy-gradient-1">Policy Gradient</h3>
<ul>
<li>Initialize actor network parameters <span class="math inline"><em>θ</em><sub>0</sub></span></li>
<li>For training iteration <span class="math inline"><em>i</em></span>=1 to T
<ul>
<li>Using actor <span class="math inline"><em>θ</em><sup><em>i</em> − 1</sup></span> to interact</li>
<li>Obtain data <span class="math inline">{<em>s</em><sub>1</sub>, <em>a</em><sub>1</sub>}</span>,<span class="math inline">{<em>s</em><sub>2</sub>, <em>a</em><sub>2</sub>}</span>,…,<span class="math inline">{<em>s</em><sub><em>N</em></sub>, <em>a</em><sub><em>N</em></sub>}</span></li>
<li>Compute <span class="math inline"><em>A</em><sub>1</sub></span>, <span class="math inline"><em>A</em><sub>2</sub></span>,…,<span class="math inline"><em>A</em><sub><em>N</em></sub></span></li>
<li>Compute loss <span class="math inline"><em>L</em></span></li>
<li><span class="math inline"><em>θ</em><sup><em>i</em></sup> = <em>θ</em><sup><em>i</em> − 1</sup> − <em>η</em>∇<em>L</em></span></li>
</ul></li>
<li>Data collection is in the “for loop” of training iterations.</li>
<li>However the experience of <span class="math inline"><em>θ</em><sup><em>i</em> − 1</sup></span> <strong>cannot</strong> use to train <span class="math inline"><em>θ</em><sup><em>i</em></sup></span>, that’s why we need <strong>Policy Gradient</strong></li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250727234829525.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="on-policy-v.s-.-off-policy">On-policy v.s . Off-policy</h3>
<ul>
<li>The actor to train and the actor for interacting is the <strong>same</strong>. →On-policy</li>
<li>Can the actor to train and the actor for interacting be <strong>different</strong>? →Off-policy</li>
<li>In this way, we do not have to collection data after each update.</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250727235030494.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="off-policy-proximal-policy-optimization-ppo">Off-policy → Proximal Policy Optimization (PPO)</h4>
<ul>
<li>The actor to train has to know its difference from the actor to interact.</li>
</ul>
<blockquote>
<p>PPO 要求训练用的策略（actor）知道交互用的策略与自己的差异，这样才能在训练中合理控制更新幅度，避免策略偏离太远而不稳定。</p>
</blockquote>
<h2 id="actor-critic">Actor-Critic</h2>
<blockquote>
<p>The output values of a critic depend on the actor evaluated.</p>
</blockquote>
<ul>
<li>A critic does not directly determine the action.</li>
<li>Given an actor 𝜃, it evaluates how good the actor is</li>
<li><strong>Value function</strong> <span class="math inline"><em>V</em><sup><em>θ</em></sup>(<em>s</em>)</span>: When using actor 𝜃, the <strong>discounted cumulated reward</strong> expects to be obtained after seeing s</li>
</ul>
<h4 id="how-to-estimate-value-function">How to estimate Value Function</h4>
<ul>
<li><strong>Monte-Carlo (MC) based approach</strong>: The critic watches actor 𝜃 to interact with the environment.</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250728000230973.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li><strong>Temporal difference (TD) approach</strong></li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250728000333367.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="version-3.5">Version 3.5</h4>
<p>令 Version 3<hanla></hanla>中的<hanla></hanla>baseline <span class="math inline"><em>b</em> = <em>V</em><sup><em>θ</em></sup>(<em>s</em><sub><em>t</em></sub>)</span></p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250728000526513.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250728000609028.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="version-4">Version 4</h4>
<p><strong>Advantage Actor-Critic</strong>: 令<hanla></hanla>Version 3.5 中的<hanla></hanla><span class="math inline"><em>G</em><sub><em>t</em></sub><sup>′</sup></span>也取平均，避免由于采样随机导致的问题</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250728000808856.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="reward-shaping">Reward Shaping</h2>
<p>在很多强化学习任务中，环境提供的<strong>原始奖励稀疏或难以学习</strong>，例如：</p>
<ul>
<li>游戏中只有通关才给一次奖励；</li>
<li>机器人导航中，只有到达目标点才得分。</li>
</ul>
<p>这会导致：</p>
<ul>
<li>学习慢；</li>
<li>策略不稳定；</li>
<li>甚至学不到有效策略。</li>
</ul>
<p><strong>Reward Shaping 就是通过增加额外奖励，引导智能体朝着正确方向前进。</strong></p>
<p>Take playing VizDoom as an example:</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250728001055693.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="curriculum-learning">Curriculum Learning</h4>
<p>Starting from simple training examples, and then becoming harder and harder.</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250728001143056.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="curiosity">Curiosity</h4>
<p>https://arxiv.org/abs/1705.05363</p>
<blockquote>
<p><strong>智能体对<hanla></hanla>“预测不了”<hanla></hanla>的事情更好奇。</strong></p>
</blockquote>
<ul>
<li>如果智能体对动作导致的结果预测不准，就说明这个环境变化很<hanla></hanla>“新奇”</li>
<li>ICM 就会给予智能体一个<strong>内在奖励（intrinsic reward）</strong>，驱动它去继续探索这种<hanla></hanla>“不可预测”<hanla></hanla>的状态变化</li>
<li>这样即使环境不给外部奖励，智能体也能持续学习</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250728001226083.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="no-reward-learning-from-demonstration">No Reward: Learning from Demonstration</h2>
<h3 id="motivation">Motivation</h3>
<ul>
<li>Even define reward can be challenging in some tasks.</li>
<li>Hand crafted rewards can lead to uncontrolled behavior.</li>
</ul>
<h3 id="inverse-reinforcement-learning">Inverse Reinforcement Learning</h3>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250728001516209.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>Principle: <strong><u>The teacher is always the best</u></strong></li>
<li>Basic idea:
<ul>
<li>Initialize an <strong>actor</strong></li>
<li>In each iteration
<ul>
<li>The <strong>actor</strong> interacts with the <strong>environments</strong> to obtain some trajectories.</li>
<li>Define a <strong>reward function</strong> , which makes the trajectories of the teacher <strong>better</strong> than the actor.</li>
<li>The actor learns to <strong>maximize the reward</strong> based on the new <strong>reward function</strong>.</li>
</ul></li>
<li>Output the <strong>reward function</strong> and the actor learned from the reward function</li>
</ul></li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250728001813546.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<p>依旧类似<hanla></hanla>GAN，只是应用场景不同。</p>
</blockquote>
<h2 id="homework12-reinforcement-learning">Homework12: Reinforcement Learning</h2>
<blockquote>
<p>本次作业在<hanla></hanla>JudgeBoi<hanla></hanla>上，非台大学生无法提交，因此我只按照要求修改了代码。此外<hanla></hanla>pyvirtualdisplay<hanla></hanla>只能在<hanla></hanla>Linux<hanla></hanla>系统上运行，Windows<hanla></hanla>系统运行需要改动画实现的部分，并不兼容。</p>
</blockquote>
<h3 id="hw-content">HW Content</h3>
<p>In this Homework, you can implement some Deep Reinforcement Learning methods by yourself：</p>
<ul>
<li>Policy Gradient</li>
<li>Actor-Critic ( Implement by yourself to get high score !)</li>
</ul>
<p>The environment of this HW is <a target="_blank" rel="noopener" href="https://gym.openai.com/envs/LunarLander-v2/">Lunar Lander</a> in gym of OpenAI. Other details can be found in the sample code.</p>
<h3 id="policy-gradient-2">Policy Gradient</h3>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250728115139605.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="actor-critic-1">Actor-Critic</h3>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250728120139365.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="medium-baseline-7">Medium Baseline</h3>
<p>将<hanla></hanla>Reward<hanla></hanla>调整成<hanla></hanla>accumulative decaying reward</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 蒐集訓練資料<hanla></hanla></span><br><span class="hljs-keyword">for</span> episode <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(EPISODE_PER_BATCH):<br>    <br>    state = env.reset()<br>    total_reward, total_step = <span class="hljs-number">0</span>, <span class="hljs-number">0</span><br>    episode_rewards = []<br>    episode_log_probs = []<br>    <br>    <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>        action, log_prob = agent.sample(state)<br>        next_state, reward, done, _ = env.step(action)<br><br>        episode_log_probs.append(log_prob)<br>        episode_rewards.append(reward)<br>        state = next_state<br>        total_reward += reward<br>        total_step += <span class="hljs-number">1</span><br>        <br>        <span class="hljs-keyword">if</span> done:<br>            <span class="hljs-comment"># 计算该<hanla></hanla>episode<hanla></hanla>的累计衰减奖励</span><br>            discounted_rewards = []<br>            <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(episode_rewards)):<br>                cumulative = <span class="hljs-built_in">sum</span>(<span class="hljs-number">0.99</span>**(k-t) * episode_rewards[k] <br>                               <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(t, <span class="hljs-built_in">len</span>(episode_rewards)))<br>                discounted_rewards.append(cumulative)<br>            <br>            <span class="hljs-comment"># 添加到總列表</span><br>            log_probs.extend(episode_log_probs)<br>            rewards.extend(discounted_rewards)<br>            <br>            final_rewards.append(reward)<br>            total_rewards.append(total_reward)<br>            <span class="hljs-keyword">break</span><br></code></pre></td></tr></tbody></table></figure>
<h3 id="boss-baseline-4">Boss Baseline</h3>
<p>将模型改为<hanla></hanla>DQN<hanla></hanla>模型</p>
<blockquote>
<p>以下代码来自<hanla></hanla>GitHub</p>
</blockquote>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">DQN</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, state_size=<span class="hljs-number">8</span>, action_size=<span class="hljs-number">4</span>, fc1_units=<span class="hljs-number">64</span>, fc2_units=<span class="hljs-number">64</span></span>):<br>        <span class="hljs-string">"""Initialize parameters and build model.</span><br><span class="hljs-string">        初始化参数并构建模型</span><br><span class="hljs-string">        Params</span><br><span class="hljs-string">        ======</span><br><span class="hljs-string">            state_size (int): 状态空间的维度 (LunarLander-v2<hanla></hanla>中为<hanla></hanla>8)</span><br><span class="hljs-string">            action_size (int): 动作空间的维度 (LunarLander-v2<hanla></hanla>中为<hanla></hanla>4)</span><br><span class="hljs-string">            fc1_units (int): 第一个隐藏层的神经元数量</span><br><span class="hljs-string">            fc2_units (int): 第二个隐藏层的神经元数量</span><br><span class="hljs-string">        """</span><br>        <span class="hljs-built_in">super</span>(DQN, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.fc1 = nn.Linear(state_size, fc1_units)<br>        <span class="hljs-variable language_">self</span>.fc2 = nn.Linear(fc1_units, fc2_units)<br>        <span class="hljs-variable language_">self</span>.fc3 = nn.Linear(fc2_units, action_size)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, state</span>):<br>        x = F.relu(<span class="hljs-variable language_">self</span>.fc1(state))<br>        x = F.relu(<span class="hljs-variable language_">self</span>.fc2(x))<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.fc3(x)<br><br><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> namedtuple<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ReplayMemory</span>:<br>    <span class="hljs-string">"""Fixed-size buffer to store experience tuples.</span><br><span class="hljs-string">    """</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, CAPACITY</span>):<br>        <span class="hljs-comment"># 设置缓冲区的最大容量</span><br>        <span class="hljs-variable language_">self</span>.capacity = CAPACITY  <br>        <span class="hljs-comment"># 初始化存储经验的列表</span><br>        <span class="hljs-variable language_">self</span>.memory = []  <br>        <span class="hljs-comment"># 当前写入位置的索引（循环使用）</span><br>        <span class="hljs-variable language_">self</span>.index = <span class="hljs-number">0</span>  <br>        <span class="hljs-comment"># 定义经验元组的结构：(状态, 动作, 下一状态, 奖励)</span><br>        <span class="hljs-variable language_">self</span>.transition = namedtuple(<span class="hljs-string">'Transition'</span>, (<span class="hljs-string">'state'</span>, <span class="hljs-string">'action'</span>, <span class="hljs-string">'next_state'</span>, <span class="hljs-string">'reward'</span>))<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">push</span>(<span class="hljs-params">self, state, action, state_next, reward</span>):<br>        <span class="hljs-string">"""Push a new experience to memory.</span><br><span class="hljs-string">        """</span><br>        <span class="hljs-comment"># 如果内存还没满，先添加<hanla></hanla>None<hanla></hanla>占位</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.memory) &lt; <span class="hljs-variable language_">self</span>.capacity:<br>            <span class="hljs-variable language_">self</span>.memory.append(<span class="hljs-literal">None</span>)<br><br>        <span class="hljs-comment"># 在当前索引位置存储新的经验元组</span><br>        <span class="hljs-variable language_">self</span>.memory[<span class="hljs-variable language_">self</span>.index] = <span class="hljs-variable language_">self</span>.transition(state, action, state_next, reward)<br><br>        <span class="hljs-comment"># 更新索引，使用模运算实现循环覆盖（当内存满时覆盖最旧的经验）</span><br>        <span class="hljs-variable language_">self</span>.index = (<span class="hljs-variable language_">self</span>.index + <span class="hljs-number">1</span>) % <span class="hljs-variable language_">self</span>.capacity<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">sample</span>(<span class="hljs-params">self, batch_size</span>):<br>        <span class="hljs-string">"""Randomly sample a batch of experiences from memory.</span><br><span class="hljs-string">        """</span><br>        <span class="hljs-keyword">return</span> random.sample(<span class="hljs-variable language_">self</span>.memory, batch_size)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">"""Return the current size of internal memory.</span><br><span class="hljs-string">        """</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.memory)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DQNAgent</span>():<br>    <span class="hljs-string">"""Interacts with and learns from the environment.</span><br><span class="hljs-string">    """</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_states, num_actions</span>):<br>        <span class="hljs-string">"""Initialize an Agent object.<hanla></hanla>象</span><br><span class="hljs-string">        """</span><br>        <span class="hljs-comment"># 保存状态空间和动作空间的维度</span><br>        <span class="hljs-variable language_">self</span>.num_states = num_states<br>        <span class="hljs-variable language_">self</span>.num_actions = num_actions<br>        <br>        <span class="hljs-comment"># 经验回放缓冲区</span><br>        <span class="hljs-comment"># 设置经验回放内存的容量</span><br>        <span class="hljs-variable language_">self</span>.memory_capacity = <span class="hljs-number">10000</span><br>        <span class="hljs-comment"># 创建经验回放内存实例</span><br>        <span class="hljs-variable language_">self</span>.memory = ReplayMemory(<span class="hljs-variable language_">self</span>.memory_capacity)<br>        <br>        <span class="hljs-comment"># Q<hanla></hanla>网络</span><br>        <span class="hljs-comment"># 主<hanla></hanla>Q<hanla></hanla>网络：用于选择动作和训练</span><br>        <span class="hljs-variable language_">self</span>.main_q_network = DQN() <br>        <span class="hljs-comment"># 目标<hanla></hanla>Q<hanla></hanla>网络：用于计算目标<hanla></hanla>Q<hanla></hanla>值，提高训练稳定性</span><br>        <span class="hljs-variable language_">self</span>.target_q_network = DQN()<br>        <br>        <span class="hljs-comment"># 优化器</span><br>        <span class="hljs-variable language_">self</span>.optimizer = optim.RMSprop(<span class="hljs-variable language_">self</span>.main_q_network.parameters(), lr=<span class="hljs-number">1e-4</span>)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">update_q_function</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">'''update q function</span><br><span class="hljs-string">        '''</span><br>        <br>        <span class="hljs-comment"># 如果内存中的样本数量不足一个批次，直接返回</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.memory) &lt; BATCH_SIZE:<br>            <span class="hljs-keyword">return</span><br>        <span class="hljs-comment"># 如果有足够的样本，创建<hanla></hanla>mini-batch<hanla></hanla>并学习</span><br>        <span class="hljs-variable language_">self</span>.batch, <span class="hljs-variable language_">self</span>.state_batch, <span class="hljs-variable language_">self</span>.action_batch, <span class="hljs-variable language_">self</span>.reward_batch, <span class="hljs-variable language_">self</span>.non_final_next_states = <span class="hljs-variable language_">self</span>.make_minibatch()<br>        <br>        <span class="hljs-comment"># 计算期望的状态-动作价值</span><br>        <span class="hljs-variable language_">self</span>.expected_state_action_values = <span class="hljs-variable language_">self</span>.get_expected_state_action_values()<br><br>        <span class="hljs-comment"># 更新主<hanla></hanla>Q<hanla></hanla>网络</span><br>        <span class="hljs-variable language_">self</span>.update_main_q_network()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">make_minibatch</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">'''Creating a mini-batch</span><br><span class="hljs-string">        '''</span><br><br>        <span class="hljs-comment"># 从经验回放内存中采样一批经验</span><br>        transitions = <span class="hljs-variable language_">self</span>.memory.sample(BATCH_SIZE)<br><br>        <span class="hljs-comment"># 将采样的经验按类型重新组织</span><br>        batch = Transition(*<span class="hljs-built_in">zip</span>(*transitions))<br><br>        <span class="hljs-comment"># 将同类型的数据合并成张量</span><br>        <span class="hljs-comment"># 合并所有状态</span><br>        state_batch = torch.cat(batch.state)<br>        <span class="hljs-comment"># 合并所有动作</span><br>        action_batch = torch.cat(batch.action)<br>        <span class="hljs-comment"># 合并所有奖励</span><br>        reward_batch = torch.cat(batch.reward)<br>        <span class="hljs-comment"># 合并所有非终止的下一状态（过滤掉<hanla></hanla>None<hanla></hanla>值）</span><br>        non_final_next_states = torch.cat([s <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> batch.next_state<br>                                           <span class="hljs-keyword">if</span> s <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>])<br><br>        <span class="hljs-keyword">return</span> batch, state_batch, action_batch, reward_batch, non_final_next_states<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_expected_state_action_values</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">'''calculate Q（St,at）</span><br><span class="hljs-string">        '''</span><br><br>        <span class="hljs-comment"># 将网络设置为评估模式（不计算梯度）</span><br>        <span class="hljs-variable language_">self</span>.main_q_network.<span class="hljs-built_in">eval</span>()<br>        <span class="hljs-variable language_">self</span>.target_q_network.<span class="hljs-built_in">eval</span>()<br><br>        <span class="hljs-comment"># 使用主网络计算当前状态-动作对的<hanla></hanla>Q<hanla></hanla>值</span><br>        <span class="hljs-comment"># gather(1, self.action_batch)<hanla></hanla>选择对应动作的<hanla></hanla>Q<hanla></hanla>值</span><br>        <span class="hljs-variable language_">self</span>.state_action_values = <span class="hljs-variable language_">self</span>.main_q_network(<br>            <span class="hljs-variable language_">self</span>.state_batch).gather(<span class="hljs-number">1</span>, <span class="hljs-variable language_">self</span>.action_batch)<br><br>        <span class="hljs-comment"># 创建掩码，标识哪些状态不是终止状态</span><br>        non_final_mask = torch.BoolTensor(<span class="hljs-built_in">tuple</span>(<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> s: s <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>,<br>                                                    <span class="hljs-variable language_">self</span>.batch.next_state)))<br>        <span class="hljs-comment"># 初始化下一状态的<hanla></hanla>Q<hanla></hanla>值为<hanla></hanla>0</span><br>        next_state_values = torch.zeros(BATCH_SIZE)<br><br>        <span class="hljs-comment"># 使用目标网络计算非终止下一状态的最大<hanla></hanla>Q<hanla></hanla>值</span><br>        next_state_values[non_final_mask] = <span class="hljs-variable language_">self</span>.target_q_network(<br>            <span class="hljs-variable language_">self</span>.non_final_next_states).<span class="hljs-built_in">max</span>(<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>].detach()<br>        <span class="hljs-comment"># 使用<hanla></hanla>Bellman<hanla></hanla>方程计算期望的<hanla></hanla>Q<hanla></hanla>值：Q_target = reward + γ * max_Q(next_state)</span><br>        expected_state_action_values = <span class="hljs-variable language_">self</span>.reward_batch + GAMMA * next_state_values<br>        <br>        <span class="hljs-keyword">return</span> expected_state_action_values <br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_action</span>(<span class="hljs-params">self, state, episode, test=<span class="hljs-literal">False</span></span>):<br>        <span class="hljs-string">"""Returns actions for given state as per current policy.</span><br><span class="hljs-string">        """</span><br>        <span class="hljs-comment"># 如果是测试模式</span><br>        <span class="hljs-keyword">if</span> test:<br>            <span class="hljs-comment"># 设置网络为评估模式</span><br>            <span class="hljs-variable language_">self</span>.main_q_network.<span class="hljs-built_in">eval</span>()<br>            <span class="hljs-comment"># 不计算梯度</span><br>            <span class="hljs-keyword">with</span> torch.no_grad():<br>                <span class="hljs-comment"># max(1)<hanla></hanla>返回每行的最大值</span><br>                <span class="hljs-comment"># [1]<hanla></hanla>获取最大值的索引（即最优动作）</span><br>                <span class="hljs-comment"># view(1, 1)<hanla></hanla>重塑张量形状</span><br>                action = <span class="hljs-variable language_">self</span>.main_q_network(torch.from_numpy(state).unsqueeze(<span class="hljs-number">0</span>)).<span class="hljs-built_in">max</span>(<span class="hljs-number">1</span>)[<span class="hljs-number">1</span>].view(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>            <span class="hljs-comment"># 返回动作的数值</span><br>            <span class="hljs-keyword">return</span> action.item()<br>        <br>        <span class="hljs-comment"># 全局步数计数器（用于<hanla></hanla>epsilon<hanla></hanla>衰减）</span><br>        <span class="hljs-keyword">global</span> steps_done<br>        <span class="hljs-comment"># ε-贪心策略中的<hanla></hanla>epsilon<hanla></hanla>值计算</span><br>        <span class="hljs-comment"># 使用指数衰减：epsilon<hanla></hanla>随着<hanla></hanla>steps_done<hanla></hanla>增加而减少</span><br>        epsilon = EPS_END + (EPS_START - EPS_END) * \<br>                np.exp(-<span class="hljs-number">1.</span> * steps_done / EPS_DECAY)<br>        <br>        <span class="hljs-comment"># 增加步数计数</span><br>        steps_done += <span class="hljs-number">1</span><br>        <br>        <span class="hljs-comment"># 如果随机数大于<hanla></hanla>epsilon，选择贪心动作（利用）</span><br>        <span class="hljs-keyword">if</span> epsilon &lt;= np.random.uniform(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>):<br>            <span class="hljs-comment"># 设置网络为评估模式</span><br>            <span class="hljs-variable language_">self</span>.main_q_network.<span class="hljs-built_in">eval</span>()<br>            <span class="hljs-comment"># 不计算梯度</span><br>            <span class="hljs-keyword">with</span> torch.no_grad():<br>                <span class="hljs-comment"># 选择<hanla></hanla>Q<hanla></hanla>值最大的动作</span><br>                action = <span class="hljs-variable language_">self</span>.main_q_network(state).<span class="hljs-built_in">max</span>(<span class="hljs-number">1</span>)[<span class="hljs-number">1</span>].view(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># 否则随机选择动作（探索）</span><br>            action = torch.LongTensor(<br>                [[random.randrange(<span class="hljs-variable language_">self</span>.num_actions)]])  <br>            <br>        <span class="hljs-keyword">return</span> action<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">update_main_q_network</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">'''update main q net</span><br><span class="hljs-string">        更新主<hanla></hanla>Q<hanla></hanla>网络</span><br><span class="hljs-string">        '''</span><br><br>        <span class="hljs-comment"># 设置网络为训练模式</span><br>        <span class="hljs-variable language_">self</span>.main_q_network.train()<br>        <span class="hljs-comment"># 使用<hanla></hanla>Huber<hanla></hanla>损失函数（smooth_l1_loss）</span><br>        <span class="hljs-comment"># 将<hanla></hanla>expected_state_action_values<hanla></hanla>从<hanla></hanla>(batch_size,)<hanla></hanla>扩展到<hanla></hanla>(batch_size, 1)</span><br>        loss = F.smooth_l1_loss(<span class="hljs-variable language_">self</span>.state_action_values,<br>                                <span class="hljs-variable language_">self</span>.expected_state_action_values.unsqueeze(<span class="hljs-number">1</span>))<br><br>        <span class="hljs-comment"># 更新网络参数</span><br>        <span class="hljs-variable language_">self</span>.optimizer.zero_grad()  <span class="hljs-comment"># 清零梯度</span><br>        loss.backward()  <span class="hljs-comment"># 反向传播计算梯度</span><br>        <span class="hljs-comment"># 梯度裁剪，防止梯度爆炸</span><br>        <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.main_q_network.parameters():<br>            param.grad.data.clamp_(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        <span class="hljs-variable language_">self</span>.optimizer.step()  <span class="hljs-comment"># 更新网络参数</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">memorize</span>(<span class="hljs-params">self, state, action, state_next, reward</span>):<br>        <span class="hljs-string">'''save state, action, state_next, reward into replay memory</span><br><span class="hljs-string">        将状态、动作、下一状态、奖励保存到经验回放内存中</span><br><span class="hljs-string">        '''</span><br>        <span class="hljs-variable language_">self</span>.memory.push(state, action, state_next, reward)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">update_target_q_function</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">'''synchronize Target Q-Network to Main Q-Network</span><br><span class="hljs-string">        将目标<hanla></hanla>Q<hanla></hanla>网络同步到主<hanla></hanla>Q<hanla></hanla>网络</span><br><span class="hljs-string">        '''</span><br>        <span class="hljs-comment"># 将主网络的参数复制到目标网络</span><br>        <span class="hljs-variable language_">self</span>.target_q_network.load_state_dict(<span class="hljs-variable language_">self</span>.main_q_network.state_dict())<br><br><span class="hljs-comment"># 创建<hanla></hanla>DQN<hanla></hanla>网络实例<hanla></hanla></span><br>network = DQN()<br><span class="hljs-comment"># 创建<hanla></hanla>DQN<hanla></hanla>智能体实例，传入环境的状态空间和动作空间维度<hanla></hanla></span><br>agent = DQNAgent(env.observation_space.shape[<span class="hljs-number">0</span>], env.action_space.n)<br></code></pre></td></tr></tbody></table></figure>
<h1 id="network-compression">Network Compression</h1>
<h2 id="network-pruning">Network Pruning</h2>
<blockquote>
<p><strong>Network Pruning（网络剪枝）</strong>是指在不显著影响模型性能的前提下，通过去除网络中冗余的参数或结构，使模型更小、更快、更高效的过程。</p>
</blockquote>
<ul>
<li>Importance of a weight: absolute values, life long …</li>
<li>Importance of a neuron: the number of times it wasn’t zero on a given data set ……</li>
<li>After pruning, the accuracy will drop (hopefully not too much)</li>
<li>Fine tuning on training data for recover</li>
<li>Don’t prune too much at once, or the network won’t recover.</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250729191630989.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="weight-pruning">Weight pruning</h3>
<blockquote>
<p>在保持模型结构不变的前提下，移除（置零）神经网络中不重要的权重。</p>
</blockquote>
<p>The network architecture becomes irregular.</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250729191949540.png" srcset="/img/loading.gif" lazyload></p>
<p>核心思想：</p>
<ul>
<li>对每一层的参数进行评估</li>
<li>将<strong>权重绝对值小</strong>的参数视为<hanla></hanla>“不重要”</li>
<li>将这些参数设为 0（剪除）</li>
<li>通常保留一定的 <strong>sparsity ratio（稀疏率）</strong>，如剪掉 30% 的参数</li>
</ul>
<p>To Learn more: https://arxiv.org/pdf/1608.03665.pdf</p>
<h3 id="neuron-pruning">Neuron pruning</h3>
<blockquote>
<p>剪除神经网络中<hanla></hanla>“整个神经元”<hanla></hanla>的操作。</p>
</blockquote>
<p>在 MLP 或 CNN 网络中，每一个神经元都对应一个 <strong>输出维度</strong> 或 <strong>通道（channel）</strong>，因此剪除神经元也就是剪除：</p>
<ul>
<li>对于 <strong>全连接层（FC）</strong>：剪除一整行权重（对应一个神经元）</li>
<li>对于 <strong>卷积层（Conv）</strong>：剪除一个输出通道（即一个 filter）</li>
</ul>
<p>The network architecture is regular.</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250729192558133.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="why-pruning">Why Pruning?</h3>
<blockquote>
<p><strong>大网络更容易训练、更容易获得好的初始表示能力</strong>，而剪枝是在此基础上<strong>精简冗余参数</strong>的过程。</p>
</blockquote>
<ol type="1">
<li><strong>大网络具有更强的表示能力和学习能力</strong></li>
</ol>
<ul>
<li>大网络的<strong>容量更大</strong>，可以更好地拟合复杂数据。</li>
<li>在训练过程中，网络会形成大量<hanla></hanla>“冗余结构”，以便更好地寻找最优解。</li>
<li>剪枝是在模型已经学会了任务之后，对这些冗余结构的<strong>精简过程</strong>。</li>
</ul>
<ol start="2" type="1">
<li><strong>小网络不一定容易训练好（optimization issue）</strong></li>
</ol>
<ul>
<li>训练小网络更容易陷入<strong>局部最优</strong>或优化困难。</li>
<li>剪枝相当于：<strong>先用大模型找到全局结构，再从中剔除不必要的部分</strong>。</li>
<li>从理论上讲，<strong>剪枝后的子网络可能更优于从头训练的小网络</strong>。</li>
</ul>
<ol start="3" type="1">
<li><strong>剪枝后的子网络 ≠ 简单的小网络</strong></li>
</ol>
<ul>
<li>剪枝后的模型虽然小，但它<strong>继承了原大模型的初始化<hanla></hanla>+<hanla></hanla>结构<hanla></hanla>+<hanla></hanla>训练经验</strong>。</li>
<li>简单从头训练的小网络可能<strong>找不到这种<hanla></hanla>“有利结构”</strong>。</li>
<li>剪枝得到的子网络有时被称为 <strong>“winning tickets”</strong>：在大模型中找到的可独立成功的小网络。</li>
</ul>
<h4 id="lottery-ticket-hypothesis">Lottery Ticket Hypothesis</h4>
<p>To Learn More:</p>
<ul>
<li>https://arxiv.org/abs/1803.03635</li>
<li>https://arxiv.org/abs/1905.01067</li>
<li>https://arxiv.org/abs/1906.04358</li>
</ul>
<p><strong>乐透票假说（Lottery Ticket Hypothesis, LTH）</strong>认为：</p>
<blockquote>
<p>对于一个随机初始化的大型神经网络，总存在一个<strong>子网络</strong>，它：</p>
<ul>
<li>使用原始网络中的一部分参数（即<hanla></hanla>“子网络”）</li>
<li>使用<strong>原始的初始化值</strong></li>
<li>可以在相同的训练步骤下达到和原始大网络<strong>相当甚至更好的性能</strong></li>
</ul>
</blockquote>
<p>这个<hanla></hanla>“子网络”<hanla></hanla>被称为一个： <strong>Lottery Ticket</strong></p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250729193440635.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250729193500662.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="rethinking-the-value-of-network-pruning">Rethinking the Value of Network Pruning</h4>
<p>To Learn More: https://arxiv.org/abs/1810.05270</p>
<blockquote>
<ol type="1">
<li>training a large, over-parameterized model is often <strong>not necessary</strong> to obtain an efficient final model,</li>
<li>learned “important” weights of the large model are typically <strong>not useful for the small pruned model</strong>,</li>
<li><strong>the pruned architecture itself</strong>, rather than a set of inherited “important” weights, is <strong>more crucial</strong> to the efficiency in the final model, which suggests that in some cases pruning can be useful as an architecture search paradigm.</li>
<li>We also compare with the “Lottery Ticket Hypothesis”, and find that with <strong>optimal learning rate</strong>, the “winning ticket” initialization <strong>does not bring improvement over random initialization</strong>.</li>
</ol>
</blockquote>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250729193818305.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>New random initialization, not original random initialization in “Lottery Ticket Hypothesis”</li>
<li>Limitation of “Lottery Ticket Hypothesis” (small lr , unstructured)</li>
</ul>
<blockquote>
<p>本文观点：剪枝过程中<hanla></hanla>“得到的网络结构”<hanla></hanla>远比<hanla></hanla>“保留的原始权重”<hanla></hanla>重要。</p>
</blockquote>
<p>论文提出提出了一个替代策略：</p>
<p>Train small network <strong>with architecture inherited from pruning</strong>：</p>
<ol type="1">
<li>先训练大网络 + 剪枝</li>
<li>获取小网络的结构（即通道数等）</li>
<li>用相同结构、<strong>随机初始化</strong>重新训练 ➜ 表现同样好甚至更好</li>
</ol>
<h2 id="knowledge-distillation">Knowledge Distillation</h2>
<blockquote>
<p><strong>Knowledge Distillation（知识蒸馏）</strong> 是指将一个性能较强的 <strong>大模型（teacher）</strong> 的知识<hanla></hanla>“转移”<hanla></hanla>到一个 <strong>小模型（student）</strong> 上，从而提升小模型的性能。</p>
</blockquote>
<p>To Learn More:</p>
<ul>
<li>https://arxiv.org/pdf/1503.02531.pdf</li>
<li>https://arxiv.org/pdf/1312.6184.pdf</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250729212959703.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>Temperature for SoftMax</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250729213234364.png" srcset="/img/loading.gif" lazyload></p>
<p>使用温度参数<hanla></hanla><span class="math inline"><em>T</em></span><hanla></hanla>控制 softmax 的平滑程度：<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.525ex;" xmlns="http://www.w3.org/2000/svg" width="17.243ex" height="3.925ex" role="img" focusable="false" viewBox="0 -1060.7 7621.4 1734.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="TeXAtom" transform="translate(536,530.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mo" transform="translate(1093,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><g data-mml-node="mi" transform="translate(536,-293.8) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(1911.7,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mfrac" transform="translate(2967.5,0)"><g data-mml-node="mrow" transform="translate(801.4,516.8) scale(0.707)"><g data-mml-node="mi"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(466,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(1038,0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mo" transform="translate(1541,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(1930,0)"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="mi" transform="translate(498,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(2722,0)"><g data-mml-node="mo"><path data-c="2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z"></path></g></g><g data-mml-node="mi" transform="translate(3222,0)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mo" transform="translate(3926,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><g data-mml-node="mrow" transform="translate(220,-370.3) scale(0.707)"><g data-mml-node="munder"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path></g><g data-mml-node="mi" transform="translate(1089,-285.4) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g><g data-mml-node="mi" transform="translate(1597,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(2063,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(2635,0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mo" transform="translate(3138,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(3527,0)"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="mi" transform="translate(498,-150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(4366.3,0)"><g data-mml-node="mo"><path data-c="2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z"></path></g></g><g data-mml-node="mi" transform="translate(4866.3,0)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mo" transform="translate(5570.3,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><rect width="4413.9" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container></span></p>
<ul>
<li><p>T=1：普通 softmax</p></li>
<li><p>T&gt;1：分布更平滑，便于学习类间关系</p></li>
</ul>
<blockquote>
<p>T<hanla></hanla>也不宜太大，过大会导致老师的输出概率基本都为零向量。</p>
</blockquote>
<h2 id="parameter-quantization">Parameter Quantization</h2>
<blockquote>
<p><strong>Parameter Quantization</strong> 是指将原本以高精度（如 32-bit 浮点数，<code>float32</code>）存储的模型参数，<strong>转换为低精度表示</strong>（如 16-bit、8-bit、甚至 1-bit）的一种模型压缩方法。</p>
</blockquote>
<ol type="1">
<li><p>Using less bits to represent a value</p></li>
<li><p>Weight clustering</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250729213637026.png" srcset="/img/loading.gif" lazyload></p></li>
<li><p>Represent frequent clusters by less bits, represent rare clusters by more bits</p>
<ul>
<li>e.g.&nbsp;Huffman encoding</li>
</ul></li>
</ol>
<h3 id="binary-weights">Binary Weights</h3>
<blockquote>
<p>Your weights are always +1 or -1</p>
</blockquote>
<p>To Learn More:</p>
<ul>
<li>Binary Connect:https://arxiv.org/abs/1511.00363</li>
<li>Binary Network:https://arxiv.org/abs/1602.02830</li>
<li>XNOR-net:https://arxiv.org/abs/1603.05279</li>
<li>https://arxiv.org/abs/1511.00363</li>
</ul>
<h4 id="binary-connect">Binary Connect</h4>
<table>
<thead>
<tr class="header">
<th>步骤</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1. 训练时仍保留实数权重</td>
<td>用来进行权重更新（储存信息）</td>
</tr>
<tr class="even">
<td>2. 前向和反向传播时，将权重二值化</td>
<td>将实数权重 <code>w</code> 转换为二值权重 <code>w_b ∈ {+1, -1}</code></td>
</tr>
<tr class="odd">
<td>3. 使用实数权重更新</td>
<td>使用 SGD 更新实数权重 <code>w</code>，然后再进行二值化</td>
</tr>
</tbody>
</table>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250729214145198.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="architecture-design">Architecture Design</h2>
<h3 id="depthwise-separable-convolution">Depthwise Separable Convolution</h3>
<h4 id="depthwise-convolution逐通道卷积">Depthwise Convolution(逐通道卷积)</h4>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250729214335964.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>Filter number = Input channel number</li>
<li>Each filter only considers one channel</li>
<li>The filters are <span class="math inline"><em>𝑘</em> × <em>𝑘</em></span> matrices</li>
<li>There is no interaction between channels.</li>
</ul>
<h4 id="pointwise-convolution逐点卷积">Pointwise Convolution(逐点卷积)</h4>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250729214447332.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<p>使用<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="4.023ex" height="1.507ex" role="img" focusable="false" viewBox="0 -666 1778 666"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mi" transform="translate(500,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mn" transform="translate(1278,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></svg></mjx-container>卷积融合通道</p>
</blockquote>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250729214639798.png" srcset="/img/loading.gif" lazyload></p>
<p>To learn more:</p>
<ul>
<li>SqueezeNet: https://arxiv.org/abs/1602.07360</li>
<li>MobileNet: https://arxiv.org/abs/1704.04861</li>
<li>ShuffleNet: https://arxiv.org/abs/1707.01083</li>
<li>Xception: https://arxiv.org/abs/1610.02357</li>
</ul>
<h2 id="dynamic-computation">Dynamic Computation</h2>
<p>https://arxiv.org/abs/1703.09844</p>
<p>传统神经网络对所有输入都执行<strong>固定的计算图</strong>，即每个输入都经历相同的层、计算量和参数。</p>
<p>而<hanla></hanla>Dynamic Computation<hanla></hanla>则允许模型根据输入或当前中间状态，动态地：</p>
<ul>
<li>调整需要执行的<strong>层或模块</strong>；</li>
<li>激活<hanla></hanla>/<hanla></hanla>跳过部分<strong>神经元或通道</strong>；</li>
<li>改变模型的<strong>精度</strong>或<strong>宽度<hanla></hanla>/<hanla></hanla>深度</strong>；</li>
<li>在推理过程中<strong>提前退出计算（early exiting）</strong>。</li>
</ul>
<blockquote>
<p>简单来说，不再<hanla></hanla>“一个模型对所有输入一视同仁”，而是<hanla></hanla>“对不同输入量身定制计算量”。</p>
</blockquote>
<ol type="1">
<li>Train multiple classifiers</li>
<li>Classifiers at the intermedia layer</li>
</ol>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250729215043120.png" srcset="/img/loading.gif" lazyload></p>
<p><strong>在主模型的多个中间层处接入独立的分类器</strong>，使模型能够根据当前层的输出判断是否已经足够自信，可以<hanla></hanla>“提前输出预测”<hanla></hanla>而不继续向下计算。</p>
<p>Training Methods:</p>
<ul>
<li>Joint Training: 将所有出口的损失加权求和，一起训练主干网络和各个中间分类器</li>
<li>Stage-wise Training: 先训练主网络，然后冻结主网络参数，只训练中间分类器</li>
</ul>
<h2 id="homework13-network-compression">Homework13: Network Compression</h2>
<h3 id="task-description-2">Task Description</h3>
<ul>
<li>Network Compression: Use a small model to simulate the prediction/accuracy of the large model.</li>
<li>In this task, you need to train a very small model to complete HW3, that is, do the classification on the food-11 dataset.</li>
</ul>
<h3 id="intro">Intro</h3>
<h4 id="knowledge-distillation-1">Knowledge Distillation</h4>
<ul>
<li>When training a small model, add some information from the large model (such as the probability distribution of the prediction) to help the small model learn better.</li>
<li>We have provided a well-trained network to help you do knowledge distillation (Acc ~= 0.855).</li>
<li>Please note that you can only use the pre-trained model we provide when writing homework.</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250730215809443.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="design-architecture">Design Architecture</h4>
<ul>
<li>Depthwise &amp; Pointwise Convolution Layer (Proposed in MobileNet)
<ul>
<li>You can consider the original convolution as a Dense/Linear Layer, but each line/each weight is a filter, and the original multiplication becomes a convolution operation. (input<em>weight → input </em> filter)</li>
<li>Depthwise: let each channel pass a respective filter first, and let every pixel pass the shared-weight Dense/Linear.</li>
<li>It is strongly recommended that you use similar techniques to design your model.(NMkk / Nkk+NM)</li>
</ul></li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250730215934521.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="baseline-guides-1">Baseline Guides</h3>
<ul>
<li>Simple Baseline (2pts, acc ≥ 0.59856, 2 hour)
<ul>
<li>Just run the code and submit answer.</li>
</ul></li>
<li>Medium Baseline (2 pts, acc ≥ 0.65412, 2 hours)
<ul>
<li>Complete the loss in knowledge distillation and control alpha &amp; T.</li>
</ul></li>
<li>Strong Baseline (1.5 pts, acc ≥ 0.72819, 4 hours)
<ul>
<li>Modify model architecture with depth- and point-wise convolution layer.
<ul>
<li>Or, you can take great ideas from MobileNet, ShuffleNet, DenseNet, SqueezeNet, GhostNet, etc.</li>
</ul></li>
<li>Any techniques and methods you learned in HW3 - CNN. For example, make data augmentation stronger, modify semi-supervised learning, etc.</li>
</ul></li>
<li>Boss Baseline (0.5 pts, acc ≥ 0.81003)
<ul>
<li>Make your teacher net more stronger.
<ul>
<li>If your teacher net is too strong, you can consider <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1902.03393">TAKD</a> techniques.</li>
</ul></li>
<li>Implement other advanced knowledge distillation.
<ul>
<li>For example, <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Deep_Mutual_Learning_CVPR_2018_paper.pdf">DML</a>, <a target="_blank" rel="noopener" href="http://cvlab.postech.ac.kr/research/RKD/">Relational KD</a>….</li>
</ul></li>
<li>If the number of the parameters of your model is slightly larger than the constraint (100, 000), you can use network pruning.</li>
<li>If you got confused of previous techniques, you can check out TA’s lesson in last year. (<a target="_blank" rel="noopener" href="https://slides.com/arvinliu/model-compression">slides</a>, <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=9CCn9uPfJ64&amp;ab_channel=Hung-yiLee">video</a>)</li>
</ul></li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250730225921346.png" srcset="/img/loading.gif" lazyload></p>
<p>作业提供的所有<hanla></hanla>Teacher Network<hanla></hanla>链接都已经失效了，所以我只修改了代码，并没有运行。</p>
<h3 id="simple-baseline-7">Simple Baseline</h3>
<p><a target="_blank" rel="noopener" href="https://github.com/Striver98/Machine-Learning-Homework/blob/master/HW13/hw13_Network_Compression.ipynb">Code Link</a></p>
<h3 id="medium-baseline-8">Medium Baseline</h3>
<p><a target="_blank" rel="noopener" href="https://github.com/Striver98/Machine-Learning-Homework/blob/master/HW13/hw13_medium.ipynb">Code Link</a></p>
<p>Complete the loss in knowledge distillation and control alpha &amp; T.</p>
<p><span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.697ex;" xmlns="http://www.w3.org/2000/svg" width="59.963ex" height="6ex" role="img" focusable="false" viewBox="0 -1460 26503.6 2652.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g><g data-mml-node="mi" transform="translate(889,0)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mo" transform="translate(1570,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1959,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(2710,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mo" transform="translate(2988,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(3266,0)"><path data-c="1D444" d="M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z"></path></g><g data-mml-node="mo" transform="translate(4057,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(4723.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="munder" transform="translate(5779.6,0)"><g data-mml-node="mo"><path data-c="2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z"></path></g><g data-mml-node="mi" transform="translate(600,-1084.4) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mi" transform="translate(7390.2,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(8141.2,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(8530.2,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(8875.2,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mi" transform="translate(9430.9,0)"><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(278,0)"></path><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(778,0)"></path></g><g data-mml-node="mo" transform="translate(10708.9,0)"><path data-c="2061" d=""></path></g><g data-mml-node="mfrac" transform="translate(10875.6,0)"><g data-mml-node="mrow" transform="translate(240,710)"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1140,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(1485,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><g data-mml-node="mrow" transform="translate(220,-710)"><g data-mml-node="mi"><path data-c="1D444" d="M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z"></path></g><g data-mml-node="mo" transform="translate(791,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1180,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(1525,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><rect width="2114" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(13507.3,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="munder" transform="translate(14563.1,0)"><g data-mml-node="mo"><path data-c="2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z"></path></g><g data-mml-node="mi" transform="translate(600,-1084.4) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mi" transform="translate(16173.8,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(16924.8,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(17313.8,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(17658.8,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(18047.8,0)"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path></g><g data-mml-node="mi" transform="translate(18325.8,0)"><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(278,0)"></path><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(778,0)"></path></g><g data-mml-node="mo" transform="translate(19603.8,0)"><path data-c="2061" d=""></path></g><g data-mml-node="mi" transform="translate(19770.4,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(20521.4,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(20910.4,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(21255.4,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(21866.7,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(22866.9,0)"><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(278,0)"></path><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(778,0)"></path></g><g data-mml-node="mo" transform="translate(24144.9,0)"><path data-c="2061" d=""></path></g><g data-mml-node="mi" transform="translate(24311.6,0)"><path data-c="1D444" d="M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z"></path></g><g data-mml-node="mo" transform="translate(25102.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(25491.6,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(25836.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(26225.6,0)"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path></g></g></g></svg></mjx-container></span></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">loss_fn_kd</span>(<span class="hljs-params">outputs, labels, teacher_outputs, alpha=<span class="hljs-number">0.7</span>, T=<span class="hljs-number">4</span></span>):<br>    hard_loss = F.cross_entropy(outputs, labels) * (<span class="hljs-number">1.</span> - alpha)    <br>    <span class="hljs-comment"># ---------- TODO ----------</span><br>    <span class="hljs-comment"># Complete soft loss in knowledge distillation</span><br>    teacher_soft = F.softmax(teacher_outputs / T, dim=<span class="hljs-number">1</span>)<br>    student_log_soft = F.log_softmax(outputs / T, dim=<span class="hljs-number">1</span>)   <br>    kl_loss = F.kl_div(student_log_soft, teacher_soft, reduction=<span class="hljs-string">'batchmean'</span>)<br>    soft_loss = alpha * (T ** <span class="hljs-number">2</span>) * kl_loss<br>    <span class="hljs-keyword">return</span> hard_loss + soft_loss<br></code></pre></td></tr></tbody></table></figure>
<h3 id="strong-baseline-7">Strong baseline</h3>
<p><a target="_blank" rel="noopener" href="https://github.com/Striver98/Machine-Learning-Homework/blob/master/HW13/hw13_strong.ipynb">Code Link</a></p>
<p>Modify model architecture with depth- and point-wise convolution layer.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 定义深度可分离卷积块<hanla></hanla></span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">depthwise_separable_conv</span>(<span class="hljs-params">in_channels, out_channels, stride=<span class="hljs-number">1</span></span>):<br>    <span class="hljs-keyword">return</span> nn.Sequential(<br>        <span class="hljs-comment"># 深度卷积：每个输入通道独立进行卷积，groups=in_channels</span><br>        <span class="hljs-comment"># 参数量：in_channels * kernel_size^2 = in_channels * 9</span><br>        nn.Conv2d(in_channels, in_channels, kernel_size=<span class="hljs-number">3</span>, stride=stride, <br>                 padding=<span class="hljs-number">1</span>, groups=in_channels, bias=<span class="hljs-literal">False</span>),<br>        nn.BatchNorm2d(in_channels),<br>        nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>        <br>        <span class="hljs-comment"># 逐点卷积：使用<hanla></hanla>1x1<hanla></hanla>卷积混合通道信息</span><br>        <span class="hljs-comment"># 参数量：in_channels * out_channels * 1 = in_channels * out_channels</span><br>        nn.Conv2d(in_channels, out_channels, kernel_size=<span class="hljs-number">1</span>, stride=<span class="hljs-number">1</span>, <br>                 padding=<span class="hljs-number">0</span>, bias=<span class="hljs-literal">False</span>),<br>        nn.BatchNorm2d(out_channels),<br>        nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>    )<br><br><span class="hljs-comment"># 定义学生网络类，继承自<hanla></hanla>nn.Module</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">StudentNet</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>      <span class="hljs-built_in">super</span>(StudentNet, <span class="hljs-variable language_">self</span>).__init__()<br><br>      <span class="hljs-comment"># 参考<hanla></hanla>MobileNet<hanla></hanla>的设计思想</span><br>      <span class="hljs-variable language_">self</span>.cnn = nn.Sequential(<br>        nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">16</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>),<br>        nn.BatchNorm2d(<span class="hljs-number">16</span>),<br>        nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>        <br>        depthwise_separable_conv(<span class="hljs-number">16</span>, <span class="hljs-number">32</span>, stride=<span class="hljs-number">1</span>),<br>        <br>        depthwise_separable_conv(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, stride=<span class="hljs-number">2</span>),<br>        <br>        depthwise_separable_conv(<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, stride=<span class="hljs-number">1</span>),<br>        <br>        depthwise_separable_conv(<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, stride=<span class="hljs-number">2</span>),<br>        <br>        depthwise_separable_conv(<span class="hljs-number">128</span>, <span class="hljs-number">128</span>, stride=<span class="hljs-number">1</span>),<br>        <br>        depthwise_separable_conv(<span class="hljs-number">128</span>, <span class="hljs-number">256</span>, stride=<span class="hljs-number">2</span>),<br>        <br>        nn.AdaptiveAvgPool2d((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)),<br>        <br>        nn.Dropout(<span class="hljs-number">0.2</span>),<br>      )<br>      <br>      <span class="hljs-variable language_">self</span>.fc = nn.Sequential(<br>        nn.Linear(<span class="hljs-number">256</span>, <span class="hljs-number">11</span>),<br>      )<br>      <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>      out = <span class="hljs-variable language_">self</span>.cnn(x)<br>      out = out.view(out.size()[<span class="hljs-number">0</span>], -<span class="hljs-number">1</span>)<br>      <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.fc(out)<br></code></pre></td></tr></tbody></table></figure>
<h1 id="life-long-learning">Life Long Learning</h1>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250801155206141.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<p>Lifelong Learning（终身学习，也称为持续学习、增量学习）是指模型在训练过程中<strong>持续接收新任务或新知识</strong>，而<strong>无需访问原始旧任务的数据</strong>，同时尽可能<strong>保留和整合先前学到的知识</strong>。</p>
</blockquote>
<p>Goal: A model can beat all task</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250801155232858.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="catastrophic-forgetting">Catastrophic Forgetting</h2>
<p>Catastrophic Forgetting 指的是神经网络在学习新任务时，<strong>对先前已学任务的性能急剧下降</strong>，即新知识的学习<strong>干扰并覆盖了旧知识</strong>。</p>
<p><strong>本质原因：</strong>神经网络通常通过梯度下降全局更新参数，当在不访问旧任务数据的前提下训练新任务时，参数调整可能会破坏对旧任务有用的表征。</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250801155619845.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="evaluation-1">Evaluation</h2>
<p>First of all, we need a sequence of tasks.</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250801155716042.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>Accuracy= <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.781ex;" xmlns="http://www.w3.org/2000/svg" width="12.077ex" height="2.943ex" role="img" focusable="false" viewBox="0 -955.8 5338.1 1300.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mfrac"><g data-mml-node="mn" transform="translate(292.1,394) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mi" transform="translate(220,-345) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><rect width="697.8" height="60" x="120" y="220"></rect></g><g data-mml-node="munderover" transform="translate(1104.5,0)"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path></g><g data-mml-node="mi" transform="translate(1089,477.1) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="TeXAtom" transform="translate(1089,-285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1123,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="msub" transform="translate(3557.8,0)"><g data-mml-node="mi"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path></g><g data-mml-node="TeXAtom" transform="translate(792,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mo" transform="translate(704,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(982,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></g></svg></mjx-container></span>
<ul>
<li>这是<strong>在所有任务学习完之后</strong>，模型在每个任务上的平均测试准确率。衡量模型在完成所有任务后，是否仍然<strong>保留对旧任务的知识</strong>。</li>
</ul></li>
<li>Backward Transfer= <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.912ex;" xmlns="http://www.w3.org/2000/svg" width="20.915ex" height="3.074ex" role="img" focusable="false" viewBox="0 -955.8 9244.6 1358.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mfrac"><g data-mml-node="mn" transform="translate(744,394) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mrow" transform="translate(220,-345) scale(0.707)"><g data-mml-node="mi"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mo" transform="translate(704,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(1482,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><rect width="1601.5" height="60" x="120" y="220"></rect></g><g data-mml-node="munderover" transform="translate(2008.2,0)"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path></g><g data-mml-node="TeXAtom" transform="translate(1089,477.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mo" transform="translate(704,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(1482,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(1089,-285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1123,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="msub" transform="translate(4715.3,0)"><g data-mml-node="mi"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path></g><g data-mml-node="TeXAtom" transform="translate(792,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mo" transform="translate(704,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(982,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g><g data-mml-node="mo" transform="translate(6717.9,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="msub" transform="translate(7718.1,0)"><g data-mml-node="mi"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path></g><g data-mml-node="TeXAtom" transform="translate(792,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(623,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></g></svg></mjx-container></span>
<ul>
<li>衡量模型在学习新任务后，对<strong>旧任务表现变化</strong>的平均值。</li>
<li><span class="math inline"><em>R</em><sub><em>T</em>, <em>i</em></sub></span>: 学习完所有任务后，在第 <span class="math inline"><em>i</em></span> 个任务上的准确率（最终表现）。</li>
<li><span class="math inline"><em>R</em><sub><em>i</em>, <em>i</em></sub></span>: 刚学完任务<hanla></hanla><span class="math inline"><em>i</em></span><hanla></hanla>时，在该任务上的准确率（即时表现）。</li>
</ul></li>
<li>Forward Transfer= <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.912ex;" xmlns="http://www.w3.org/2000/svg" width="22.059ex" height="3.074ex" role="img" focusable="false" viewBox="0 -955.8 9750.1 1358.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mfrac"><g data-mml-node="mn" transform="translate(744,394) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mrow" transform="translate(220,-345) scale(0.707)"><g data-mml-node="mi"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mo" transform="translate(704,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(1482,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><rect width="1601.5" height="60" x="120" y="220"></rect></g><g data-mml-node="munderover" transform="translate(2008.2,0)"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path></g><g data-mml-node="mi" transform="translate(1089,477.1) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="TeXAtom" transform="translate(1089,-285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1123,0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g><g data-mml-node="msub" transform="translate(4461.5,0)"><g data-mml-node="mi"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path></g><g data-mml-node="TeXAtom" transform="translate(792,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(1123,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(1623,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(1901,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g><g data-mml-node="mo" transform="translate(7113.8,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="msub" transform="translate(8114.1,0)"><g data-mml-node="mi"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path></g><g data-mml-node="TeXAtom" transform="translate(792,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g><g data-mml-node="mo" transform="translate(500,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(778,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></g></svg></mjx-container></span>
<ul>
<li>衡量模型在尚未学习任务 <span class="math inline"><em>i</em></span> 前（即只学到任务 <span class="math inline"><em>i</em> − 1</span>）在任务 <span class="math inline"><em>i</em></span> 上的表现 <strong>相对于初始水平的提升</strong>。</li>
<li><span class="math inline"><em>R</em><sub><em>i</em> − 1, <em>i</em></sub></span>: 模型在完成任务 <span class="math inline"><em>i</em> − 1</span> 后，在任务 <span class="math inline"><em>i</em></span> 上的准确率（虽然还没学过）。</li>
<li><span class="math inline"><em>R</em><sub>0, <em>i</em></sub></span>: 模型在任务 <span class="math inline"><em>i</em></span> 尚未学习前的准确率（通常为随机初始化状态或预训练状态下的结果）。</li>
</ul></li>
</ul>
<blockquote>
<p><span class="math inline"><em>R</em><sub><em>i</em>, <em>j</em></sub></span>: : after training task i , performance on task j</p>
<ul>
<li>if i&gt;j: After training task i , does task j be forgot</li>
<li>if i&lt;j: Can we transfer the skill of task i to task j</li>
</ul>
</blockquote>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250801160918216.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="research-directions">Research Directions</h2>
<h3 id="selective-synaptic-plasticity">Selective Synaptic Plasticity</h3>
<p><strong>Basic Idea</strong>: Some parameters in the model are important to the previous tasks. Only change the unimportant parameters.</p>
<p><span class="math inline"><em>θ</em><sup><em>b</em></sup></span> is the model learned from the previous tasks.</p>
<p>Each parameter <span class="math inline"><em>θ</em><sub><em>i</em></sub><sup><em>b</em></sup></span> has a “guard” <span class="math inline"><em>b</em><sub><em>i</em></sub></span></p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250801161200199.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li><p>If <span class="math inline"><em>b</em><sub><em>i</em></sub> = 0</span>, there is no constraint on <span class="math inline"><em>θ</em><sub><em>i</em></sub></span>: Catastrophic Forgetting</p></li>
<li><p>If <span class="math inline"><em>b</em><sub><em>i</em></sub> = ∞</span>, <span class="math inline"><em>θ</em><sub><em>i</em></sub></span> would always be equal to <span class="math inline"><em>θ</em><sub><em>i</em></sub><sup><em>b</em></sup></span>: Intransigence(固执)</p></li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250801161516942.png" srcset="/img/loading.gif" lazyload></p>
<p>To Learn More:</p>
<ul>
<li>Elastic Weight Consolidation (EWC): https://arxiv.org/abs/1612.00796</li>
<li>Synaptic Intelligence (SI): https://arxiv.org/abs/1703.04200</li>
<li>Memory Aware Synapses (MAS): https://arxiv.org/abs/1711.09601</li>
<li>RWalk: https://arxiv.org/abs/1801.10112</li>
<li>Sliced Cramer Preservation (SCP): https://openreview.net/forum?id=BJge3TNKwH</li>
<li>Gradient Episodic Memory (GEM): https://arxiv.org/abs/1706.08840</li>
</ul>
<h3 id="additional-neural-resource-allocation">Additional Neural Resource Allocation</h3>
<h4 id="progressive-neural-networks">Progressive Neural Networks</h4>
<p>https://arxiv.org/abs/1606.04671</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250801161738725.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="packnet">PackNet</h4>
<p>https://arxiv.org/abs/1711.05769</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250801161806135.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="compacting-picking-and-growing-cpg">Compacting, Picking, and Growing (CPG)</h4>
<p>https://arxiv.org/abs/1910.06562</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250801161826579.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="memory-reply">Memory Reply</h3>
<h4 id="generating-data">Generating Data</h4>
<p>https://arxiv.org/abs/1705.08690 https://arxiv.org/abs/1711.10563 https://arxiv.org/abs/1909.03329</p>
<ul>
<li>Generating pseudo data using generative model for previous tasks</li>
</ul>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250801161943168.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="adding-new-classes">Adding new classes</h4>
<ul>
<li><p>Learning without forgetting (LwF): https://arxiv.org/abs/1606.09282</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250801162023513.png" srcset="/img/loading.gif" lazyload></p></li>
<li><p>iCaRL: Incremental Classifier and Representation Learning: https://arxiv.org/abs/1611.07725</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250801162052752.png" srcset="/img/loading.gif" lazyload></p></li>
</ul>
<h4 id="three-scenarios-for-continual-learning">Three scenarios for continual learning</h4>
<p>https://arxiv.org/abs/1904.07734</p>
<h2 id="homework-14-lifelong-learning">Homework 14: Lifelong Learning</h2>
<blockquote>
<p>本次作业是<hanla></hanla>NTU COOL<hanla></hanla>上的选择题，非台大学生无法看到题目。</p>
</blockquote>
<h3 id="introduction-2">Introduction</h3>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250801162229005.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="dataset-2">Dataset</h3>
<p>Permuted MNIST</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250801162258168.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="sample-code">Sample Code</h3>
<p><a target="_blank" rel="noopener" href="https://colab.research.google.com/drive/1cALjoyJcoKtYPGUT2c_oeC5IFkUx5O80?usp=sharing">Colab Link</a></p>
<h4 id="guideline">Guideline</h4>
<ul>
<li>Utility
<ul>
<li>Permutation</li>
<li>Dataloader and Training Argument</li>
<li>Model</li>
<li>train</li>
<li>evaluate</li>
<li>evaluate metric</li>
</ul></li>
<li>Visualization</li>
<li>Methods
<ul>
<li>Baseline</li>
<li>EWC</li>
<li>MAS</li>
<li>SI</li>
<li>RWalk</li>
<li>SCP</li>
</ul></li>
</ul>
<h4 id="baseline-1">Baseline</h4>
<p>baseline<hanla></hanla>类实现了一个<hanla></hanla><strong>“什么都不做”<hanla></hanla>的终身学习算法</strong>，作为其他复杂算法（EWC、MAS<hanla></hanla>等）的性能基准。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">baseline</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-string">"""</span><br><span class="hljs-string">    baseline technique: do nothing in regularization term [initialize and all weight is zero]</span><br><span class="hljs-string">    """</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model, dataloaders, device</span>):<br>    <br>        <span class="hljs-variable language_">self</span>.model = model<br>        <span class="hljs-variable language_">self</span>.dataloaders = dataloaders<br>        <span class="hljs-variable language_">self</span>.device = device<br><br>        <span class="hljs-variable language_">self</span>.params = {n: p <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.model.named_parameters() <span class="hljs-keyword">if</span> p.requires_grad} <span class="hljs-comment">#extract all parameters in models</span><br>        <span class="hljs-variable language_">self</span>.p_old = {} <span class="hljs-comment"># store current parameters</span><br>        <span class="hljs-variable language_">self</span>._precision_matrices = <span class="hljs-variable language_">self</span>._calculate_importance() <span class="hljs-comment"># generate weight matrix </span><br><br>        <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.params.items():<br>            <span class="hljs-variable language_">self</span>.p_old[n] = p.clone().detach() <span class="hljs-comment"># keep the old parameter in self.p_old</span><br>  <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_calculate_importance</span>(<span class="hljs-params">self</span>):<br>        precision_matrices = {}<br>        <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.params.items(): <span class="hljs-comment"># initialize weight matrix（fill zero）</span><br>            precision_matrices[n] = p.clone().detach().fill_(<span class="hljs-number">0</span>)<br><br>        <span class="hljs-keyword">return</span> precision_matrices<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">penalty</span>(<span class="hljs-params">self, model: nn.Module</span>):<br>        loss = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> model.named_parameters():<br>            _loss = <span class="hljs-variable language_">self</span>._precision_matrices[n] * (p - <span class="hljs-variable language_">self</span>.p_old[n]) ** <span class="hljs-number">2</span><br>            loss += _loss.<span class="hljs-built_in">sum</span>()<br>        <span class="hljs-keyword">return</span> loss<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">update</span>(<span class="hljs-params">self, model</span>):<br>        <span class="hljs-comment"># do nothing</span><br>        <span class="hljs-keyword">return</span> <br></code></pre></td></tr></tbody></table></figure>
<h4 id="ewc---elastic-weight-consolidation">EWC - Elastic Weight Consolidation</h4>
<p>EWC<hanla></hanla>通过<hanla></hanla><strong>Fisher<hanla></hanla>信息矩阵</strong>来衡量每个参数对旧任务的重要性，并在学习新任务时保护重要参数不被大幅修改。</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250801164818923.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ewc</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-string">"""</span><br><span class="hljs-string">    @article{kirkpatrick2017overcoming,</span><br><span class="hljs-string">        title={Overcoming catastrophic forgetting in neural networks},  </span><br><span class="hljs-string">        author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and others},</span><br><span class="hljs-string">        journal={Proceedings of the national academy of sciences},</span><br><span class="hljs-string">        year={2017},</span><br><span class="hljs-string">        url={https://arxiv.org/abs/1612.00796}</span><br><span class="hljs-string">    }</span><br><span class="hljs-string">    """</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model, dataloaders, device</span>):<br>        <span class="hljs-variable language_">self</span>.model = model<br>        <span class="hljs-variable language_">self</span>.dataloaders = dataloaders<br>        <span class="hljs-variable language_">self</span>.device = device<br><br>        <span class="hljs-variable language_">self</span>.params = {n: p <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.model.named_parameters() <span class="hljs-keyword">if</span> p.requires_grad}<br>        <span class="hljs-variable language_">self</span>.p_old = {}<br>        <span class="hljs-variable language_">self</span>._precision_matrices = <span class="hljs-variable language_">self</span>._calculate_importance()<br><br>        <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.params.items():<br>            <span class="hljs-variable language_">self</span>.p_old[n] = p.clone().detach()<br>  <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_calculate_importance</span>(<span class="hljs-params">self</span>):<br>        precision_matrices = {}<br>        <span class="hljs-comment"># 为每个参数创建对应的<hanla></hanla>Fisher<hanla></hanla>矩阵，初始化为零</span><br>        <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.params.items(): <br>            precision_matrices[n] = p.clone().detach().fill_(<span class="hljs-number">0</span>)<br><br>        <span class="hljs-variable language_">self</span>.model.<span class="hljs-built_in">eval</span>()<br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.dataloaders[<span class="hljs-number">0</span>] <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            dataloader_num = <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.dataloaders)<br>            number_data = <span class="hljs-built_in">sum</span>([<span class="hljs-built_in">len</span>(loader) <span class="hljs-keyword">for</span> loader <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.dataloaders])<br>            <span class="hljs-keyword">for</span> dataloader <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.dataloaders:<br>                <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataloader:<br>                    <span class="hljs-variable language_">self</span>.model.zero_grad()<br>                    <span class="hljs-built_in">input</span> = data[<span class="hljs-number">0</span>].to(<span class="hljs-variable language_">self</span>.device)<br>                    output = <span class="hljs-variable language_">self</span>.model(<span class="hljs-built_in">input</span>)<br>                    label = data[<span class="hljs-number">1</span>].to(<span class="hljs-variable language_">self</span>.device)<br>                    <br>                    <span class="hljs-comment">############################################################################</span><br>                    <span class="hljs-comment">#####                     generate Fisher(F) matrix for EWC            #####</span><br>                    <span class="hljs-comment">############################################################################    </span><br>                    <span class="hljs-comment"># 计算负对数似然损失：F.log_softmax()<hanla></hanla>计算对数概率，然后计算负对数似然</span><br>                    loss = F.nll_loss(F.log_softmax(output, dim=<span class="hljs-number">1</span>), label)             <br>                    <span class="hljs-comment"># 对损失进行反向传播，计算每个参数的梯度</span><br>                    loss.backward()                                                    <br>                    <span class="hljs-comment">############################################################################</span><br><br>                    <span class="hljs-comment"># 遍历模型的所有参数</span><br>                    <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.model.named_parameters():<br>                        <span class="hljs-comment"># 将参数梯度的平方累加到<hanla></hanla>Fisher<hanla></hanla>矩阵中，并除以总样本数进行平均</span><br>                        <span class="hljs-comment"># p.grad.data ** 2 是<hanla></hanla>Fisher<hanla></hanla>信息矩阵的核心：梯度平方的期望</span><br>                        precision_matrices[n].data += p.grad.data ** <span class="hljs-number">2</span> / number_data   <br>                                                                            <br>            precision_matrices = {n: p <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> precision_matrices.items()}<br><br>        <span class="hljs-keyword">return</span> precision_matrices<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">penalty</span>(<span class="hljs-params">self, model: nn.Module</span>):<br>        loss = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> model.named_parameters():<br>            <span class="hljs-comment"># 计算<hanla></hanla>EWC<hanla></hanla>正则化项：Fisher<hanla></hanla>矩阵 × (当前参数 - 旧参数)²</span><br>            <span class="hljs-comment"># self._precision_matrices[n]：Fisher<hanla></hanla>信息矩阵（参数重要性权重）</span><br>            <span class="hljs-comment"># (p - self.p_old[n]) ** 2：参数变化量的平方</span><br>            _loss = <span class="hljs-variable language_">self</span>._precision_matrices[n] * (p - <span class="hljs-variable language_">self</span>.p_old[n]) ** <span class="hljs-number">2</span><br>            loss += _loss.<span class="hljs-built_in">sum</span>()<br>        <span class="hljs-keyword">return</span> loss<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">update</span>(<span class="hljs-params">self, model</span>):<br>        <span class="hljs-comment"># EWC<hanla></hanla>算法在训练过程中不需要更新任何内部状态</span><br>        <span class="hljs-comment"># Fisher<hanla></hanla>矩阵和旧参数只在任务切换时重新计算</span><br>        <span class="hljs-keyword">return</span> <br></code></pre></td></tr></tbody></table></figure>
<h4 id="mas---memory-aware-synapse">MAS - Memory Aware Synapse</h4>
<blockquote>
<p>trace the class mas and its calculate_importance function</p>
</blockquote>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250801164842698.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">mas</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-string">"""</span><br><span class="hljs-string">    @article{aljundi2017memory,</span><br><span class="hljs-string">      title={Memory Aware Synapses: Learning what (not) to forget},</span><br><span class="hljs-string">      author={Aljundi, Rahaf and Babiloni, Francesca and Elhoseiny, Mohamed and Rohrbach, Marcus and Tuytelaars, Tinne},</span><br><span class="hljs-string">      booktitle={ECCV},</span><br><span class="hljs-string">      year={2018},</span><br><span class="hljs-string">      url={https://eccv2018.org/openaccess/content_ECCV_2018/papers/Rahaf_Aljundi_Memory_Aware_Synapses_ECCV_2018_paper.pdf}</span><br><span class="hljs-string">    }</span><br><span class="hljs-string">    """</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model: nn.Module, dataloaders: <span class="hljs-built_in">list</span>, device</span>):<br>        <span class="hljs-variable language_">self</span>.model = model <br>        <span class="hljs-variable language_">self</span>.dataloaders = dataloaders<br>        <span class="hljs-variable language_">self</span>.params = {n: p <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.model.named_parameters() <span class="hljs-keyword">if</span> p.requires_grad}<br>        <span class="hljs-variable language_">self</span>.p_old = {}<br>        <span class="hljs-variable language_">self</span>.device = device<br>        <span class="hljs-variable language_">self</span>._precision_matrices = <span class="hljs-variable language_">self</span>.calculate_importance()<br>    <br>        <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.params.items():<br>            <span class="hljs-variable language_">self</span>.p_old[n] = p.clone().detach()<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">calculate_importance</span>(<span class="hljs-params">self</span>):<br>        precision_matrices = {}<br>        <span class="hljs-comment"># 为每个参数创建对应的<hanla></hanla>Omega<hanla></hanla>矩阵，初始化为零</span><br>        <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.params.items():<br>            precision_matrices[n] = p.clone().detach().fill_(<span class="hljs-number">0</span>)<br><br>        <span class="hljs-variable language_">self</span>.model.<span class="hljs-built_in">eval</span>()<br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.dataloaders[<span class="hljs-number">0</span>] <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            dataloader_num = <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.dataloaders)<br>            num_data = <span class="hljs-built_in">sum</span>([<span class="hljs-built_in">len</span>(loader) <span class="hljs-keyword">for</span> loader <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.dataloaders])<br>            <span class="hljs-keyword">for</span> dataloader <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.dataloaders:                <br>                <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataloader:<br>                    <span class="hljs-variable language_">self</span>.model.zero_grad()<br>                    output = <span class="hljs-variable language_">self</span>.model(data[<span class="hljs-number">0</span>].to(<span class="hljs-variable language_">self</span>.device))<br><br>                    <span class="hljs-comment">###########################################################################################################################################</span><br>                    <span class="hljs-comment">#####  TODO BLOCK: generate Omega(Ω) matrix for MAS. (Hint: square of l2 norm of output vector, then backward and take its gradients  #####</span><br>                    <span class="hljs-comment">###########################################################################################################################################</span><br>                    <span class="hljs-comment"># 将输出向量的每个元素平方（原地操作）</span><br>                    <span class="hljs-comment"># 这是计算<hanla></hanla>L2<hanla></hanla>范数平方的第一步：每个元素自乘</span><br>                    output.pow_(<span class="hljs-number">2</span>)                                                   <br>                    <span class="hljs-comment"># 对每个样本的输出向量求和，得到每个样本的<hanla></hanla>L2<hanla></hanla>范数平方</span><br>                    <span class="hljs-comment"># dim=1<hanla></hanla>表示沿着特征维度（类别维度）求和</span><br>                    loss = torch.<span class="hljs-built_in">sum</span>(output,dim=<span class="hljs-number">1</span>)                                   <br>                    <span class="hljs-comment"># 对批次中所有样本的<hanla></hanla>L2<hanla></hanla>范数平方取平均，得到标量损失</span><br>                    loss = loss.mean()   <br>                    <span class="hljs-comment"># 对损失进行反向传播，计算关于模型参数的梯度</span><br>                    <span class="hljs-comment"># 这里计算的是 ∂(||output||²)/∂θ</span><br>                    loss.backward() <br>                    <span class="hljs-comment">###########################################################################################################################################                          </span><br>                                            <br>                    <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.model.named_parameters():                      <br>                        <span class="hljs-comment"># 将参数梯度的绝对值累加到<hanla></hanla>Omega<hanla></hanla>矩阵中，并除以总样本数进行平均</span><br>                        <span class="hljs-comment"># 注意：这里使用<hanla></hanla>p.grad.abs()<hanla></hanla>而不是<hanla></hanla>p.grad.data ** 2（与<hanla></hanla>EWC<hanla></hanla>的区别）</span><br>                        precision_matrices[n].data += p.grad.<span class="hljs-built_in">abs</span>() / num_data<br>                        <br>        precision_matrices = {n: p <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> precision_matrices.items()}<br>        <span class="hljs-keyword">return</span> precision_matrices<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">penalty</span>(<span class="hljs-params">self, model: nn.Module</span>):<br>        loss = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> model.named_parameters():<br>            <span class="hljs-comment"># 计算<hanla></hanla>MAS<hanla></hanla>正则化项：Omega<hanla></hanla>矩阵 × (当前参数 - 旧参数)²</span><br>            <span class="hljs-comment"># self._precision_matrices[n]：Omega<hanla></hanla>重要性矩阵</span><br>            <span class="hljs-comment"># (p - self.p_old[n]) ** 2：参数变化量的平方</span><br>            _loss = <span class="hljs-variable language_">self</span>._precision_matrices[n] * (p - <span class="hljs-variable language_">self</span>.p_old[n]) ** <span class="hljs-number">2</span><br>            loss += _loss.<span class="hljs-built_in">sum</span>()<br>        <span class="hljs-keyword">return</span> loss<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">update</span>(<span class="hljs-params">self, model</span>):<br>        <span class="hljs-comment"># MAS<hanla></hanla>算法在训练过程中不需要更新任何内部状态</span><br>        <span class="hljs-comment"># Omega<hanla></hanla>矩阵和旧参数只在任务切换时重新计算</span><br>        <span class="hljs-keyword">return</span> <br></code></pre></td></tr></tbody></table></figure>
<h4 id="si---synaptic-intelligence">SI - Synaptic Intelligence</h4>
<blockquote>
<p>Accumulated loss change in each update step</p>
</blockquote>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250801165432065.png" srcset="/img/loading.gif" lazyload style="zoom:50%;"></p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250801165449902.png" srcset="/img/loading.gif" lazyload style="zoom:50%;"></p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250801165521444.png" srcset="/img/loading.gif" lazyload style="zoom:50%;"></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">si</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-string">"""</span><br><span class="hljs-string">    @article{kirkpatrick2017overcoming,</span><br><span class="hljs-string">        title={Overcoming catastrophic forgetting in neural networks},</span><br><span class="hljs-string">        author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and others},</span><br><span class="hljs-string">        journal={Proceedings of the national academy of sciences},</span><br><span class="hljs-string">        year={2017},</span><br><span class="hljs-string">        url={https://arxiv.org/abs/1612.00796}</span><br><span class="hljs-string">    }</span><br><span class="hljs-string">    """</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model, dataloaders, epsilon, device</span>):<br>        <span class="hljs-variable language_">self</span>.model = model<br>        <span class="hljs-variable language_">self</span>.dataloaders = dataloaders<br>        <span class="hljs-variable language_">self</span>.device = device<br>        <span class="hljs-comment"># 存储<hanla></hanla>epsilon<hanla></hanla>值，用于数值稳定性（防止除零）</span><br>        <span class="hljs-variable language_">self</span>.epsilon = epsilon<br>        <span class="hljs-variable language_">self</span>.params = {n: p <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.model.named_parameters() <span class="hljs-keyword">if</span> p.requires_grad}<br>        <span class="hljs-comment"># 调用方法计算重要性权重，返回前一任务参数值和<hanla></hanla>omega<hanla></hanla>重要性矩阵</span><br>        <span class="hljs-variable language_">self</span>._n_p_prev, <span class="hljs-variable language_">self</span>._n_omega = <span class="hljs-variable language_">self</span>._calculate_importance() <br>        <span class="hljs-comment"># 调用初始化方法，获取<hanla></hanla>W<hanla></hanla>矩阵（梯度累积）和旧参数值</span><br>        <span class="hljs-variable language_">self</span>.W, <span class="hljs-variable language_">self</span>.p_old = <span class="hljs-variable language_">self</span>._init_()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_init_</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># 初始化<hanla></hanla>W<hanla></hanla>矩阵字典（用于累积梯度信息）</span><br>        W = {}<br>        p_old = {}<br>        <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.model.named_parameters():<br>            <span class="hljs-comment"># 将参数名中的点替换为双下划线（避免属性访问冲突）</span><br>            n = n.replace(<span class="hljs-string">'.'</span>, <span class="hljs-string">'__'</span>)<br>            <span class="hljs-comment"># 只处理需要梯度的参数</span><br>            <span class="hljs-keyword">if</span> p.requires_grad:<br>                <span class="hljs-comment"># 初始化<hanla></hanla>W<hanla></hanla>矩阵为与参数同形状的零张量</span><br>                W[n] = p.data.clone().zero_()<br>                p_old[n] = p.data.clone()<br>        <span class="hljs-keyword">return</span> W, p_old<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_calculate_importance</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># 初始化前一任务参数字典</span><br>        n_p_prev = {}<br>        <span class="hljs-comment"># 初始化<hanla></hanla>omega<hanla></hanla>重要性权重字典</span><br>        n_omega = {}<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.dataloaders[<span class="hljs-number">0</span>] != <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.model.named_parameters():<br>                <span class="hljs-comment"># 将参数名中的点替换为双下划线</span><br>                n = n.replace(<span class="hljs-string">'.'</span>, <span class="hljs-string">'__'</span>)<br>                <span class="hljs-comment"># 只处理需要梯度的参数</span><br>                <span class="hljs-keyword">if</span> p.requires_grad:<br><br>                    <span class="hljs-comment"># 查找<hanla></hanla>/<hanla></hanla>计算参数的二次惩罚新值</span><br>                    <span class="hljs-comment"># 从模型中获取前一任务的参数值</span><br>                    p_prev = <span class="hljs-built_in">getattr</span>(<span class="hljs-variable language_">self</span>.model, <span class="hljs-string">'{}_SI_prev_task'</span>.<span class="hljs-built_in">format</span>(n))<br>                    <span class="hljs-comment"># 从模型中获取累积的<hanla></hanla>W<hanla></hanla>矩阵</span><br>                    W = <span class="hljs-built_in">getattr</span>(<span class="hljs-variable language_">self</span>.model, <span class="hljs-string">'{}_W'</span>.<span class="hljs-built_in">format</span>(n))<br>                    <span class="hljs-comment"># 获取当前参数值</span><br>                    p_current = p.detach().clone()<br>                    <span class="hljs-comment"># 计算参数变化量</span><br>                    p_change = p_current - p_prev<br>                    <span class="hljs-comment"># 计算<hanla></hanla>omega<hanla></hanla>增量：W / (参数变化平方 + epsilon)</span><br>                    <span class="hljs-comment"># 这是<hanla></hanla>SI<hanla></hanla>算法的核心公式，衡量参数重要性</span><br>                    omega_add = W/(p_change**<span class="hljs-number">2</span> + <span class="hljs-variable language_">self</span>.epsilon)<br>                    <span class="hljs-keyword">try</span>:<br>                        <span class="hljs-comment"># 尝试获取已存在的<hanla></hanla>omega<hanla></hanla>值</span><br>                        omega = <span class="hljs-built_in">getattr</span>(<span class="hljs-variable language_">self</span>.model, <span class="hljs-string">'{}_SI_omega'</span>.<span class="hljs-built_in">format</span>(n))<br>                    <span class="hljs-keyword">except</span> AttributeError:<br>                        <span class="hljs-comment"># 如果不存在，初始化为零张量</span><br>                        omega = p.detach().clone().zero_()<br>                    <span class="hljs-comment"># 累积<hanla></hanla>omega<hanla></hanla>值（新的重要性 = 旧重要性 + 新增重要性）</span><br>                    omega_new = omega + omega_add<br>                    <span class="hljs-comment"># 保存新的<hanla></hanla>omega<hanla></hanla>值</span><br>                    n_omega[n] = omega_new<br>                    <span class="hljs-comment"># 保存当前参数值作为新的前一任务参数</span><br>                    n_p_prev[n] = p_current<br><br>                    <span class="hljs-comment"># 将这些新值存储到模型中作为缓冲区</span><br>                    <span class="hljs-comment"># register_buffer<hanla></hanla>确保这些值会随模型一起保存<hanla></hanla>/<hanla></hanla>加载，但不参与梯度计算</span><br>                    <span class="hljs-variable language_">self</span>.model.register_buffer(<span class="hljs-string">'{}_SI_prev_task'</span>.<span class="hljs-built_in">format</span>(n), p_current)<br>                    <span class="hljs-variable language_">self</span>.model.register_buffer(<span class="hljs-string">'{}_SI_omega'</span>.<span class="hljs-built_in">format</span>(n), omega_new)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># 第一个任务的情况：初始化所有值</span><br>            <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.model.named_parameters():<br>                <span class="hljs-comment"># 将参数名中的点替换为双下划线</span><br>                n = n.replace(<span class="hljs-string">'.'</span>, <span class="hljs-string">'__'</span>)<br>                <span class="hljs-comment"># 只处理需要梯度的参数</span><br>                <span class="hljs-keyword">if</span> p.requires_grad:<br>                    <span class="hljs-comment"># 将当前参数值作为前一任务参数</span><br>                    n_p_prev[n] = p.detach().clone()<br>                    <span class="hljs-comment"># 初始化<hanla></hanla>omega<hanla></hanla>为零</span><br>                    n_omega[n] = p.detach().clone().zero_()<br>                    <span class="hljs-comment"># 在模型中注册前一任务参数缓冲区</span><br>                    <span class="hljs-variable language_">self</span>.model.register_buffer(<span class="hljs-string">'{}_SI_prev_task'</span>.<span class="hljs-built_in">format</span>(n), p.detach().clone())<br><br>        <span class="hljs-comment"># 返回前一任务参数和<hanla></hanla>omega<hanla></hanla>重要性权重字典</span><br>        <span class="hljs-keyword">return</span> n_p_prev, n_omega<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">penalty</span>(<span class="hljs-params">self, model: nn.Module</span>):<br>        <span class="hljs-comment"># 初始化正则化损失为<hanla></hanla>0</span><br>        loss = <span class="hljs-number">0.0</span><br>        <span class="hljs-comment"># 遍历模型的所有命名参数</span><br>        <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> model.named_parameters():<br>            n = n.replace(<span class="hljs-string">'.'</span>, <span class="hljs-string">'__'</span>)<br>            <span class="hljs-keyword">if</span> p.requires_grad:<br>                <span class="hljs-comment"># 获取前一任务的参数值</span><br>                prev_values = <span class="hljs-variable language_">self</span>._n_p_prev[n]<br>                <span class="hljs-comment"># 获取<hanla></hanla>omega<hanla></hanla>重要性权重</span><br>                omega = <span class="hljs-variable language_">self</span>._n_omega[n]<br>                <span class="hljs-comment"># 计算<hanla></hanla>SI<hanla></hanla>正则化项：omega × (当前参数 - 前一任务参数)²</span><br>                <span class="hljs-comment"># omega<hanla></hanla>值越大，表示参数越重要，变化时惩罚越大</span><br>                _loss = omega * (p - prev_values) ** <span class="hljs-number">2</span><br>                loss += _loss.<span class="hljs-built_in">sum</span>()<br>         <br>        <span class="hljs-keyword">return</span> loss<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">update</span>(<span class="hljs-params">self, model</span>):<br>        <span class="hljs-comment"># 在训练过程中更新<hanla></hanla>W<hanla></hanla>矩阵（累积梯度信息）</span><br>        <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> model.named_parameters():<br>            n = n.replace(<span class="hljs-string">'.'</span>, <span class="hljs-string">'__'</span>)<br>            <span class="hljs-keyword">if</span> p.requires_grad:<br>                <span class="hljs-keyword">if</span> p.grad <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                    <span class="hljs-comment"># 更新<hanla></hanla>W<hanla></hanla>矩阵：W = W - grad × (当前参数 - 旧参数)</span><br>                    <span class="hljs-comment"># 这累积了梯度和参数变化的乘积，用于后续计算重要性</span><br>                    <span class="hljs-variable language_">self</span>.W[n].add_(-p.grad * (p.detach() - <span class="hljs-variable language_">self</span>.p_old[n]))<br>                    <span class="hljs-comment"># 将更新后的<hanla></hanla>W<hanla></hanla>矩阵注册到模型中</span><br>                    <span class="hljs-variable language_">self</span>.model.register_buffer(<span class="hljs-string">'{}_W'</span>.<span class="hljs-built_in">format</span>(n), <span class="hljs-variable language_">self</span>.W[n])<br>                <span class="hljs-comment"># 更新旧参数值为当前参数值</span><br>                <span class="hljs-variable language_">self</span>.p_old[n] = p.detach().clone()<br>        <span class="hljs-keyword">return</span> <br></code></pre></td></tr></tbody></table></figure>
<h4 id="rwalk---remanian-walk">RWalk - Remanian Walk</h4>
<p>RWalk（Riemannian Walk）算法是对<hanla></hanla>SI<hanla></hanla>算法的改进，主要创新点包括：</p>
<ol type="1">
<li><p><strong>融合<hanla></hanla>Fisher<hanla></hanla>信息</strong>：在<hanla></hanla>SI<hanla></hanla>的<hanla></hanla>omega<hanla></hanla>计算中加入了<hanla></hanla>EWC<hanla></hanla>的<hanla></hanla>Fisher<hanla></hanla>矩阵信息</p></li>
<li><p><strong>修改的重要性计算</strong>：</p>
<figure class="highlight nix"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs nix"><span class="hljs-params">SI:</span>    ω <span class="hljs-operator">=</span> W <span class="hljs-symbol">/</span> (Δθ² <span class="hljs-operator">+</span> ε)<br><span class="hljs-params">RWalk:</span> ω <span class="hljs-operator">=</span> W <span class="hljs-symbol">/</span> (<span class="hljs-number">0.5</span> × F × Δθ² <span class="hljs-operator">+</span> ε)<br></code></pre></td></tr></tbody></table></figure>
<p>其中<hanla></hanla>F<hanla></hanla>是<hanla></hanla>Fisher<hanla></hanla>信息矩阵</p></li>
<li><p><strong>平滑更新策略</strong>：使用加权平均而非直接累加</p>
<figure class="highlight sqf"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sqf">SI:    ω<span class="hljs-variable">_new</span> = ω<span class="hljs-variable">_old</span> + ω<span class="hljs-variable">_add</span><br>RWalk: ω<span class="hljs-variable">_new</span> = <span class="hljs-number">0.5</span> × ω<span class="hljs-variable">_old</span> + <span class="hljs-number">0.5</span> × ω<span class="hljs-variable">_add</span><br></code></pre></td></tr></tbody></table></figure></li>
</ol>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">rwalk</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-string">"""</span><br><span class="hljs-string"></span><br><span class="hljs-string">    """</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model, dataloaders, epsilon, device</span>):<br>    <br>        <span class="hljs-variable language_">self</span>.model = model<br>        <span class="hljs-variable language_">self</span>.dataloaders = dataloaders<br>        <span class="hljs-variable language_">self</span>.device = device<br>        <span class="hljs-variable language_">self</span>.epsilon = epsilon<br>        <span class="hljs-variable language_">self</span>.update_ewc_parameter = <span class="hljs-number">0.4</span><br>        <span class="hljs-variable language_">self</span>.params = {n: p <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.model.named_parameters() <span class="hljs-keyword">if</span> p.requires_grad} <span class="hljs-comment"># extract model parameters and store in dictionary</span><br>        <span class="hljs-variable language_">self</span>._means = {} <span class="hljs-comment"># initialize the guidance matrix</span><br>        <span class="hljs-variable language_">self</span>._precision_matrices = <span class="hljs-variable language_">self</span>._calculate_importance_ewc() <span class="hljs-comment"># Generate Fisher (F) Information Matrix </span><br>        <span class="hljs-variable language_">self</span>._n_p_prev, <span class="hljs-variable language_">self</span>._n_omega = <span class="hljs-variable language_">self</span>._calculate_importance() <br>        <span class="hljs-variable language_">self</span>.W, <span class="hljs-variable language_">self</span>.p_old = <span class="hljs-variable language_">self</span>._init_()<br><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_init_</span>(<span class="hljs-params">self</span>):<br>        W = {}<br>        p_old = {}<br>        <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.model.named_parameters():<br>            n = n.replace(<span class="hljs-string">'.'</span>, <span class="hljs-string">'__'</span>)<br>            <span class="hljs-keyword">if</span> p.requires_grad:<br>                W[n] = p.data.clone().zero_()<br>                p_old[n] = p.data.clone()<br>        <span class="hljs-keyword">return</span> W, p_old<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_calculate_importance</span>(<span class="hljs-params">self</span>):<br>        n_p_prev = {}<br>        n_omega = {}<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.dataloaders[<span class="hljs-number">0</span>] != <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.model.named_parameters():<br>                n = n.replace(<span class="hljs-string">'.'</span>, <span class="hljs-string">'__'</span>)<br>                <span class="hljs-keyword">if</span> p.requires_grad:<br><br>                    <span class="hljs-comment"># Find/calculate new values for quadratic penalty on parameters</span><br>                    p_prev = <span class="hljs-built_in">getattr</span>(<span class="hljs-variable language_">self</span>.model, <span class="hljs-string">'{}_SI_prev_task'</span>.<span class="hljs-built_in">format</span>(n))<br>                    W = <span class="hljs-built_in">getattr</span>(<span class="hljs-variable language_">self</span>.model, <span class="hljs-string">'{}_W'</span>.<span class="hljs-built_in">format</span>(n))<br>                    p_current = p.detach().clone()<br>                    p_change = p_current - p_prev<br>                    <span class="hljs-comment"># RWalk<hanla></hanla>的核心创新：结合<hanla></hanla>Fisher<hanla></hanla>矩阵修改<hanla></hanla>SI<hanla></hanla>的<hanla></hanla>omega<hanla></hanla>计算公式</span><br>                    <span class="hljs-comment"># omega_add = W / (0.5 * Fisher<hanla></hanla>矩阵 * 参数变化平方 + epsilon)</span><br>                	<span class="hljs-comment"># 这里融合了<hanla></hanla>EWC<hanla></hanla>的<hanla></hanla>Fisher<hanla></hanla>信息和<hanla></hanla>SI<hanla></hanla>的累积梯度信息</span><br>                    omega_add = W / (<span class="hljs-number">1.0</span> / <span class="hljs-number">2.0</span>*<span class="hljs-variable language_">self</span>._precision_matrices[n] *p_change**<span class="hljs-number">2</span> + <span class="hljs-variable language_">self</span>.epsilon)<br>                    <span class="hljs-keyword">try</span>:<br>                        omega = <span class="hljs-built_in">getattr</span>(<span class="hljs-variable language_">self</span>.model, <span class="hljs-string">'{}_SI_omega'</span>.<span class="hljs-built_in">format</span>(n))<br>                    <span class="hljs-keyword">except</span> AttributeError:<br>                        omega = p.detach().clone().zero_()<br>                    <span class="hljs-comment"># RWalk<hanla></hanla>特有：使用加权平均更新<hanla></hanla>omega<hanla></hanla>值，而不是直接累加</span><br>                	<span class="hljs-comment"># omega_new = 0.5 * 旧<hanla></hanla>omega + 0.5 * 新增<hanla></hanla>omega</span><br>                    omega_new = <span class="hljs-number">0.5</span> * omega + <span class="hljs-number">0.5</span> *omega_add<br>                    n_omega[n] = omega_new<br>                    n_p_prev[n] = p_current<br><br><br>                    <span class="hljs-comment"># Store these new values in the model</span><br>                    <span class="hljs-variable language_">self</span>.model.register_buffer(<span class="hljs-string">'{}_SI_prev_task'</span>.<span class="hljs-built_in">format</span>(n), p_current)<br>                    <span class="hljs-variable language_">self</span>.model.register_buffer(<span class="hljs-string">'{}_SI_omega'</span>.<span class="hljs-built_in">format</span>(n), omega_new)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.model.named_parameters():<br>                n = n.replace(<span class="hljs-string">'.'</span>, <span class="hljs-string">'__'</span>)<br>                <span class="hljs-keyword">if</span> p.requires_grad:<br>                    n_p_prev[n] = p.detach().clone()<br>                    n_omega[n] = p.detach().clone().zero_()<br>                    <span class="hljs-variable language_">self</span>.model.register_buffer(<span class="hljs-string">'{}_SI_prev_task'</span>.<span class="hljs-built_in">format</span>(n), p.detach().clone())<br><br><br>        <span class="hljs-keyword">return</span> n_p_prev, n_omega<br>    <br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_calculate_importance_ewc</span>(<span class="hljs-params">self</span>):<br>        precision_matrices = {}<br>        <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.params.items(): <br>            n = n.replace(<span class="hljs-string">'.'</span>, <span class="hljs-string">'__'</span>) <span class="hljs-comment"># 初始化 Fisher (F) 的矩陣（都補零）</span><br>            precision_matrices[n] = p.clone().detach().fill_(<span class="hljs-number">0</span>)<br><br>        <span class="hljs-variable language_">self</span>.model.<span class="hljs-built_in">eval</span>()<br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.dataloaders[<span class="hljs-number">0</span>] <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            dataloader_num=<span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.dataloaders)<br>            number_data = <span class="hljs-built_in">sum</span>([<span class="hljs-built_in">len</span>(loader) <span class="hljs-keyword">for</span> loader <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.dataloaders])<br>            <span class="hljs-keyword">for</span> dataloader <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.dataloaders:<br>                <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.model.named_parameters():                         <br>                    n = n.replace(<span class="hljs-string">'.'</span>, <span class="hljs-string">'__'</span>)<br>                    precision_matrices[n].data *= (<span class="hljs-number">1</span> -<span class="hljs-variable language_">self</span>.update_ewc_parameter)   <br>                <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataloader:<br>                    <span class="hljs-variable language_">self</span>.model.zero_grad()<br>                    <span class="hljs-built_in">input</span> = data[<span class="hljs-number">0</span>].to(<span class="hljs-variable language_">self</span>.device)<br>                    output = <span class="hljs-variable language_">self</span>.model(<span class="hljs-built_in">input</span>)<br>                    label = data[<span class="hljs-number">1</span>].to(<span class="hljs-variable language_">self</span>.device)<br><br>                    <br>                    <span class="hljs-comment">############################################################################</span><br>                    <span class="hljs-comment">#####                      Generate Fisher Matrix                      #####</span><br>                    <span class="hljs-comment">############################################################################    </span><br>                   	<span class="hljs-comment"># RWalk<hanla></hanla>的核心公式：(omega + Fisher<hanla></hanla>矩阵) × (当前参数 - 前一任务参数)²</span><br>                	<span class="hljs-comment"># 这结合了<hanla></hanla>SI<hanla></hanla>的动态重要性<hanla></hanla>(omega)<hanla></hanla>和<hanla></hanla>EWC<hanla></hanla>的理论基础<hanla></hanla>(Fisher<hanla></hanla>矩阵)</span><br>                	<span class="hljs-comment"># omega<hanla></hanla>反映了参数变化对损失的累积影响</span><br>                	<span class="hljs-comment"># Fisher<hanla></hanla>矩阵反映了参数变化对损失函数曲率的影响</span><br>                    loss = F.nll_loss(F.log_softmax(output, dim=<span class="hljs-number">1</span>), label)             <br>                    loss.backward()                                                    <br>                                                                                    <br>                    <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.model.named_parameters():                         <br>                        n = n.replace(<span class="hljs-string">'.'</span>, <span class="hljs-string">'__'</span>)<br>                        precision_matrices[n].data += <span class="hljs-variable language_">self</span>.update_ewc_parameter*p.grad.data ** <span class="hljs-number">2</span> / number_data  <br>                                                                            <br>            precision_matrices = {n: p <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> precision_matrices.items()}<br><br>        <span class="hljs-keyword">return</span> precision_matrices<br><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">penalty</span>(<span class="hljs-params">self, model: nn.Module</span>):<br>        loss = <span class="hljs-number">0.0</span><br>        <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> model.named_parameters():<br>            n = n.replace(<span class="hljs-string">'.'</span>, <span class="hljs-string">'__'</span>)<br>            <span class="hljs-keyword">if</span> p.requires_grad:<br>                prev_values = <span class="hljs-variable language_">self</span>._n_p_prev[n]<br>                omega = <span class="hljs-variable language_">self</span>._n_omega[n]<br><br>                <span class="hljs-comment">#################################################################################</span><br>                <span class="hljs-comment">####        Generate regularization term  _loss by omega and Fisher Matrix   ####</span><br>                <span class="hljs-comment">#################################################################################</span><br>                _loss = (omega + <span class="hljs-variable language_">self</span>._precision_matrices[n]) * (p - prev_values) ** <span class="hljs-number">2</span><br>                loss += _loss.<span class="hljs-built_in">sum</span>()<br>         <br>        <span class="hljs-keyword">return</span> loss<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">update</span>(<span class="hljs-params">self, model</span>):<br>        <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> model.named_parameters():<br>            n = n.replace(<span class="hljs-string">'.'</span>, <span class="hljs-string">'__'</span>)<br>            <span class="hljs-keyword">if</span> p.requires_grad:<br>                <span class="hljs-keyword">if</span> p.grad <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                    <span class="hljs-variable language_">self</span>.W[n].add_(-p.grad * (p.detach() - <span class="hljs-variable language_">self</span>.p_old[n]))<br>                    <span class="hljs-variable language_">self</span>.model.register_buffer(<span class="hljs-string">'{}_W'</span>.<span class="hljs-built_in">format</span>(n), <span class="hljs-variable language_">self</span>.W[n])<br>                <span class="hljs-variable language_">self</span>.p_old[n] = p.detach().clone()<br>        <span class="hljs-keyword">return</span> <br></code></pre></td></tr></tbody></table></figure>
<h4 id="scp---sliced-cramer-preservation">SCP - Sliced Cramer Preservation</h4>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250801171313850.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 定义采样球面向量的辅助函数<hanla></hanla></span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sample_spherical</span>(<span class="hljs-params">npoints, ndim=<span class="hljs-number">3</span></span>):<br>    <span class="hljs-comment"># 生成<hanla></hanla>ndim<hanla></hanla>维、npoints<hanla></hanla>个随机向量，每个向量的元素服从标准正态分布</span><br>    vec = np.random.randn(ndim, npoints)<br>    <span class="hljs-comment"># 对每个向量进行<hanla></hanla>L2<hanla></hanla>范数归一化，使其成为单位向量（在球面上）</span><br>    <span class="hljs-comment"># axis=0<hanla></hanla>表示对每一列（每个向量）分别计算范数并归一化</span><br>    vec /= np.linalg.norm(vec, axis=<span class="hljs-number">0</span>)<br>    <span class="hljs-keyword">return</span> torch.from_numpy(vec)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">scp</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-string">"""</span><br><span class="hljs-string">    SCP (Sliced Cramer Preservation) 算法类</span><br><span class="hljs-string">    用于终身学习中的灾难性遗忘问题</span><br><span class="hljs-string">    参考论文：https://openreview.net/forum?id=BJge3TNKwH</span><br><span class="hljs-string">    """</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model: nn.Module, dataloaders: <span class="hljs-built_in">list</span>, L: <span class="hljs-built_in">int</span>, device</span>):<br>        <span class="hljs-variable language_">self</span>.model = model <br>        <span class="hljs-variable language_">self</span>.dataloaders = dataloaders<br>        <span class="hljs-variable language_">self</span>.params = {n: p <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.model.named_parameters() <span class="hljs-keyword">if</span> p.requires_grad}<br>        <span class="hljs-variable language_">self</span>._state_parameters = {}<br>        <span class="hljs-comment"># 存储<hanla></hanla>L<hanla></hanla>值，表示随机采样的球面向量数量</span><br>        <span class="hljs-variable language_">self</span>.L= L<br>        <span class="hljs-variable language_">self</span>.device = device<br>        <span class="hljs-comment"># 调用方法计算重要性矩阵（SCP<hanla></hanla>的<hanla></hanla>Gamma<hanla></hanla>矩阵）</span><br>        <span class="hljs-variable language_">self</span>._precision_matrices = <span class="hljs-variable language_">self</span>.calculate_importance()<br>    <br>        <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.params.items():<br>            <span class="hljs-variable language_">self</span>._state_parameters[n] = p.clone().detach()<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">calculate_importance</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># 初始化重要性矩阵字典（SCP<hanla></hanla>的<hanla></hanla>Gamma<hanla></hanla>矩阵）</span><br>        precision_matrices = {}<br>        <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.params.items():<br>            precision_matrices[n] = p.clone().detach().fill_(<span class="hljs-number">0</span>)<br><br>        <span class="hljs-variable language_">self</span>.model.<span class="hljs-built_in">eval</span>()<br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.dataloaders[<span class="hljs-number">0</span>] <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            dataloader_num = <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.dataloaders)<br>            num_data = <span class="hljs-built_in">sum</span>([<span class="hljs-built_in">len</span>(loader) <span class="hljs-keyword">for</span> loader <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.dataloaders])<br>            <span class="hljs-keyword">for</span> dataloader <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.dataloaders:<br>                <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataloader:<br>                    <span class="hljs-variable language_">self</span>.model.zero_grad()<br>                    output = <span class="hljs-variable language_">self</span>.model(data[<span class="hljs-number">0</span>].to(<span class="hljs-variable language_">self</span>.device))<br>                    <br>                    <span class="hljs-comment">####################################################################################</span><br>                    <span class="hljs-comment">##### 生成<hanla></hanla>SCP<hanla></hanla>的<hanla></hanla>Gamma(Γ)<hanla></hanla>矩阵（类似于<hanla></hanla>MAS<hanla></hanla>的<hanla></hanla>Omega(Ω)<hanla></hanla>和<hanla></hanla>EWC<hanla></hanla>的<hanla></hanla>Fisher(F)） #####</span><br>                    <span class="hljs-comment">####################################################################################</span><br>                    <span class="hljs-comment">#####   步骤<hanla></hanla>1: 对批次输出向量取平均得到向量φ(:,θ_A*)                    ####</span><br>                    <span class="hljs-comment">####################################################################################</span><br>                    <span class="hljs-comment"># 计算当前批次所有样本输出的平均值</span><br>                    <span class="hljs-comment"># output.shape<hanla></hanla>通常为<hanla></hanla>[batch_size, output_dim]</span><br>                    <span class="hljs-comment"># mean_vec.shape<hanla></hanla>为<hanla></hanla>[output_dim]，代表平均输出向量φ</span><br>                    mean_vec = output.mean(dim=<span class="hljs-number">0</span>)<br><br>                    <span class="hljs-comment">####################################################################################</span><br>                    <span class="hljs-comment">#####   步骤<hanla></hanla>2: 随机采样<hanla></hanla>L<hanla></hanla>个球面向量ξ（使用<hanla></hanla>sample_spherical()<hanla></hanla>函数）     #####</span><br>                    <span class="hljs-comment">####################################################################################</span><br>                    <span class="hljs-comment"># 采样<hanla></hanla>L<hanla></hanla>个单位向量，维度与输出向量相同</span><br>                    <span class="hljs-comment"># output.shape[-1]<hanla></hanla>是输出维度（最后一维）</span><br>                    L_vectors = sample_spherical(<span class="hljs-variable language_">self</span>.L, output.shape[-<span class="hljs-number">1</span>])<br>                    <span class="hljs-comment"># 转置矩阵使其形状从<hanla></hanla>[output_dim, L]<hanla></hanla>变为<hanla></hanla>[L, output_dim]</span><br>                    <span class="hljs-comment"># 然后移动到指定设备并转换为<hanla></hanla>float<hanla></hanla>类型</span><br>                    L_vectors = L_vectors.transpose(<span class="hljs-number">1</span>,<span class="hljs-number">0</span>).to(<span class="hljs-variable language_">self</span>.device).<span class="hljs-built_in">float</span>()<br><br>                    <span class="hljs-comment">####################################################################################</span><br>                    <span class="hljs-comment">#####   步骤<hanla></hanla>3: 每个向量ξ与向量φ(:,θ_A*)<hanla></hanla>内积得到标量ρ                       ####</span><br>                    <span class="hljs-comment">#####          对标量ρ取<hanla></hanla>backward，每个参数得到各自的梯度∇ρ                 ####</span><br>                    <span class="hljs-comment">#####          每个参数的梯度∇ρ取绝对值并对<hanla></hanla>L<hanla></hanla>个向量取平均得到各参数的Γ标量        ####  </span><br>                    <span class="hljs-comment">#####          所有参数的Γ标量组合而成就是Γ矩阵                          ####</span><br>                    <span class="hljs-comment">####	(记得每次<hanla></hanla>backward<hanla></hanla>之后要<hanla></hanla>zero_grad<hanla></hanla>去清除梯度，否则梯度会累加)        ####   </span><br>                    <span class="hljs-comment">####################################################################################</span><br>                    <span class="hljs-comment"># 初始化总标量为<hanla></hanla>0（用于累积<hanla></hanla>L<hanla></hanla>个内积结果）</span><br>                    total_scalar = <span class="hljs-number">0</span><br>                    <span class="hljs-comment"># 遍历<hanla></hanla>L<hanla></hanla>个随机采样的球面向量</span><br>                    <span class="hljs-keyword">for</span> vec <span class="hljs-keyword">in</span> L_vectors:<br>                        <span class="hljs-comment"># 计算当前向量ξ与平均输出向量φ的内积，得到标量ρ</span><br>                        <span class="hljs-comment"># torch.matmul<hanla></hanla>执行向量内积运算：ξ·φ = Σ(ξᵢ×φᵢ)</span><br>                        scalar=torch.matmul(vec, mean_vec)<br>                        <span class="hljs-comment"># 累积所有内积结果</span><br>                        total_scalar += scalar<br>                    <span class="hljs-comment"># 对<hanla></hanla>L<hanla></hanla>个内积结果取平均</span><br>                    total_scalar /= L_vectors.shape[<span class="hljs-number">0</span>] <br>                    <span class="hljs-comment"># 对平均内积进行反向传播，计算相对于模型参数的梯度</span><br>                    <span class="hljs-comment"># 这个梯度反映了参数变化对"投影距离"的敏感性</span><br>                    total_scalar.backward()<br>                    <span class="hljs-comment">##################################################################################      </span><br>                     <br>                    <span class="hljs-comment"># 遍历模型的所有命名参数                                </span><br>                    <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.model.named_parameters():                      <br>                        <span class="hljs-comment"># 累积每个参数的重要性权重</span><br>                        <span class="hljs-comment"># 使用梯度的绝对值（与<hanla></hanla>EWC<hanla></hanla>的梯度平方不同）</span><br>                        <span class="hljs-comment"># 这反映了参数对输出分布"切片投影"的重要性</span><br>                        precision_matrices[n].data += p.grad.<span class="hljs-built_in">abs</span>() / num_data <span class="hljs-comment">## 与<hanla></hanla>EWC<hanla></hanla>的差异      </span><br>                        <br>        precision_matrices = {n: p <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> precision_matrices.items()}<br>        <span class="hljs-comment"># 返回重要性矩阵（Gamma<hanla></hanla>矩阵）</span><br>        <span class="hljs-keyword">return</span> precision_matrices<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">penalty</span>(<span class="hljs-params">self, model: nn.Module</span>):<br>        <span class="hljs-comment"># 初始化正则化损失为<hanla></hanla>0</span><br>        loss = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> model.named_parameters():<br>            <span class="hljs-comment"># 计算<hanla></hanla>SCP<hanla></hanla>正则化项：Γ × (当前参数 - 前一任务参数)²</span><br>            <span class="hljs-comment"># _precision_matrices[n]<hanla></hanla>是参数<hanla></hanla>n<hanla></hanla>的重要性权重（Gamma<hanla></hanla>值）</span><br>            <span class="hljs-comment"># _state_parameters[n]<hanla></hanla>是前一任务中参数<hanla></hanla>n<hanla></hanla>的值</span><br>            <span class="hljs-comment"># 公式：L_reg = Σ Γᵢ × (θᵢ - θᵢ*)²</span><br>            _loss = <span class="hljs-variable language_">self</span>._precision_matrices[n] * (p - <span class="hljs-variable language_">self</span>._state_parameters[n]) ** <span class="hljs-number">2</span><br>            <span class="hljs-comment"># 累积所有参数的正则化损失</span><br>            loss += _loss.<span class="hljs-built_in">sum</span>()<br>        <span class="hljs-comment"># 返回总的<hanla></hanla>SCP<hanla></hanla>正则化损失</span><br>        <span class="hljs-keyword">return</span> loss<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">update</span>(<span class="hljs-params">self, model</span>):<br>        <span class="hljs-comment"># SCP<hanla></hanla>算法在训练过程中不需要更新任何状态</span><br>        <span class="hljs-comment"># 重要性权重在任务间隙计算，训练期间保持不变</span><br>        <span class="hljs-keyword">return</span> <br></code></pre></td></tr></tbody></table></figure>
<h1 id="meta-learning">Meta Learning</h1>
<h2 id="introduction-3">Introduction</h2>
<p>Step 1: What is <strong><u>learnable</u></strong> in a learning algorithm?</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250804141513530.png" srcset="/img/loading.gif" lazyload></p>
<p>Step 2: Define <strong><u>loss function</u></strong> <span class="math inline"><em>L</em>(<em>ϕ</em>)</span> for <u><strong>learning algorithm</strong></u> <span class="math inline"><em>F</em><sub><em>ϕ</em></sub></span></p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250804142038950.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250804142108945.png" srcset="/img/loading.gif" lazyload></p>
<p><span class="math inline"><em>θ</em><sup>1⋆</sup></span>: ∗: parameters of the classifier learned by <span class="math inline"><em>F</em><sub><em>ϕ</em></sub></span> , using the training examples of task 1</p>
<p>How can we know a classifier is good or bad? <strong>Evaluate the classifier on testing set</strong></p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250804142329741.png" srcset="/img/loading.gif" lazyload></p>
<p>Total Loss: <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.777ex;" xmlns="http://www.w3.org/2000/svg" width="15.447ex" height="2.949ex" role="img" focusable="false" viewBox="0 -960 6827.4 1303.3"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mo" transform="translate(681,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1070,0)"><path data-c="1D719" d="M409 688Q413 694 421 694H429H442Q448 688 448 686Q448 679 418 563Q411 535 404 504T392 458L388 442Q388 441 397 441T429 435T477 418Q521 397 550 357T579 260T548 151T471 65T374 11T279 -10H275L251 -105Q245 -128 238 -160Q230 -192 227 -198T215 -205H209Q189 -205 189 -198Q189 -193 211 -103L234 -11Q234 -10 226 -10Q221 -10 206 -8T161 6T107 36T62 89T43 171Q43 231 76 284T157 370T254 422T342 441Q347 441 348 445L378 567Q409 686 409 688ZM122 150Q122 116 134 91T167 53T203 35T237 27H244L337 404Q333 404 326 403T297 395T255 379T211 350T170 304Q152 276 137 237Q122 191 122 150ZM500 282Q500 320 484 347T444 385T405 400T381 404H378L332 217L284 29Q284 27 285 27Q293 27 317 33T357 47Q400 66 431 100T475 170T494 234T500 282Z"></path></g><g data-mml-node="mo" transform="translate(1666,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(2332.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="munderover" transform="translate(3388.6,0)"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path></g><g data-mml-node="mi" transform="translate(1089,477.1) scale(0.707)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g><g data-mml-node="TeXAtom" transform="translate(1089,-285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(600,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1378,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="msup" transform="translate(6022.2,0)"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(331,363) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container></span> ( <span class="math inline"><em>𝑁</em></span> is the number of the training tasks)</p>
<ul>
<li>In typical ML, you compute the loss based on <strong>training examples</strong></li>
<li>In meta, you compute the loss based on <strong>testing examples</strong> of <strong>training tasks</strong></li>
</ul>
<p>Step 3: Using the optimization approach you know to fine <span class="math inline"><em>ϕ</em></span> that can minimize <span class="math inline"><em>L</em>(<em>ϕ</em>)</span></p>
<p><span class="math inline"><em>ϕ</em><sup>⋆</sup> = <em>a</em><em>r</em><em>g</em>&nbsp;<em>m</em><em>i</em><em>n</em><sub><em>ϕ</em></sub><em>L</em>(<em>ϕ</em>)</span></p>
<ul>
<li>If you know how to compute <span class="math inline">∂<em>L</em>(<em>ϕ</em>)/∂<em>ϕ</em></span>: <strong>Gradient descent</strong> is your friend.</li>
<li>What if <span class="math inline"><em>L</em>(<em>ϕ</em>)</span> is not differentiable?: Reinforcement Learning / Evolutionary Algorithm</li>
</ul>
<h3 id="framework">Framework</h3>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250804143152438.png" srcset="/img/loading.gif" lazyload></p>
<pre><code class=" mermaid">graph TB
  A["Step 1: 定义可学习的φ"] --&gt; B["学习算法<hanla></hanla>F_φ: D_train → θ*"]
  B --&gt; C["Step 2: 元损失<hanla></hanla>L(φ) = E_Ti[L_Ti^test(θ*)]"]
  C --&gt; D["Step 3: 优化 min_φ L(φ)"]
  D --&gt; E{输出最优<hanla></hanla>F_φ*}
  
  subgraph "任务分布 p(T)"
    F["采样任务 Ti"]
    F --&gt; G["内循环: F_φ(D_i^train) → θ_i*"]
    G --&gt; H["外循环: 计算<hanla></hanla>L_Ti^test(θ_i*)"]
  end
  H --&gt; C
</code></pre>
<h3 id="ml-v.s.-meta">ML v.s. Meta</h3>
<table>
<thead>
<tr class="header">
<th></th>
<th style="text-align: left;">Machine Learning</th>
<th style="text-align: left;">Meta Learning</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Goal</td>
<td style="text-align: left;">find a function f</td>
<td style="text-align: left;">find a function F that finds a function f</td>
</tr>
<tr class="even">
<td>Training</td>
<td style="text-align: left;">Within-task Training &amp; Testing</td>
<td style="text-align: left;">Across-task Training &amp; Testing</td>
</tr>
<tr class="odd">
<td>Loss</td>
<td style="text-align: left;"><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250804144745952.png" srcset="/img/loading.gif" lazyload></td>
<td style="text-align: left;"><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250804144758404.png" srcset="/img/loading.gif" lazyload></td>
</tr>
</tbody>
</table>
<h2 id="what-is-learnable-in-a-learning-algorithm">What is learnable in a learning algorithm?</h2>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250804144941734.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="learning-to-initialize">Learning to initialize</h3>
<ul>
<li>Model Agnostic Meta Learning (MAML)</li>
<li>Reptile</li>
</ul>
<h4 id="maml">MAML</h4>
<p>Model-Agnostic Meta-Learning（MAML，模型无关元学习）是一种通用的元学习方法，旨在学习一个模型参数初始化，使其可以在<strong>仅经过少量梯度更新后快速适应新任务</strong>。</p>
<blockquote>
<p><strong>模型无关性（Model-Agnostic）</strong>：MAML 不依赖于模型的具体结构，只要该模型可以通过梯度下降进行优化，它就可以应用 MAML。</p>
</blockquote>
<figure class="highlight r"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs r">initialize ϕ randomly<br><br><span class="hljs-keyword">for</span> each meta<span class="hljs-operator">-</span>iteration<span class="hljs-operator">:</span><br>    sample batch of tasks <span class="hljs-punctuation">{</span><span class="hljs-built_in">T</span>₁<span class="hljs-punctuation">,</span> <span class="hljs-built_in">T</span>₂<span class="hljs-punctuation">,</span> ...<span class="hljs-punctuation">,</span> T_N<span class="hljs-punctuation">}</span><br><br>    <span class="hljs-keyword">for</span> each task <span class="hljs-built_in">T</span>ᵢ<span class="hljs-operator">:</span><br>        <span class="hljs-comment"># Inner loop: 任务内的快速适应</span><br>        θᵢ <span class="hljs-operator">=</span> ϕ <span class="hljs-operator">-</span> α ∇_ϕ L_trainᵢ<span class="hljs-punctuation">(</span>ϕ<span class="hljs-punctuation">)</span><br><br>        <span class="hljs-comment"># Outer loop: 用任务测试集评估效果</span><br>        L_metaᵢ <span class="hljs-operator">=</span> L_testᵢ<span class="hljs-punctuation">(</span>θᵢ<span class="hljs-punctuation">)</span><br><br>    <span class="hljs-comment"># Meta-update：在多个任务上平均</span><br>    L_meta <span class="hljs-operator">=</span> mean<span class="hljs-punctuation">(</span>L_metaᵢ <span class="hljs-keyword">for</span> <span class="hljs-built_in">all</span> i<span class="hljs-punctuation">)</span><br>    ϕ ← ϕ <span class="hljs-operator">-</span> β ∇_ϕ L_meta<br></code></pre></td></tr></tbody></table></figure>
<h4 id="reptile">Reptile</h4>
<p>To Learn More: https://arxiv.org/abs/1803.02999</p>
<p>Reptile<hanla></hanla>的设计目标是实现 <strong>MAML 的目标</strong>（快速适应新任务），但<strong>避免计算昂贵的二阶梯度</strong>。它是一种<hanla></hanla><strong>first-order</strong> 方法（只用一阶梯）。</p>
<p>Basic Idea：通过在任务上训练几步后，把初始化参数朝着 task-specific 参数的方向移动。</p>
<blockquote>
<p>也就是说，在多个任务上重复执行<hanla></hanla>“训练一会儿 → 返回初始点 → 更新方向”<hanla></hanla>的过程，使得初始点最终接近所有任务的最优解的<hanla></hanla>“公共区域”。</p>
</blockquote>
<h3 id="optimizer">Optimizer</h3>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250804150922376.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250804150934616.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250804150940444.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<p>Marcin Andrychowicz , et al., Learning to learn by gradient descent by gradient descent, NIPS, 2016</p>
</blockquote>
<h3 id="network-architecture-search-nas">Network Architecture Search (NAS)</h3>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250804151201367.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="reinforcement-learning-1">Reinforcement Learning</h4>
<ul>
<li>Barret Zoph , et al., Neural Architecture Search with Reinforcement Learning, ICLR 2017</li>
<li>Barret Zoph , et al., Learning Transferable Architectures for Scalable Image Recognition, CVPR, 2018</li>
<li>Hieu Pham, et al., Efficient Neural Architecture Search via Parameter Sharing, ICML, 2018</li>
</ul>
<p>An agent uses a set of actions to determine the network architecture.</p>
<p><span class="math inline"><em>ϕ</em></span>: the agent’s parameters</p>
<p><span class="math inline"> − <em>L</em>(<em>ϕ</em>)</span>: Reward to be maximized</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250804151354095.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="evolution-algorithm">Evolution Algorithm</h4>
<ul>
<li>Esteban Real, et al., Large Scale Evolution of Image Classifiers, ICML 2017</li>
<li>Esteban Real, et al., Regularized Evolution for Image Classifier Architecture Search, AAAI, 2019</li>
<li>Hanxiao Liu, et al., Hierarchical Representations for Efficient Architecture Search, ICLR, 2018</li>
</ul>
<h4 id="darts">DARTS</h4>
<p>Hanxiao Liu, et al., DARTS: Differentiable Architecture Search, ICLR, 2019</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250804151519563.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<p>将神经网络结构搜索问题从<strong>离散的组合优化问题</strong>转化为<strong>连续的可导问题</strong>，从而可以使用<strong>梯度下降直接优化结构参数</strong>。</p>
</blockquote>
<figure class="highlight applescript"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs applescript"><hanla></hanla>初始化结构参数 α，模型参数 w<br><br><span class="hljs-keyword">repeat</span> <span class="hljs-keyword">until</span> convergence:<br>    <span class="hljs-comment"># Inner update (model weights)</span><br>    使用训练集 D_train 更新 w<br>    <br>    <span class="hljs-comment"># Outer update (architecture)</span><br>    使用验证集 D_val 更新 α<br><br>提取最优结构（每对节点选择最大权重操作）<br>使用该结构重新训练网络参数 w<br></code></pre></td></tr></tbody></table></figure>
<h3 id="data-augmentation">Data Augmentation</h3>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250804151827395.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>Yonggang Li, Guosheng Hu, Yongtao Wang, Timothy Hospedales Neil M. Robertson, Yongxin Yang, DADA: Differentiable Automatic Data Augmentation, ECCV, 2020</li>
<li>Daniel Ho, Eric Liang, Ion Stoica, Pieter Abbeel, Xi Chen, Population Based Augmentation: Efficient Learning of Augmentation Policy Schedules, ICML, 2019</li>
<li>Ekin D. Cubuk Barret Zoph Dandelion Mane Vijay Vasudevan Quoc V. Le, AutoAugment : Learning Augmentation Policies from Data, CVPR, 2019</li>
</ul>
<h3 id="sample-reweighting">Sample Reweighting</h3>
<p><strong>Give different samples different weights</strong></p>
<blockquote>
<p>在训练过程中，对训练样本赋予不同的权重（importance weight），以调控模型更关注哪些样本。</p>
</blockquote>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250804152130274.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>Jun Shu, Qi Xie Lixuan Yi Qian Zhao Sanping Zhou Zongben Xu Deyu Meng, Meta-Weight Net: Learning an Explicit Mapping For Sample Weighting, NeurIPS , 2019</li>
<li>Mengye Ren, Wenyuan Zeng, Bin Yang, Raquel Urtasun, Learning to Reweight Examples for Robust Deep Learning, ICML, 2018</li>
</ul>
<h2 id="homework-15-few-shot-classification">Homework 15: Few-shot Classification</h2>
<h3 id="task-description-3">Task Description</h3>
<p>The Omniglot dataset - background set: 30 alphabets - evaluation set: 20 alphabets</p>
<p>Problem setup: 5-way 1-shot classification</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250804155307575.png" srcset="/img/loading.gif" lazyload></p>
<p>Training MAML on Omniglot classification task.</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250804155346890.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="baseline-guide">Baseline Guide</h3>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250804155442027.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="simple-baseline-8">Simple Baseline</h3>
<p><a target="_blank" rel="noopener" href="https://github.com/Striver98/Machine-Learning-Homework/blob/master/ML2022_HW15_Meta_Learning.ipynb">Sample Code</a></p>
<h3 id="medium-baseline-9">Medium baseline</h3>
<p>Use FO-MAML</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> Finish the inner loop update rule</span><br>            <span class="hljs-comment"># 计算损失对快速权重的梯度</span><br>            <span class="hljs-comment"># create_graph=True 是关键：保持计算图以便后续的二阶梯度计算</span><br>            grads = torch.autograd.grad(loss, fast_weights.values())<br>            <br>            <span class="hljs-comment"># 执行梯度下降更新快速权重</span><br>            <span class="hljs-comment"># 这是<hanla></hanla>MAML<hanla></hanla>内循环的核心：θ' = θ - α∇L(fθ(support_set), support_labels)</span><br>            fast_weights = OrderedDict(<br>                (name, param - inner_lr * grad)<br>                <span class="hljs-keyword">for</span> ((name, param), grad) <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(fast_weights.items(), grads)<br>            )<br>            <span class="hljs-comment"># raise NotImplementedError</span><br></code></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> Finish the outer loop update</span><br>        <span class="hljs-comment"># 计算元损失对原始模型参数的梯度（外循环更新）</span><br>        <span class="hljs-comment"># 这是<hanla></hanla>MAML<hanla></hanla>外循环的核心：θ = θ - β∇L(fθ'(query_set), query_labels)</span><br>        <span class="hljs-comment"># 其中θ'是经过内循环更新后的参数，β是外循环学习率（由<hanla></hanla>optimizer<hanla></hanla>管理）</span><br>        meta_batch_loss.backward()<br>        optimizer.step()<br>        <span class="hljs-comment"># raise NotimplementedError</span><br></code></pre></td></tr></tbody></table></figure>
<p>Output:</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250804162623468.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="strong-baseline-8">Strong Baseline</h3>
<p>Use MAML+ Epoch = 100</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> Finish the inner loop update rule</span><br>            <span class="hljs-comment"># 计算损失对快速权重的梯度</span><br>            <span class="hljs-comment"># create_graph=True 是关键：保持计算图以便后续的二阶梯度计算</span><br>            grads = torch.autograd.grad(loss, fast_weights.values(), create_graph=<span class="hljs-literal">True</span>)<br>            <br>            <span class="hljs-comment"># 执行梯度下降更新快速权重</span><br>            <span class="hljs-comment"># 这是<hanla></hanla>MAML<hanla></hanla>内循环的核心：θ' = θ - α∇L(fθ(support_set), support_labels)</span><br>            fast_weights = OrderedDict(<br>                (name, param - inner_lr * grad)<br>                <span class="hljs-keyword">for</span> ((name, param), grad) <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(fast_weights.items(), grads)<br>            )<br>            <span class="hljs-comment"># raise NotImplementedError</span><br></code></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> Finish the outer loop update</span><br>        <span class="hljs-comment"># 计算元损失对原始模型参数的梯度（外循环更新）</span><br>        <span class="hljs-comment"># 这是<hanla></hanla>MAML<hanla></hanla>外循环的核心：θ = θ - β∇L(fθ'(query_set), query_labels)</span><br>        <span class="hljs-comment"># 其中θ'是经过内循环更新后的参数，β是外循环学习率（由<hanla></hanla>optimizer<hanla></hanla>管理）</span><br>        meta_batch_loss.backward()<br>        optimizer.step()<br>        <span class="hljs-comment"># raise NotimplementedError</span><br></code></pre></td></tr></tbody></table></figure>
<p>Output:</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250804163919579.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="boss-baseline-5">Boss Baseline</h3>
<p>MAML + Task Augmentation</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">MetaSolver</span>(<span class="hljs-params"></span><br><span class="hljs-params">    model,</span><br><span class="hljs-params">    optimizer,</span><br><span class="hljs-params">    x,</span><br><span class="hljs-params">    n_way,</span><br><span class="hljs-params">    k_shot,</span><br><span class="hljs-params">    q_query,</span><br><span class="hljs-params">    loss_fn,</span><br><span class="hljs-params">    inner_train_step=<span class="hljs-number">1</span>,</span><br><span class="hljs-params">    inner_lr=<span class="hljs-number">0.4</span>,</span><br><span class="hljs-params">    train=<span class="hljs-literal">True</span>,</span><br><span class="hljs-params">    return_labels=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    task_augment=<span class="hljs-literal">True</span>,  <span class="hljs-comment"># 新增：是否启用任务增强</span></span><br><span class="hljs-params">    augment_prob=<span class="hljs-number">0.5</span>    <span class="hljs-comment"># 新增：任务增强的概率</span></span><br><span class="hljs-params"></span>):<br>    criterion, task_loss, task_acc = loss_fn, [], []<br>    labels = []<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">apply_task_augmentation</span>(<span class="hljs-params">support_set, query_set</span>):<br>        augmented_support = support_set.clone()<br>        augmented_query = query_set.clone()<br>        <br>        <span class="hljs-keyword">if</span> np.random.random() &lt; augment_prob:<br>            <span class="hljs-comment"># 添加高斯噪声</span><br>            noise_std = <span class="hljs-number">0.05</span><br>            noise_support = torch.randn_like(augmented_support) * noise_std<br>            noise_query = torch.randn_like(augmented_query) * noise_std<br>            augmented_support = torch.clamp(augmented_support + noise_support, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)<br>            augmented_query = torch.clamp(augmented_query + noise_query, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)<br>        <br>        <span class="hljs-keyword">if</span> np.random.random() &lt; augment_prob:<br>            <span class="hljs-comment"># 随机亮度调整</span><br>            brightness_factor = <span class="hljs-number">0.8</span> + <span class="hljs-number">0.4</span> * np.random.random()  <span class="hljs-comment"># 0.8-1.2</span><br>            augmented_support = torch.clamp(augmented_support * brightness_factor, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)<br>            augmented_query = torch.clamp(augmented_query * brightness_factor, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)<br>        <br>        <span class="hljs-keyword">if</span> np.random.random() &lt; augment_prob:<br>            <span class="hljs-comment"># 随机对比度调整</span><br>            contrast_factor = <span class="hljs-number">0.8</span> + <span class="hljs-number">0.4</span> * np.random.random()  <span class="hljs-comment"># 0.8-1.2</span><br>            mean_support = augmented_support.mean(dim=(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>), keepdim=<span class="hljs-literal">True</span>)<br>            mean_query = augmented_query.mean(dim=(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>), keepdim=<span class="hljs-literal">True</span>)<br>            augmented_support = torch.clamp(<br>                mean_support + contrast_factor * (augmented_support - mean_support), <span class="hljs-number">0</span>, <span class="hljs-number">1</span><br>            )<br>            augmented_query = torch.clamp(<br>                mean_query + contrast_factor * (augmented_query - mean_query), <span class="hljs-number">0</span>, <span class="hljs-number">1</span><br>            )<br>        <br>        <span class="hljs-keyword">return</span> augmented_support, augmented_query<br><br>    <span class="hljs-keyword">for</span> meta_batch <span class="hljs-keyword">in</span> x:<br>        <span class="hljs-comment"># Get data</span><br>        support_set = meta_batch[: n_way * k_shot]<br>        query_set = meta_batch[n_way * k_shot :]<br><br>        <span class="hljs-comment"># 应用任务增强（仅在训练时）</span><br>        <span class="hljs-keyword">if</span> train <span class="hljs-keyword">and</span> task_augment:<br>            support_set, query_set = apply_task_augmentation(support_set, query_set)<br><br>        <span class="hljs-comment"># Copy the params for inner loop</span><br>        fast_weights = OrderedDict(model.named_parameters())<br><br>        <span class="hljs-comment">### ---------- INNER TRAIN LOOP ---------- ###</span><br>        <span class="hljs-keyword">for</span> inner_step <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(inner_train_step):<br>            <span class="hljs-comment"># Simply training</span><br>            train_label = create_label(n_way, k_shot).to(device)<br>            logits = model.functional_forward(support_set, fast_weights)<br>            loss = criterion(logits, train_label)<br>            <span class="hljs-comment"># Inner gradients update! vvvvvvvvvvvvvvvvvvvv #</span><br>            <span class="hljs-string">""" Inner Loop Update """</span><br>            grads = torch.autograd.grad(loss, fast_weights.values(), create_graph=<span class="hljs-literal">True</span>)<br>            <br>            fast_weights = OrderedDict(<br>                (name, param - inner_lr * grad)<br>                <span class="hljs-keyword">for</span> ((name, param), grad) <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(fast_weights.items(), grads)<br>            )<br>        <span class="hljs-comment">### ---------- INNER VALID LOOP ---------- ###</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> return_labels:<br>            <span class="hljs-string">""" training / validation """</span><br>            val_label = create_label(n_way, q_query).to(device)<br><br>            <span class="hljs-comment"># Collect gradients for outer loop</span><br>            logits = model.functional_forward(query_set, fast_weights)<br>            loss = criterion(logits, val_label)<br>            task_loss.append(loss)<br>            task_acc.append(calculate_accuracy(logits, val_label))<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-string">""" testing """</span><br>            logits = model.functional_forward(query_set, fast_weights)<br>            labels.extend(torch.argmax(logits, -<span class="hljs-number">1</span>).cpu().numpy())<br><br>    <span class="hljs-keyword">if</span> return_labels:<br>        <span class="hljs-keyword">return</span> labels<br><br>    <span class="hljs-comment"># Update outer loop</span><br>    model.train()<br>    optimizer.zero_grad()<br><br>    meta_batch_loss = torch.stack(task_loss).mean()<br>    <span class="hljs-keyword">if</span> train:<br>        <span class="hljs-string">""" Outer Loop Update """</span><br>        meta_batch_loss.backward()<br>        optimizer.step()<br><br>    task_acc = np.mean(task_acc)<br>    <span class="hljs-keyword">return</span> meta_batch_loss, task_acc<br></code></pre></td></tr></tbody></table></figure>
<p>Output:</p>
<p><img src="https://blogwzx.oss-cn-beijing.aliyuncs.com/image-20250804165751554.png" srcset="/img/loading.gif" lazyload></p>

                
              </div>
            
            <hr>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E5%AD%A6%E4%B9%A0/" class="category-chain-item">学习</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="print-no-link">#深度学习</a>
      
        <a href="/tags/pytorch/" class="print-no-link">#pytorch</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div><hanla></hanla>机器学习笔记<hanla></hanla></div>
      <div>https://striver98.github.io/2025/05/20/<hanla></hanla>机器学习笔记<hanla></hanla>/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div><hanla></hanla>作者<hanla></hanla></div>
          <div>Wang Zhixuan</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div><hanla></hanla>发布于<hanla></hanla></div>
          <div>2025<hanla></hanla>年<hanla></hanla>5<hanla></hanla>月<hanla></hanla>20<hanla></hanla>日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/05/31/%E6%9C%AC%E7%A7%91%E6%AF%95%E4%B8%9A%E6%97%85%E8%A1%8C%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E8%A5%84%E9%98%B3&amp;%E5%AE%9C%E5%9F%8E/" title="本科毕业旅行（一）：襄阳&amp;宜城">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">本科毕业旅行（一）：襄阳<hanla></hanla>&amp;<hanla></hanla>宜城</span>
                        <span class="visible-mobile">上一篇<hanla></hanla></span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/05/05/2025%E9%9F%B3%E5%BE%8B%E8%81%94%E8%A7%89%E7%86%A0%E6%9B%B2%E4%B8%B0%E7%A2%91%E4%B8%AA%E4%BA%BA%E5%87%BA%E8%A1%8C%E8%AE%B0%E5%BD%95/" title="2025音律联觉熠曲丰碑个人出行记录">
                        <span class="hidden-mobile">2025<hanla></hanla>音律联觉熠曲丰碑个人出行记录</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索<hanla></hanla></h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">×</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input"><hanla></hanla>关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a>
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    

  

</div>

  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script>
  <link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css">

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js"></script>
<script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script>
<script src="/js/events.js"></script>
<script src="/js/plugins.js"></script>





  
    <script src="/js/img-lazyload.js"></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script defer="" src="/js/leancloud.js"></script>

  <script src="/js/local-search.js"></script>

  <script defer="" src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script src="/js/boot.js"></script>


  

  <noscript>
    <div class="noscript-warning"><hanla></hanla>博客在允许 JavaScript 运行的环境下浏览效果更佳<hanla></hanla></div>
  </noscript>


</body></html>